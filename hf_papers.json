{
    "date": {
        "ru": "28 января",
        "en": "January 28",
        "zh": "1月28日"
    },
    "time_utc": "2026-01-28 03:30",
    "weekday": 2,
    "issue_id": 754,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2601.17645",
            "title": "AVMeme Exam: A Multimodal Multilingual Multicultural Benchmark for LLMs' Contextual and Cultural Knowledge and Thinking",
            "url": "https://huggingface.co/papers/2601.17645",
            "abstract": "Current multimodal models demonstrate limited understanding of cultural and contextual audio-visual content, particularly excelling only in surface-level analysis rather than deeper semantic comprehension.  \t\t\t\t\tAI-generated summary \t\t\t\t Internet audio-visual clips convey meaning through time-varying sound and motion, which extend beyond what text alone can represent. To examine whether AI models can understand such signals in human cultural contexts, we introduce AVMeme Exam, a human-curated benchmark of over one thousand iconic Internet sounds and videos spanning speech, songs, music, and sound effects. Each meme is paired with a unique Q&A assessing levels of understanding from surface content to context and emotion to usage and world knowledge, along with metadata such as original year, transcript, summary, and sensitivity. We systematically evaluate state-of-the-art multimodal large language models (MLLMs) alongside human participants using this benchmark. Our results reveal a consistent limitation: current models perform poorly on textless music and sound effects, and struggle to think in context and in culture compared to surface content. These findings highlight a key gap in human-aligned multimodal intelligence and call for models that can perceive contextually and culturally beyond the surface of what they hear and see. Project page: avmemeexam.github.io/public",
            "score": 5,
            "issue_id": 754,
            "pub_date": "2026-01-25",
            "pub_date_card": {
                "ru": "25 января",
                "en": "January 25",
                "zh": "1月25日"
            },
            "hash": "3469909232102a15",
            "github_url": "",
            "github_stars": 0,
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "error": "401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth"
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.19834",
            "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models",
            "url": "https://huggingface.co/papers/2601.19834",
            "abstract": "Visual generation enhances reasoning capabilities in multimodal models by providing more natural world models for physical and spatial tasks, while verbal reasoning remains sufficient for abstract domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI.",
            "score": 4,
            "issue_id": 754,
            "pub_date": "2026-01-27",
            "pub_date_card": {
                "ru": "27 января",
                "en": "January 27",
                "zh": "1月27日"
            },
            "hash": "64820d8bb35c71e0",
            "github_url": "https://github.com/thuml/reasoning-visual-world",
            "github_stars": 2,
            "pdf_title_img": "assets/pdf/title_img/2601.19834.jpg",
            "data": {
                "error": "401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth"
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.19362",
            "title": "Revisiting Parameter Server in LLM Post-Training",
            "url": "https://huggingface.co/papers/2601.19362",
            "abstract": "On-Demand Communication (ODC) adapts parameter server principles to Fully Sharded Data Parallel training by replacing collective communication with point-to-point communication, improving device utilization and throughput in imbalanced large language model training scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern data parallel (DP) training favors collective communication over parameter servers (PS) for its simplicity and efficiency under balanced workloads. However, the balanced workload assumption no longer holds in large language model (LLM) post-training due to the high variance in sequence lengths. Under imbalanced workloads, collective communication creates synchronization barriers, leading to under-utilization of devices with smaller workloads. This change in training dynamics calls for a revisit of the PS paradigm for its robustness to such imbalance. We propose On-Demand Communication (ODC), which adapts PS into Fully Sharded Data Parallel (FSDP) by replacing collective all-gather and reduce-scatter with direct point-to-point communication. Compared to FSDP, ODC reduces the synchronization barrier from once per layer to once per minibatch and decouples the workload on each device so that faster workers are not stalled. It also enables simpler and more effective load balancing at the minibatch level. Across diverse LLM post-training tasks, ODC consistently improves device utilization and training throughput, achieving up to a 36\\% speedup over standard FSDP. These results demonstrate that ODC is a superior fit for the prevalent imbalanced workloads in LLM post-training. Our implementation of ODC and integration with FSDP is open-sourced at https://github.com/sail-sg/odc.",
            "score": 1,
            "issue_id": 754,
            "pub_date": "2026-01-27",
            "pub_date_card": {
                "ru": "27 января",
                "en": "January 27",
                "zh": "1月27日"
            },
            "hash": "bc9857133ce776e8",
            "github_url": "https://github.com/sail-sg/odc",
            "github_stars": 3,
            "pdf_title_img": "assets/pdf/title_img/2601.19362.jpg",
            "data": {
                "error": "401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth"
            }
        }
    ],
    "link_prev": "2026-01-27.html",
    "link_next": "2026-01-29.html",
    "link_month": "2026-01.html",
    "short_date_prev": {
        "ru": "27.01",
        "en": "01/27",
        "zh": "1月27日"
    },
    "short_date_next": {
        "ru": "29.01",
        "en": "01/29",
        "zh": "1月29日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0,
        "#memory": 0,
        "#retrieval": 0,
        "#prompting": 0,
        "#context_window": 0,
        "#compression": 0,
        "#memorization": 0,
        "#continual_learning": 0,
        "#knowledge_graphs": 0,
        "#retrieval_eval": 0,
        "#prompt_caching": 0,
        "#session_memory": 0,
        "#tool_use": 0,
        "#agent_memory": 0
    }
}