[21.01.2026 03:30] Read previous papers.
[21.01.2026 03:30] Generating top page (month).
[21.01.2026 03:30] Writing top page (month).
[21.01.2026 04:39] Read previous papers.
[21.01.2026 04:39] Get feed.
[21.01.2026 04:39] Extract page data from URL. URL: https://huggingface.co/papers/2601.11522
[21.01.2026 04:39] Extract page data from URL. URL: https://huggingface.co/papers/2601.13247
[21.01.2026 04:39] Extract page data from URL. URL: https://huggingface.co/papers/2601.11969
[21.01.2026 04:39] Extract page data from URL. URL: https://huggingface.co/papers/2601.12294
[21.01.2026 04:39] Extract page data from URL. URL: https://huggingface.co/papers/2601.11655
[21.01.2026 04:39] Extract page data from URL. URL: https://huggingface.co/papers/2601.13288
[21.01.2026 04:39] Updating GitHub stars.
[21.01.2026 04:39] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[21.01.2026 04:39] No deleted papers detected.
[21.01.2026 04:39] Downloading and parsing papers (pdf, html). Total: 6.
[21.01.2026 04:39] Downloading and parsing paper https://huggingface.co/papers/2601.11522.
[21.01.2026 04:39] Downloading paper 2601.11522 from https://arxiv.org/pdf/2601.11522v1...
[21.01.2026 04:39] Extracting affiliations from text.
[21.01.2026 04:39] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 6 1 ] . [ 1 2 2 5 1 1 . 1 0 6 2 : r UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation Ruiheng Zhang1,, Jingfeng Yao2,, Huangxuan Zhao1,,, Hao Yan1, Xiao He1, Lei Chen2, Zhou Wei1, Yong Luo1, Zengmao Wang1, Lefei Zhang1, Dacheng Tao3, Bo Du1, 1Wuhan University 2Huazhong University of Science and Technology 3Nanyang Technological University "
[21.01.2026 04:39] Failed to download and parse paper https://huggingface.co/papers/2601.11522: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[21.01.2026 04:39] Downloading and parsing paper https://huggingface.co/papers/2601.13247.
[21.01.2026 04:39] Downloading paper 2601.13247 from https://arxiv.org/pdf/2601.13247v1...
[21.01.2026 04:39] Extracting affiliations from text.
[21.01.2026 04:39] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Baochang Ren, Yunzhi Yao, Rui Sun, Shuofei Qiao, Ningyu Zhang*, Huajun Chen* Zhejiang University University of California, Los Angeles {baochang.ren, zhangningyu}@zju.edu.cn Code "
[21.01.2026 04:39] Failed to download and parse paper https://huggingface.co/papers/2601.13247: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[21.01.2026 04:39] Downloading and parsing paper https://huggingface.co/papers/2601.11969.
[21.01.2026 04:39] Downloading paper 2601.11969 from https://arxiv.org/pdf/2601.11969v1...
[21.01.2026 04:39] Extracting affiliations from text.
[21.01.2026 04:39] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"MemoryRewardBench: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models Zecheng Tang1,2, Baibei Ji1,2, Ruoxi Sun1,2, Haitian Wang1,2, Wangjie You1 Yijun Zhang3, Wenpeng Zhu3, Ji Qi3, Juntao Li1,2*, Min Zhang1 1Soochow University, China 2LCM Laboratory 3China Mobile (Suzhou), China {zctang, bbji}@stu.suda.edu.cn {ljt, minzhang}@suda.edu.cn 6 2 0 2 7 1 ] . [ 1 9 6 9 1 1 . 1 0 6 2 : r a "
[21.01.2026 04:39] Failed to download and parse paper https://huggingface.co/papers/2601.11969: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[21.01.2026 04:39] Downloading and parsing paper https://huggingface.co/papers/2601.12294.
[21.01.2026 04:39] Downloading paper 2601.12294 from https://arxiv.org/pdf/2601.12294v1...
[21.01.2026 04:39] Extracting affiliations from text.
[21.01.2026 04:39] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents Dawei Li, Yuguang Yao, Zhen Tan, Huan Liu, Ruocheng Guo Arizona State University, Intuit AI Research {daweili5, ztan36, huanliu}@asu.edu {yuguang_yao,ruocheng_guo}@intuit.com "
[21.01.2026 04:39] Failed to download and parse paper https://huggingface.co/papers/2601.12294: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[21.01.2026 04:39] Downloading and parsing paper https://huggingface.co/papers/2601.11655.
[21.01.2026 04:39] Downloading paper 2601.11655 from https://arxiv.org/pdf/2601.11655v1...
[21.01.2026 04:39] Extracting affiliations from text.
[21.01.2026 04:39] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Advances and Frontiers of LLM-based Issue Resolution in Software Engineering: Comprehensive Survey Caihua Li1, Lianghong Guo1, Yanlin Wang1*, Daya Guo1, Wei Tao2*, Zhenyu Shan3, Mingwei Liu1, Jiachi Chen4, Haoyu Song5, Duyu Tang5, Hongyu Zhang6, Zibin Zheng1 1Sun Yat-sen University, 2Independent Researcher, 3Hangzhou Normal University, 4Zhejiang University, 5Huawei Technologies Co, Ltd, 6Chongqing University {lich535, guolh8, guody5}@mail2.sysu.edu.cn, {wangylin36, zhzibin}@mail.sysu.edu.cn, wtao@ieee.org, 20100119@hznu.edu.cn, chenjiachi317@gmail.com, {songhaoyu1, tangduyu}@huawei.com, hyzhang@cqu.edu.cn 6 2 0 2 5 1 ] . [ 1 5 5 6 1 1 . 1 0 6 2 : r a "
[21.01.2026 04:39] Failed to download and parse paper https://huggingface.co/papers/2601.11655: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[21.01.2026 04:39] Downloading and parsing paper https://huggingface.co/papers/2601.13288.
[21.01.2026 04:39] Downloading paper 2601.13288 from https://arxiv.org/pdf/2601.13288v1...
[21.01.2026 04:39] Extracting affiliations from text.
[21.01.2026 04:39] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 9 1 ] . [ 1 8 8 2 3 1 . 1 0 6 2 : r BERTology View of LLM Orchestrations: Tokenand Layer-Selective Probes for Efficient Single-Pass Classification Departamento de Computación, FCEyN Universidad de Buenos Aires Buenos Aires, Argentina gmeyoyan@dc.uba.ar Luciano Del Corro ELIAS Lab, Departamento de Ingeniería Universidad de San Andrés Victoria, Argentina delcorrol@udesa.edu.ar "
[21.01.2026 04:40] Failed to download and parse paper https://huggingface.co/papers/2601.13288: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[21.01.2026 04:40] Enriching papers with extra data.
[21.01.2026 04:40] ********************************************************************************
[21.01.2026 04:40] Abstract 0. UniX presents a unified medical foundation model that decouples visual understanding and generation tasks using distinct autoregressive and diffusion branches with cross-modal attention for enhanced performance.  					AI-generated summary 				 Despite recent progress, medical foundation models still...
[21.01.2026 04:40] ********************************************************************************
[21.01.2026 04:40] Abstract 1. WorldMind addresses the modal disconnect in LLMs by autonomously building a symbolic world knowledge repository that enhances physical feasibility and task optimality through experience-based learning.  					AI-generated summary 				 Current Large Language Models (LLMs) exhibit a critical modal disc...
[21.01.2026 04:40] ********************************************************************************
[21.01.2026 04:40] Abstract 2. A benchmark called MemoryRewardBench is introduced to systematically evaluate reward models' ability to assess long-term memory management in large language models across various context lengths and memory patterns.  					AI-generated summary 				 Existing works increasingly adopt memory-centric mec...
[21.01.2026 04:40] ********************************************************************************
[21.01.2026 04:40] Abstract 3. ToolPRMBench is introduced as a large-scale benchmark for evaluating process reward models in tool-using agents, featuring step-level test cases and multi-LLM verification to ensure data quality.  					AI-generated summary 				 Reward-guided search methods have demonstrated strong potential in enhan...
[21.01.2026 04:40] ********************************************************************************
[21.01.2026 04:40] Abstract 4. Large language models face significant challenges in software issue resolution, prompting the development of autonomous coding agents through various training-free and training-based methodologies.  					AI-generated summary 				 Issue resolution, a complex Software Engineering (SWE) task integral t...
[21.01.2026 04:40] ********************************************************************************
[21.01.2026 04:40] Abstract 5. Lightweight probes trained on hidden states of LLMs enable efficient classification tasks without additional computational overhead, improving safety and sentiment analysis performance.  					AI-generated summary 				 Production LLM systems often rely on separate models for safety and other classifi...
[21.01.2026 04:40] Read previous papers.
[21.01.2026 04:40] Generating reviews via LLM API.
[21.01.2026 04:40] Querying the API.
[21.01.2026 04:40] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

UniX presents a unified medical foundation model that decouples visual understanding and generation tasks using distinct autoregressive and diffusion branches with cross-modal attention for enhanced performance.  					AI-generated summary 				 Despite recent progress, medical foundation models still struggle to unify visual understanding and generation, as these tasks have inherently conflicting goals: semantic abstraction versus pixel-level reconstruction. Existing approaches, typically based on parameter-shared autoregressive architectures, frequently lead to compromised performance in one or both tasks. To address this, we present UniX, a next-generation unified medical foundation model for chest X-ray understanding and generation. UniX decouples the two tasks into an autoregressive branch for understanding and a diffusion branch for high-fidelity generation. Crucially, a cross-modal self-attention mechanism is introduced to dynamically guide the generation process with understanding features. Coupled with a rigorous data cleaning pipeline and a multi-stage training strategy, this architecture enables synergistic collaboration between tasks while leveraging the strengths of diffusion models for superior generation. On two representative benchmarks, UniX achieves a 46.1% improvement in understanding performance (Micro-F1) and a 24.2% gain in generation quality (FD-RadDino), using only a quarter of the parameters of LLM-CXR. By achieving performance on par with task-specific models, our work establishes a scalable paradigm for synergistic medical image understanding and generation. Codes and models are available at https://github.com/ZrH42/UniX.
[21.01.2026 04:40] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[21.01.2026 04:40] Querying the API.
[21.01.2026 04:40] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

WorldMind addresses the modal disconnect in LLMs by autonomously building a symbolic world knowledge repository that enhances physical feasibility and task optimality through experience-based learning.  					AI-generated summary 				 Current Large Language Models (LLMs) exhibit a critical modal disconnect: they possess vast semantic knowledge but lack the procedural grounding to respect the immutable laws of the physical world. Consequently, while these agents implicitly function as world models, their simulations often suffer from physical hallucinations-generating plans that are logically sound but physically unexecutable. Existing alignment strategies predominantly rely on resource-intensive training or fine-tuning, which attempt to compress dynamic environmental rules into static model parameters. However, such parametric encapsulation is inherently rigid, struggling to adapt to the open-ended variability of physical dynamics without continuous, costly retraining. To bridge this gap, we introduce WorldMind, a framework that autonomously constructs a symbolic World Knowledge Repository by synthesizing environmental feedback. Specifically, it unifies Process Experience to enforce physical feasibility via prediction errors and Goal Experience to guide task optimality through successful trajectories. Experiments on EB-ALFRED and EB-Habitat demonstrate that WorldMind achieves superior performance compared to baselines with remarkable cross-model and cross-environment transferability.
[21.01.2026 04:40] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[21.01.2026 04:40] Querying the API.
[21.01.2026 04:40] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A benchmark called MemoryRewardBench is introduced to systematically evaluate reward models' ability to assess long-term memory management in large language models across various context lengths and memory patterns.  					AI-generated summary 				 Existing works increasingly adopt memory-centric mechanisms to process long contexts in a segment manner, and effective memory management is one of the key capabilities that enables large language models to effectively propagate information across the entire sequence. Therefore, leveraging reward models (RMs) to automatically and reliably evaluate memory quality is critical. In this work, we introduce MemoryRewardBench, the first benchmark to systematically study the ability of RMs to evaluate long-term memory management processes. MemoryRewardBench covers both long-context comprehension and long-form generation tasks, featuring 10 distinct settings with different memory management patterns, with context length ranging from 8K to 128K tokens. Evaluations on 13 cutting-edge RMs indicate a diminishing performance gap between open-source and proprietary models, with newer-generation models consistently outperforming their predecessors regardless of parameter count. We further expose the capabilities and fundamental limitations of current RMs in evaluating LLM memory management across diverse settings.
[21.01.2026 04:40] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[21.01.2026 04:40] Querying the API.
[21.01.2026 04:40] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

ToolPRMBench is introduced as a large-scale benchmark for evaluating process reward models in tool-using agents, featuring step-level test cases and multi-LLM verification to ensure data quality.  					AI-generated summary 				 Reward-guided search methods have demonstrated strong potential in enhancing tool-using agents by effectively guiding sampling and exploration over complex action spaces. As a core design, those search methods utilize process reward models (PRMs) to provide step-level rewards, enabling more fine-grained monitoring. However, there is a lack of systematic and reliable evaluation benchmarks for PRMs in tool-using settings. In this paper, we introduce ToolPRMBench, a large-scale benchmark specifically designed to evaluate PRMs for tool-using agents. ToolPRMBench is built on top of several representative tool-using benchmarks and converts agent trajectories into step-level test cases. Each case contains the interaction history, a correct action, a plausible but incorrect alternative, and relevant tool metadata. We respectively utilize offline sampling to isolate local single-step errors and online sampling to capture realistic multi-step failures from full agent rollouts. A multi-LLM verification pipeline is proposed to reduce label noise and ensure data quality. We conduct extensive experiments across large language models, general PRMs, and tool-specialized PRMs on ToolPRMBench. The results reveal clear differences in PRM effectiveness and highlight the potential of specialized PRMs for tool-using. Code and data will be released at https://github.com/David-Li0406/ToolPRMBench.
[21.01.2026 04:40] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[21.01.2026 04:40] Querying the API.
[21.01.2026 04:40] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Large language models face significant challenges in software issue resolution, prompting the development of autonomous coding agents through various training-free and training-based methodologies.  					AI-generated summary 				 Issue resolution, a complex Software Engineering (SWE) task integral to real-world development, has emerged as a compelling challenge for artificial intelligence. The establishment of benchmarks like SWE-bench revealed this task as profoundly difficult for large language models, thereby significantly accelerating the evolution of autonomous coding agents. This paper presents a systematic survey of this emerging domain. We begin by examining data construction pipelines, covering automated collection and synthesis approaches. We then provide a comprehensive analysis of methodologies, spanning training-free frameworks with their modular components to training-based techniques, including supervised fine-tuning and reinforcement learning. Subsequently, we discuss critical analyses of data quality and agent behavior, alongside practical applications. Finally, we identify key challenges and outline promising directions for future research. An open-source repository is maintained at https://github.com/DeepSoftwareAnalytics/Awesome-Issue-Resolution to serve as a dynamic resource in this field.
[21.01.2026 04:40] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[21.01.2026 04:40] Querying the API.
[21.01.2026 04:40] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Lightweight probes trained on hidden states of LLMs enable efficient classification tasks without additional computational overhead, improving safety and sentiment analysis performance.  					AI-generated summary 				 Production LLM systems often rely on separate models for safety and other classification-heavy steps, increasing latency, VRAM footprint, and operational complexity. We instead reuse computation already paid for by the serving LLM: we train lightweight probes on its hidden states and predict labels in the same forward pass used for generation. We frame classification as representation selection over the full token-layer hidden-state tensor, rather than committing to a fixed token or fixed layer (e.g., first-token logits or final-layer pooling). To implement this, we introduce a two-stage aggregator that (i) summarizes tokens within each layer and (ii) aggregates across layer summaries to form a single representation for classification. We instantiate this template with direct pooling, a 100K-parameter scoring-attention gate, and a downcast multi-head self-attention (MHA) probe with up to 35M trainable parameters. Across safety and sentiment benchmarks our probes improve over logit-only reuse (e.g., MULI) and are competitive with substantially larger task-specific baselines, while preserving near-serving latency and avoiding the VRAM and latency costs of a separate guard-model pipeline.
[21.01.2026 04:40] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[21.01.2026 04:40] Renaming data file.
[21.01.2026 04:40] Renaming previous data. hf_papers.json to ./d/2026-01-21.json
[21.01.2026 04:40] Saving new data file.
[21.01.2026 04:40] Generating page.
[21.01.2026 04:40] Renaming previous page.
[21.01.2026 04:40] Renaming previous data. index.html to ./d/2026-01-21.html
[21.01.2026 04:40] Writing result.
[21.01.2026 04:40] Renaming log file.
[21.01.2026 04:40] Renaming previous data. log.txt to ./logs/2026-01-21_last_log.txt
