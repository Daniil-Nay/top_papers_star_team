[19.01.2026 23:12] Read previous papers.
[19.01.2026 23:12] Generating top page (month).
[19.01.2026 23:12] Writing top page (month).
[19.01.2026 23:34] Read previous papers.
[19.01.2026 23:34] Get feed.
[19.01.2026 23:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.08521
[19.01.2026 23:34] Refreshing page data for GitHub info. URL: https://huggingface.co/papers/2601.08521
[19.01.2026 23:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11496
[19.01.2026 23:34] Refreshing page data for GitHub info. URL: https://huggingface.co/papers/2601.11496
[19.01.2026 23:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.10355
[19.01.2026 23:34] Refreshing page data for GitHub info. URL: https://huggingface.co/papers/2601.10355
[19.01.2026 23:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.08430
[19.01.2026 23:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11000
[19.01.2026 23:34] Refreshing page data for GitHub info. URL: https://huggingface.co/papers/2601.11000
[19.01.2026 23:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11404
[19.01.2026 23:34] Refreshing page data for GitHub info. URL: https://huggingface.co/papers/2601.11404
[19.01.2026 23:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11037
[19.01.2026 23:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.10909
[19.01.2026 23:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.09001
[19.01.2026 23:34] Refreshing page data for GitHub info. URL: https://huggingface.co/papers/2601.09001
[19.01.2026 23:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.09195
[19.01.2026 23:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.10781
[19.01.2026 23:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11514
[19.01.2026 23:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.10825
[19.01.2026 23:34] Refreshing page data for GitHub info. URL: https://huggingface.co/papers/2601.10825
[19.01.2026 23:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11087
[19.01.2026 23:34] Refreshing page data for GitHub info. URL: https://huggingface.co/papers/2601.11087
[19.01.2026 23:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.09636
[19.01.2026 23:34] Refreshing page data for GitHub info. URL: https://huggingface.co/papers/2601.09636
[19.01.2026 23:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11516
[19.01.2026 23:34] Refreshing page data for GitHub info. URL: https://huggingface.co/papers/2601.11516
[19.01.2026 23:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11044
[19.01.2026 23:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.07812
[19.01.2026 23:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11354
[19.01.2026 23:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11227
[19.01.2026 23:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.10922
[19.01.2026 23:34] Refreshing page data for GitHub info. URL: https://huggingface.co/papers/2601.10922
[19.01.2026 23:34] Get page data from previous paper. URL: https://huggingface.co/papers/2601.09255
[19.01.2026 23:34] Refreshing page data for GitHub info. URL: https://huggingface.co/papers/2601.09255
[19.01.2026 23:34] Updating GitHub stars.
[19.01.2026 23:34] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[19.01.2026 23:34] No deleted papers detected.
[19.01.2026 23:34] Downloading and parsing papers (pdf, html). Total: 22.
[19.01.2026 23:34] Downloading and parsing paper https://huggingface.co/papers/2601.08521.
[19.01.2026 23:34] Extra JSON file exists (./assets/json/2601.08521.json), skip PDF parsing.
[19.01.2026 23:34] Paper image links file exists (./assets/img_data/2601.08521.json), skip HTML parsing.
[19.01.2026 23:34] Success.
[19.01.2026 23:34] Downloading and parsing paper https://huggingface.co/papers/2601.11496.
[19.01.2026 23:34] Extra JSON file exists (./assets/json/2601.11496.json), skip PDF parsing.
[19.01.2026 23:34] Paper image links file exists (./assets/img_data/2601.11496.json), skip HTML parsing.
[19.01.2026 23:34] Success.
[19.01.2026 23:34] Downloading and parsing paper https://huggingface.co/papers/2601.10355.
[19.01.2026 23:34] Extra JSON file exists (./assets/json/2601.10355.json), skip PDF parsing.
[19.01.2026 23:34] Paper image links file exists (./assets/img_data/2601.10355.json), skip HTML parsing.
[19.01.2026 23:34] Success.
[19.01.2026 23:34] Downloading and parsing paper https://huggingface.co/papers/2601.08430.
[19.01.2026 23:34] Extra JSON file exists (./assets/json/2601.08430.json), skip PDF parsing.
[19.01.2026 23:34] Paper image links file exists (./assets/img_data/2601.08430.json), skip HTML parsing.
[19.01.2026 23:34] Success.
[19.01.2026 23:34] Downloading and parsing paper https://huggingface.co/papers/2601.11000.
[19.01.2026 23:34] Extra JSON file exists (./assets/json/2601.11000.json), skip PDF parsing.
[19.01.2026 23:34] Paper image links file exists (./assets/img_data/2601.11000.json), skip HTML parsing.
[19.01.2026 23:34] Success.
[19.01.2026 23:34] Downloading and parsing paper https://huggingface.co/papers/2601.11404.
[19.01.2026 23:34] Extra JSON file exists (./assets/json/2601.11404.json), skip PDF parsing.
[19.01.2026 23:34] Paper image links file exists (./assets/img_data/2601.11404.json), skip HTML parsing.
[19.01.2026 23:34] Success.
[19.01.2026 23:34] Downloading and parsing paper https://huggingface.co/papers/2601.11037.
[19.01.2026 23:34] Extra JSON file exists (./assets/json/2601.11037.json), skip PDF parsing.
[19.01.2026 23:34] Paper image links file exists (./assets/img_data/2601.11037.json), skip HTML parsing.
[19.01.2026 23:34] Success.
[19.01.2026 23:34] Downloading and parsing paper https://huggingface.co/papers/2601.10909.
[19.01.2026 23:34] Extra JSON file exists (./assets/json/2601.10909.json), skip PDF parsing.
[19.01.2026 23:34] Paper image links file exists (./assets/img_data/2601.10909.json), skip HTML parsing.
[19.01.2026 23:34] Success.
[19.01.2026 23:34] Downloading and parsing paper https://huggingface.co/papers/2601.09001.
[19.01.2026 23:34] Extra JSON file exists (./assets/json/2601.09001.json), skip PDF parsing.
[19.01.2026 23:34] Paper image links file exists (./assets/img_data/2601.09001.json), skip HTML parsing.
[19.01.2026 23:34] Success.
[19.01.2026 23:34] Downloading and parsing paper https://huggingface.co/papers/2601.09195.
[19.01.2026 23:34] Extra JSON file exists (./assets/json/2601.09195.json), skip PDF parsing.
[19.01.2026 23:34] Paper image links file exists (./assets/img_data/2601.09195.json), skip HTML parsing.
[19.01.2026 23:34] Success.
[19.01.2026 23:34] Downloading and parsing paper https://huggingface.co/papers/2601.10781.
[19.01.2026 23:34] Extra JSON file exists (./assets/json/2601.10781.json), skip PDF parsing.
[19.01.2026 23:34] Paper image links file exists (./assets/img_data/2601.10781.json), skip HTML parsing.
[19.01.2026 23:34] Success.
[19.01.2026 23:34] Downloading and parsing paper https://huggingface.co/papers/2601.11514.
[19.01.2026 23:34] Extra JSON file exists (./assets/json/2601.11514.json), skip PDF parsing.
[19.01.2026 23:34] Paper image links file exists (./assets/img_data/2601.11514.json), skip HTML parsing.
[19.01.2026 23:34] Success.
[19.01.2026 23:34] Downloading and parsing paper https://huggingface.co/papers/2601.10825.
[19.01.2026 23:34] Extra JSON file exists (./assets/json/2601.10825.json), skip PDF parsing.
[19.01.2026 23:34] Paper image links file exists (./assets/img_data/2601.10825.json), skip HTML parsing.
[19.01.2026 23:34] Success.
[19.01.2026 23:34] Downloading and parsing paper https://huggingface.co/papers/2601.11087.
[19.01.2026 23:34] Extra JSON file exists (./assets/json/2601.11087.json), skip PDF parsing.
[19.01.2026 23:34] Paper image links file exists (./assets/img_data/2601.11087.json), skip HTML parsing.
[19.01.2026 23:34] Success.
[19.01.2026 23:34] Downloading and parsing paper https://huggingface.co/papers/2601.09636.
[19.01.2026 23:34] Extra JSON file exists (./assets/json/2601.09636.json), skip PDF parsing.
[19.01.2026 23:34] Paper image links file exists (./assets/img_data/2601.09636.json), skip HTML parsing.
[19.01.2026 23:34] Success.
[19.01.2026 23:34] Downloading and parsing paper https://huggingface.co/papers/2601.11516.
[19.01.2026 23:34] Extra JSON file exists (./assets/json/2601.11516.json), skip PDF parsing.
[19.01.2026 23:34] Paper image links file exists (./assets/img_data/2601.11516.json), skip HTML parsing.
[19.01.2026 23:34] Success.
[19.01.2026 23:34] Downloading and parsing paper https://huggingface.co/papers/2601.11044.
[19.01.2026 23:34] Extra JSON file exists (./assets/json/2601.11044.json), skip PDF parsing.
[19.01.2026 23:34] Paper image links file exists (./assets/img_data/2601.11044.json), skip HTML parsing.
[19.01.2026 23:34] Success.
[19.01.2026 23:34] Downloading and parsing paper https://huggingface.co/papers/2601.07812.
[19.01.2026 23:34] Extra JSON file exists (./assets/json/2601.07812.json), skip PDF parsing.
[19.01.2026 23:34] Paper image links file exists (./assets/img_data/2601.07812.json), skip HTML parsing.
[19.01.2026 23:34] Success.
[19.01.2026 23:34] Downloading and parsing paper https://huggingface.co/papers/2601.11354.
[19.01.2026 23:34] Extra JSON file exists (./assets/json/2601.11354.json), skip PDF parsing.
[19.01.2026 23:34] Paper image links file exists (./assets/img_data/2601.11354.json), skip HTML parsing.
[19.01.2026 23:34] Success.
[19.01.2026 23:34] Downloading and parsing paper https://huggingface.co/papers/2601.11227.
[19.01.2026 23:34] Extra JSON file exists (./assets/json/2601.11227.json), skip PDF parsing.
[19.01.2026 23:34] Paper image links file exists (./assets/img_data/2601.11227.json), skip HTML parsing.
[19.01.2026 23:34] Success.
[19.01.2026 23:34] Downloading and parsing paper https://huggingface.co/papers/2601.10922.
[19.01.2026 23:34] Extra JSON file exists (./assets/json/2601.10922.json), skip PDF parsing.
[19.01.2026 23:34] Paper image links file exists (./assets/img_data/2601.10922.json), skip HTML parsing.
[19.01.2026 23:34] Success.
[19.01.2026 23:34] Downloading and parsing paper https://huggingface.co/papers/2601.09255.
[19.01.2026 23:34] Extra JSON file exists (./assets/json/2601.09255.json), skip PDF parsing.
[19.01.2026 23:34] Paper image links file exists (./assets/img_data/2601.09255.json), skip HTML parsing.
[19.01.2026 23:34] Success.
[19.01.2026 23:34] Enriching papers with extra data.
[19.01.2026 23:34] ********************************************************************************
[19.01.2026 23:34] Abstract 0. Group-based reinforcement learning from verifier rewards suffers from biased advantage estimation that underestimates hard prompts and overestimates easy prompts, which is addressed through a history-aware adaptive difficulty weighting method.  					AI-generated summary 				 Reinforcement Learning f...
[19.01.2026 23:34] ********************************************************************************
[19.01.2026 23:34] Abstract 1. The integration of AI agents into economic markets fundamentally alters the landscape of strategic interaction. We investigate the economic implications of expanding the set of available technologies in three canonical game-theoretic settings: bargaining (resource division), negotiation (asymmetric ...
[19.01.2026 23:34] ********************************************************************************
[19.01.2026 23:34] Abstract 2. A text-based data synthesis approach generates multi-turn tool-use trajectories for large language models, achieving improved performance and reduced computational costs through a specialized trajectory synthesizer.  					AI-generated summary 				 Enabling Large Language Models (LLMs) to effectively...
[19.01.2026 23:34] ********************************************************************************
[19.01.2026 23:34] Abstract 3. RLVR has advanced reasoning capabilities but struggles with open-ended generation due to lack of ground truth; this work proposes an automated rubric generation framework and dataset to improve performance in health reasoning benchmarks.  					AI-generated summary 				 Reinforcement Learning with Ve...
[19.01.2026 23:34] ********************************************************************************
[19.01.2026 23:34] Abstract 4. Personalized large language models can generate false information aligned with user history instead of factual truth, but a new method called FPPS helps maintain both factual accuracy and personalized responses while preserving existing personalization effects.  					AI-generated summary 				 Person...
[19.01.2026 23:34] ********************************************************************************
[19.01.2026 23:34] Abstract 5. Vision-Language-Action models are enhanced by incorporating action-space reasoning through a structured sequence of coarse action intents, improving manipulation task performance in both simulation and real-world environments.  					AI-generated summary 				 Vision-Language-Action (VLA) models have ...
[19.01.2026 23:34] ********************************************************************************
[19.01.2026 23:34] Abstract 6. Reinforcement learning framework for agentic search that improves reliability by teaching agents to recognize reasoning limits and respond appropriately when evidence is insufficient.  					AI-generated summary 				 RL-based agentic search enables LLMs to solve complex questions via dynamic planning...
[19.01.2026 23:34] ********************************************************************************
[19.01.2026 23:34] Abstract 7. A diffusion-based framework generates human motion from text prompts with fine-grained part-level control using a newly constructed dataset with atomic, temporally-aware annotations.  					AI-generated summary 				 Human motion generation from text prompts has made remarkable progress in recent year...
[19.01.2026 23:34] ********************************************************************************
[19.01.2026 23:34] Abstract 8. Output-entropy profiles computed from final-layer next-token probabilities serve as a scalable signal for monitoring LLM performance and prioritizing data acquisition under domain shifts.  					AI-generated summary 				 Deploying LLMs raises two coupled challenges: (1) monitoring - estimating where ...
[19.01.2026 23:34] ********************************************************************************
[19.01.2026 23:34] Abstract 9. Supervised fine-tuning with multiple references addresses overfitting to non-core expressions by masking low-probability tokens based on their semantic importance.  					AI-generated summary 				 Supervised fine-tuning (SFT) is a fundamental post-training strategy to align Large Language Models (LLM...
[19.01.2026 23:34] ********************************************************************************
[19.01.2026 23:34] Abstract 10. A novel language-conditioned optical flow forecasting model combines Vision-Language Model and Diffusion architecture to predict future motion from noisy web-scale video data, demonstrating versatility in robotic manipulation and video generation tasks.  					AI-generated summary 				 Future motion ...
[19.01.2026 23:34] ********************************************************************************
[19.01.2026 23:34] Abstract 11. ShapeR generates high-fidelity 3D shapes from casual image sequences using visual-inertial SLAM, 3D detection, and vision-language models with rectified flow transformer conditioning.  					AI-generated summary 				 Recent advances in 3D shape generation have achieved impressive results, but most ex...
[19.01.2026 23:34] ********************************************************************************
[19.01.2026 23:34] Abstract 12. Reasoning models demonstrate enhanced performance through multi-agent-like interactions that create diverse cognitive perspectives and improve problem-solving through structured social organization.  					AI-generated summary 				 Large language models have achieved remarkable capabilities across do...
[19.01.2026 23:34] ********************************************************************************
[19.01.2026 23:34] Abstract 13. A physics-aware reinforcement learning paradigm is introduced for video generation that enforces physical collision rules directly in high-dimensional spaces, ensuring strict application of physics knowledge rather than treating it as conditional constraints.  					AI-generated summary 				 Physical...
[19.01.2026 23:34] ********************************************************************************
[19.01.2026 23:34] Abstract 14. PersonalAlign framework addresses GUI agent alignment with implicit user intents through hierarchical memory organization and long-term record reasoning, improving both execution and proactive performance.  					AI-generated summary 				 While GUI agents have shown strong performance under explicit ...
[19.01.2026 23:34] ********************************************************************************
[19.01.2026 23:34] Abstract 15. Activation probes for language model misuse mitigation face challenges with long-context generalization, requiring new architectures and diverse training for robust performance across production shifts.  					AI-generated summary 				 Frontier language model capabilities are improving rapidly. We th...
[19.01.2026 23:34] ********************************************************************************
[19.01.2026 23:34] Abstract 16. AgencyBench presents a comprehensive benchmark for evaluating autonomous agents across real-world scenarios, enabling automated evaluation through user simulation and sandbox environments while revealing performance gaps between closed-source and open-source models.  					AI-generated summary 				 L...
[19.01.2026 23:34] ********************************************************************************
[19.01.2026 23:34] Abstract 17. Large Vision Language Models exhibit significant limitations in multi-image understanding and reasoning, which are revealed through a new benchmark and addressed via procedural data generation and attention masking techniques.  					AI-generated summary 				 Large Vision Language Models (LVLMs) have...
[19.01.2026 23:34] ********************************************************************************
[19.01.2026 23:34] Abstract 18. Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained rea...
[19.01.2026 23:34] ********************************************************************************
[19.01.2026 23:34] Abstract 19. Controlling the language of thought in large language models increases output diversity by leveraging distinct thinking spaces across different languages, with mixed-language sampling providing superior results.  					AI-generated summary 				 Output diversity is crucial for Large Language Models as...
[19.01.2026 23:34] ********************************************************************************
[19.01.2026 23:34] Abstract 20. Data curation for multimodal reasoning shows that difficulty-based example selection on aligned datasets drives performance gains, while increasing dataset size mainly reduces variance and synthetic augmentation heuristics often degrade performance.  					AI-generated summary 				 We study data cura...
[19.01.2026 23:34] ********************************************************************************
[19.01.2026 23:34] Abstract 21. A three-stage pipeline decouples physical reasoning from visual synthesis in video generation, improving physical plausibility and motion controllability through distinct phases of reasoning, planning, and refinement.  					AI-generated summary 				 Recent diffusion-based video generation models can...
[19.01.2026 23:34] Read previous papers.
[19.01.2026 23:34] Generating reviews via LLM API.
[19.01.2026 23:34] Using data from previous issue: {"categories": ["#rl", "#optimization", "#reasoning", "#reinforcement_learning"], "emoji": "‚öôÔ∏è", "ru": {"title": "–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–º–µ—â–µ–Ω–∏–π –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤ –≤ –æ–±—É—á–µ–Ω–∏–∏ –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏—é —Å –ø–æ–º–æ—â—å—é –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏", "desc": "–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø
[19.01.2026 23:34] Using data from previous issue: {"categories": ["#ethics", "#games"], "emoji": "ü§ñ", "ru": {"title": "–í–ª–∏—è–Ω–∏–µ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ —ç–∫–æ–Ω–æ–º–∏–∫—É –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–ª–∏—è–Ω–∏–µ –≤–Ω–µ–¥—Ä–µ–Ω–∏—è –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –≤ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–µ —Ä—ã–Ω–∫–∏ –Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤. –†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞
[19.01.2026 23:34] Using data from previous issue: {"categories": ["#agent_memory", "#memory", "#context_window", "#tool_use", "#multiturn", "#tools", "#dataset", "#training"], "emoji": "üìö", "ru": {"title": "–°–∏–Ω—Ç–µ–∑ –º–Ω–æ–≥–æ—Çurn–æ–≤—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –ø–æ–¥—Ö
[19.01.2026 23:34] Using data from previous issue: {"categories": ["#healthcare", "#benchmark", "#transfer_learning", "#retrieval", "#reasoning", "#dataset"], "emoji": "üìö", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∏—è –æ—Ü–µ–Ω–æ—á–Ω—ã—Ö —à–∫–∞–ª –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö", "desc": "–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç
[19.01.2026 23:34] Using data from previous issue: {"categories": ["#hallucinations", "#benchmark", "#personalization", "#inference", "#data"], "emoji": "üìö", "ru": {"title": "FPPS: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –§–∞–∫—Ç–∏—á–µ—Å–∫–æ–π –¢–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –ë–æ–ª—å—à–∏–º–∏ –Ø–∑—ã–∫–æ–≤—ã–º–∏ –ú–æ–¥–µ–ª—è–º–∏", "desc": "–î–∞–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∏—Å–∫–∞–∂–µ–Ω–∏—è —Ñ–∞–∫—Ç–æ–≤ –≤ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö 
[19.01.2026 23:34] Using data from previous issue: {"categories": ["#action", "#multimodal", "#reasoning", "#language", "#action_space_reasoning", "#vision"], "emoji": "ü§ñ", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π VLA –∑–∞ —Å—á–µ—Ç —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –¥–µ–π—Å—Ç–≤–∏–π", "desc": "–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —É–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å Vision-Language-Action 
[19.01.2026 23:34] Using data from previous issue: {"categories": ["#reliability", "#rl", "#reasoning", "#boundary_aware_policy_optimization", "#agi"], "emoji": "ü§î", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç—Å–∫–∏—Ö —Å–∏—Å—Ç–µ–º —á–µ—Ä–µ–∑ –æ—Å–æ–∑–Ω–∞–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –ø–æ–¥—Ö–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏—è –¥–ª—è –∞–≥–µ–Ω—Ç—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞, –∫–æ—Ç–æ—Ä
[19.01.2026 23:34] Using data from previous issue: {"categories": ["#data", "#dataset"], "emoji": "üë©‚Äçü¶∂", "ru": {"title": "–§—Ä–∞–Ω–∫–µ–Ω–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç –¥–≤–∏–∂–µ–Ω–∏–π: –∫–æ–Ω—Ç—Ä–æ–ª—å —Ç–µ–ª–∞ –∏ –≤—Ä–µ–º–µ–Ω–∏", "desc": "–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫ —Å –≤—ã—Å–æ–∫–æ–π —Å—Ç–µ–ø–µ–Ω—å—é –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —á–∞
[19.01.2026 23:34] Using data from previous issue: {"categories": ["#training", "#interpretability", "#optimization", "#inference"], "emoji": "üìä", "ru": {"title": "–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ —É–ª—É—á—à–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –ø—Ä–æ—Ñ–∏–ª–µ–π —ç–Ω—Ç—Ä–æ–ø–∏–∏ –≤—ã–≤–æ–¥–∞", "desc": "–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—Ñ–∏–ª–µ–π —ç–Ω—Ç—Ä–æ–ø–∏–∏ –≤—ã–≤–æ–¥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π —Å–ª–µ–¥—É—é—â–µ–≥–æ 
[19.01.2026 23:34] Using data from previous issue: {"categories": ["#training", "#optimization", "#reasoning"], "emoji": "üìö", "ru": {"title": "–ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å—Å—ã–ª–∫–∞–º–∏ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω–æ–π –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–Ω–æ—Å—Ç–∏", "desc": "–î–∞–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∏–∑–±—ã—Ç–æ—á–Ω–æ–π –ø–æ–¥–≥–æ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∫ –≤—Ç–æ—Ä–æ—Å—Ç–µ–ø–µ–Ω–Ω—ã–º –≤—ã
[19.01.2026 23:34] Using data from previous issue: {"categories": ["#optical_flow_forecasting", "#vision_language_model", "#diffusion", "#robotics", "#multimodal"], "emoji": "üìà", "ru": {"title": "–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ—Ç–æ–∫–∞ —Å –ø–æ–º–æ—â—å—é –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–π Vision-Language Model –∏ Diffusion", "desc": "–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å FOFPred –¥–ª—è –ø—Ä–æ–≥
[19.01.2026 23:34] Using data from previous issue: {"categories": ["#3d"], "emoji": "üì¶", "ru": {"title": "ShapeR: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω—ã—Ö 3D —Ñ–æ—Ä–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–µ—Ñ–æ—Ä–º–∞–ª—å–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "ShapeR ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–º–æ–¥–µ–ª–µ–π –æ–±—ä–µ–∫—Ç–æ–≤ –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏–∑ –æ–±—ã—á–Ω—ã—Ö –≤–∏–¥–µ–æ–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–∏–∑—É–∞–ª—å–Ω–æ-–∏–Ω–µ—Ä—Ü–∏–∞–ª—å–Ω–æ–≥–æ SLAM,
[19.01.2026 23:34] Using data from previous issue: {"categories": ["#reasoning", "#agents", "#social_interaction", "#interpretability", "#cognitive_modeling"], "emoji": "ü§ñ", "ru": {"title": "–°–æ—Ü–∏–∞–ª—å–Ω–∞—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è –º—ã—à–ª–µ–Ω–∏—è –∫–∞–∫ –∫–ª—é—á –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", "desc": "–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ –º–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —É–ª—É
[19.01.2026 23:34] Using data from previous issue: {"categories": ["#rl", "#video_generation", "#physics"], "emoji": "üîÑ", "ru": {"title": "–§–∏–∑–∏—á–µ—Å–∫–∏ –æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ —Ñ–∏–∑–∏–∫—É, –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ
[19.01.2026 23:34] Using data from previous issue: {"categories": ["#agent_memory", "#hallucinations", "#retrieval", "#context_window", "#reasoning", "#session_memory", "#agi", "#agents"], "emoji": "üì±", "ru": {"title": "–õ–∏—á–Ω—ã–π –±–∞–ª–∞–Ω—Å: –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –Ω–∞–º–µ—Ä–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —á–µ—Ä–µ–∑ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—É—é –ø–∞–º—è—Ç—å –ò–ò-–∞–≥–µ–Ω—Ç–∞", "desc": "Framework PersonalAlign —Ä–µ—à–∞–µ—Ç –∑
[19.01.2026 23:34] Using data from previous issue: {"categories": ["#agent_memory", "#context_window", "#security", "#long_context", "#prompting", "#data"], "emoji": "üîÆ", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –∑–∞—â–∏—Ç—ã —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –∞–∫—Ç–∏–≤–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø—Ä–æ–±—ã", "desc": "–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –ø—Ä–æ–±–ª–µ–º—ã –æ–±–æ–±—â–µ–Ω–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–æ–Ω–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö
[19.01.2026 23:34] Using data from previous issue: {"categories": ["#agents", "#agent_memory", "#agi", "#tool_use", "#benchmark"], "emoji": "üöÄ", "ru": {"title": "AgencyBench: –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "AgencyBench –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é –ø–ª–∞—Ç—Ñ–æ—Ä–º—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö, –ø–æ–∑–≤–æ–ª—è—é
[19.01.2026 23:34] Using data from previous issue: {"categories": ["#reasoning", "#interpretability", "#data", "#benchmark"], "emoji": "üìä", "ru": {"title": "–í—ã—è–≤–ª–µ–Ω–∏–µ –∏ –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –º—É–ª—å—Ç–∏–∏–∑–æ–±—Ä–∞–∑–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞", "desc": "–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É—é—Ç—Å—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –∑—Ä–µ–Ω–∏—è –∏ —è
[19.01.2026 23:34] Using data from previous issue: {"categories": ["#agent_memory", "#agents", "#reasoning", "#benchmark"], "emoji": "üöÄ", "ru": {"title": "AstroReason-Bench: –∏—Å–ø—ã—Ç–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–Ω—ã—Ö LLM –≤ –∑–∞–¥–∞—á–∞—Ö –∫–æ—Å–º–∏—á–µ—Å–∫–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ —Å–æ–∑–¥–∞–Ω–∏—é –Ω–æ–≤–æ–≥–æ –∫–æ–º–ø–ª–µ–∫—Å–∞ –∏—Å–ø—ã—Ç–∞–Ω–∏–π ‚Äî AstroReason-Bench, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω–æ–≥–æ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–æ–∑–º–æ
[19.01.2026 23:34] Using data from previous issue: {"categories": ["#language_of_thought", "#language_modeling", "#output_diversity", "#interpretability", "#multilingual"], "emoji": "üåç", "ru": {"title": "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–º –º—ã—à–ª–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è", "desc": "–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–ª–∏—è–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–∞–∑–ª
[19.01.2026 23:34] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#data", "#interpretability"], "emoji": "üìö", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –º–Ω–æ–≥–æ–≤–µ–∫—Ç–æ—Ä–Ω–æ–π —Ä–∞—Å—Å—É–¥–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏", "desc": "–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ—Ü–µ—Å—Å –∫—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω–æ–π —Ä–∞—Å—Å—É–¥–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏. –ë—ã–ª–æ –æ–±
[19.01.2026 23:34] Using data from previous issue: {"categories": ["#diffusion", "#physical_reasoning", "#multimodal", "#3d"], "emoji": "üîÑ", "ru": {"title": "–¢—Ä–µ—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥ PhyRPR –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ñ–∏–∑–∏—á–µ—Å–∫–æ–π –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω–æ—Å—Ç–∏ –∏ –∫–æ–Ω—Ç—Ä–æ–ª—è –¥–≤–∏–∂–µ–Ω–∏–π –≤ –≤–∏–¥–µ–æ", "desc": "–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Ç—Ä–µ—Ö—ç—Ç–∞–ø–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ PhyRPR –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä–∞—è
[19.01.2026 23:34] Renaming data file.
[19.01.2026 23:34] Renaming previous data. hf_papers.json to ./d/2026-01-19.json
[19.01.2026 23:34] Saving new data file.
[19.01.2026 23:34] Generating page.
[19.01.2026 23:34] Renaming previous page.
[19.01.2026 23:34] Renaming previous data. index.html to ./d/2026-01-19.html
[19.01.2026 23:34] Writing result.
[19.01.2026 23:34] Renaming log file.
[19.01.2026 23:34] Renaming previous data. log.txt to ./logs/2026-01-19_last_log.txt
