[13.01.2026 10:26] Read previous papers.
[13.01.2026 10:26] Generating top page (month).
[13.01.2026 10:26] Writing top page (month).
[19.01.2026 14:39] Read previous papers.
[19.01.2026 14:39] Get feed.
[19.01.2026 14:39] Extract page data from URL. URL: https://huggingface.co/papers/2601.08521
[19.01.2026 14:39] Extract page data from URL. URL: https://huggingface.co/papers/2601.11496
[19.01.2026 14:39] Extract page data from URL. URL: https://huggingface.co/papers/2601.10355
[19.01.2026 14:39] Extract page data from URL. URL: https://huggingface.co/papers/2601.08430
[19.01.2026 14:39] Extract page data from URL. URL: https://huggingface.co/papers/2601.11000
[19.01.2026 14:39] Extract page data from URL. URL: https://huggingface.co/papers/2601.11404
[19.01.2026 14:39] Extract page data from URL. URL: https://huggingface.co/papers/2601.11037
[19.01.2026 14:39] Extract page data from URL. URL: https://huggingface.co/papers/2601.09195
[19.01.2026 14:39] Extract page data from URL. URL: https://huggingface.co/papers/2601.10909
[19.01.2026 14:39] Extract page data from URL. URL: https://huggingface.co/papers/2601.10825
[19.01.2026 14:39] Extract page data from URL. URL: https://huggingface.co/papers/2601.09636
[19.01.2026 14:39] Extract page data from URL. URL: https://huggingface.co/papers/2601.11516
[19.01.2026 14:39] Extract page data from URL. URL: https://huggingface.co/papers/2601.11087
[19.01.2026 14:39] Extract page data from URL. URL: https://huggingface.co/papers/2601.11514
[19.01.2026 14:39] Extract page data from URL. URL: https://huggingface.co/papers/2601.11354
[19.01.2026 14:39] Extract page data from URL. URL: https://huggingface.co/papers/2601.11227
[19.01.2026 14:39] Extract page data from URL. URL: https://huggingface.co/papers/2601.09255
[19.01.2026 14:39] Extract page data from URL. URL: https://huggingface.co/papers/2601.09001
[19.01.2026 14:39] Extract page data from URL. URL: https://huggingface.co/papers/2601.07812
[19.01.2026 14:39] Updating GitHub stars.
[19.01.2026 14:39] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[19.01.2026 14:39] Downloading and parsing papers (pdf, html). Total: 19.
[19.01.2026 14:39] Downloading and parsing paper https://huggingface.co/papers/2601.08521.
[19.01.2026 14:39] Downloading paper 2601.08521 from https://arxiv.org/pdf/2601.08521v1...
[19.01.2026 14:39] Extracting affiliations from text.
[19.01.2026 14:39] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Your Group-Relative Advantage Is Biased Fengkai Yang1,3,4, Zherui Chen2, Xiaohan Wang4, Xiaodong Lu1,4, Jiajun Chai4, Guojun Yin4, Wei Lin4, Shuai Ma1, Fuzhen Zhuang1, Deqing Wang1, Yaodong Yang3, Jianxin Li1, Yikun Ban1* 1Beihang University 2University of California, Berkeley 3Peking University 4Meituan 6 2 0 2 3 1 ] . [ 1 1 2 5 8 0 . 1 0 6 2 : r a "
[19.01.2026 14:39] Response: ['Beihang University', 'University of California, Berkeley', 'Peking University', 'Meituan']
[19.01.2026 14:39] Deleting PDF ./assets/pdf/2601.08521.pdf.
[19.01.2026 14:39] Success.
[19.01.2026 14:39] Downloading and parsing paper https://huggingface.co/papers/2601.11496.
[19.01.2026 14:39] Downloading paper 2601.11496 from https://arxiv.org/pdf/2601.11496v1...
[19.01.2026 14:39] Extracting affiliations from text.
[19.01.2026 14:39] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 6 1 ] . [ 1 6 9 4 1 1 . 1 0 6 2 : r The Poisoned Apple Effect: Strategic Manipulation of Mediated Markets via Technology Expansion of AI Agents Eilam Shapira, Moshe Tennenholtz, and Roi Reichart Technion Israel Institute of Technology January 2026 Abstract The integration of AI agents into economic markets fundamentally alters the landscape of strategic interaction. We investigate the economic implications of expanding the set of available technologies in three canonical game-theoretic settings: bargaining (resource division), negotiation (asymmetric information trade), and persuasion (strategic information transmission). We find that simply increasing the choice of AI delegates can drastically shift equilibrium payoffs and regulatory outcomes, often creating incentives for regulators to proactively develop and release technologies. Conversely, we identify strategic phenomenon termed the Poisoned Apple effect: an agent may release new technology, which neither they nor their opponent ultimately uses, solely to manipulate the regulators choice of market design in their favor. This strategic release improves the releasers welfare at the expense of their opponent and the regulators fairness objectives. Our findings demonstrate that static regulatory frameworks are vulnerable to manipulation via technology expansion, necessitating dynamic market designs that adapt to the evolving landscape of AI capabilities. The rapid integration of AI agents into the global economy is fundamentally altering the landscape of strategic interaction. In the near future, substantial fraction of economic activities ranging from real estate transactions to corporate partnerships will be mediated by AI delegates acting on behalf of individuals and firms [2]. While current regulatory debates focus on model safety and bias, we identify critical, overlooked economic vulnerability [12] arising from the mere availability of these technologies. We investigate the strategic implications of e"
[19.01.2026 14:39] Response: [
    "Technion Israel Institute of Technology"
]
[19.01.2026 14:39] Deleting PDF ./assets/pdf/2601.11496.pdf.
[19.01.2026 14:39] Success.
[19.01.2026 14:39] Downloading and parsing paper https://huggingface.co/papers/2601.10355.
[19.01.2026 14:39] Downloading paper 2601.10355 from https://arxiv.org/pdf/2601.10355v1...
[19.01.2026 14:39] Extracting affiliations from text.
[19.01.2026 14:39] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 5 1 ] . [ 1 5 5 3 0 1 . 1 0 6 2 : r Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text Zhihao Xu12, Rumei Li2, Jiahuan Li2, Rongxiang Weng2, Jingang Wang2, Xunliang Cai2, Xiting Wang1 1Renmin University of China, 2Meituan, China zhihaoxu@ruc.edu.cn "
[19.01.2026 14:39] Response: ['Renmin University of China', 'Meituan, China']
[19.01.2026 14:39] Deleting PDF ./assets/pdf/2601.10355.pdf.
[19.01.2026 14:39] Success.
[19.01.2026 14:39] Downloading and parsing paper https://huggingface.co/papers/2601.08430.
[19.01.2026 14:39] Downloading paper 2601.08430 from https://arxiv.org/pdf/2601.08430v1...
[19.01.2026 14:39] Extracting affiliations from text.
[19.01.2026 14:39] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"RubricHub: Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation Sunzhu Li1, Jiale Zhao1 , Miteto Wei1 * , Huimin Ren1, Yang Zhou3, Jingwen Yang2, Shunyu Liu4, Kaike Zhang1, Wei Chen1 1Li Auto Inc., China 2The Chinese University of Hong Kong, Shenzhen, China 3Zhejiang University 4Nanyang Technological University {lisunzhu, chenwei10}@lixiang.com vizzlin@foxmail.com 6 2 0 2 3 1 ] . [ 1 0 3 4 8 0 . 1 0 6 2 : r a "
[19.01.2026 14:39] Response: ['Li Auto Inc., China', 'The Chinese University of Hong Kong, Shenzhen, China', 'Zhejiang University', 'Nanyang Technological University']
[19.01.2026 14:39] Deleting PDF ./assets/pdf/2601.08430.pdf.
[19.01.2026 14:39] Success.
[19.01.2026 14:39] Downloading and parsing paper https://huggingface.co/papers/2601.11000.
[19.01.2026 14:39] Downloading paper 2601.11000 from https://arxiv.org/pdf/2601.11000v1...
[19.01.2026 14:39] Extracting affiliations from text.
[19.01.2026 14:39] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"When Personalization Misleads: Understanding and Mitigating Hallucinations in Personalized LLMs Zhongxiang Sun1, Yi Zhan1, Chenglei Shen1, Weijie Yu3, Xiao Zhang1, Ming He2, Jun Xu1 1Gaoling School of Artificial Intelligence, Renmin University of China; 2AI Lab at Lenovo Research; 3School of Artificial Intelligence and Data Science, University of International Business and Economics. {sunzhongxiang}@ruc.edu.cn 6 2 0 2 6 1 ] . [ 1 0 0 0 1 1 . 1 0 6 2 : r a "
[19.01.2026 14:39] Response: ['Gaolol School of Artificial Intelligence, Renmin University of China', 'AI Lab at Lenovo Research', 'School of Artificial Intelligence and Data Science, University of International Business and Economics']
[19.01.2026 14:39] Deleting PDF ./assets/pdf/2601.11000.pdf.
[19.01.2026 14:39] Success.
[19.01.2026 14:39] Downloading and parsing paper https://huggingface.co/papers/2601.11404.
[19.01.2026 14:39] Downloading paper 2601.11404 from https://arxiv.org/pdf/2601.11404v1...
[19.01.2026 14:39] Extracting affiliations from text.
[19.01.2026 14:39] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models Linqing Zhong1,2 Yi Liu2 Yifei Wei1,2 Ziyu Xiong2 Maoqing Yao2 Si Liu1 Guanghui Ren2* 1Beihang University 2AgiBot 6 2 0 2 6 1 ] . [ 1 4 0 4 1 1 . 1 0 6 2 : r a "
[19.01.2026 14:39] Response: ['Beihang University', 'AgiBot']
[19.01.2026 14:39] Deleting PDF ./assets/pdf/2601.11404.pdf.
[19.01.2026 14:39] Success.
[19.01.2026 14:39] Downloading and parsing paper https://huggingface.co/papers/2601.11037.
[19.01.2026 14:39] Downloading paper 2601.11037 from https://arxiv.org/pdf/2601.11037v1...
[19.01.2026 14:39] Extracting affiliations from text.
[19.01.2026 14:39] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"BAPO: Boundary-Aware Policy Optimization for Reliable Agentic Search Shiyu Liu1,2,, Yongjing Yin2, Jianhao Yan4, Yunbo Tang3, Qinggang Zhang5* Bei Li2, Xin Chen2, Jingang Wang2, Xunliang Cai2, Jinsong Su1,3* 1Institute of Artificial Intelligence, Xiamen University 2Meituan Inc. 3School of Informatics, Xiamen University 4Westlake University 5The Hong Kong Polytechnic University shiyu.liu0709@foxmail.com; zqg.zhang@hotmail.com; jssu@xmu.edu.cn 6 2 0 2 6 1 ] . [ 1 7 3 0 1 1 . 1 0 6 2 : r a "
[19.01.2026 14:39] Response: ['Meituan Inc.', 'Xiamen University', 'Westlake University', 'The Hong Kong Polytechnic University']
[19.01.2026 14:39] Deleting PDF ./assets/pdf/2601.11037.pdf.
[19.01.2026 14:39] Success.
[19.01.2026 14:39] Downloading and parsing paper https://huggingface.co/papers/2601.09195.
[19.01.2026 14:39] Downloading paper 2601.09195 from https://arxiv.org/pdf/2601.09195v1...
[19.01.2026 14:39] Extracting affiliations from text.
[19.01.2026 14:39] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"ProFit: Leveraging High-Value Signals in SFT via Probability-Guided Token Selection Tao Liu1, Taiqiang Wu2,, Runming Yang2 Shaoning Sun1 Junjie Wang1, Yujiu Yang1, 1Tsinghua University Equal contribution 2The University of Hong Kong Project Leader Corresponding authors https://github.com/Utaotao/ProFit 6 2 0 2 4 ] . [ 1 5 9 1 9 0 . 1 0 6 2 : r a "
[19.01.2026 14:39] Response: ['Tsinghua University', 'The University of Hong Kong']
[19.01.2026 14:39] Deleting PDF ./assets/pdf/2601.09195.pdf.
[19.01.2026 14:39] Success.
[19.01.2026 14:39] Downloading and parsing paper https://huggingface.co/papers/2601.10909.
[19.01.2026 14:39] Downloading paper 2601.10909 from https://arxiv.org/pdf/2601.10909v1...
[19.01.2026 14:39] Extracting affiliations from text.
[19.01.2026 14:39] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"FrankenMotion: Part-level Human Motion Generation and Composition Chuqiao Li1 Xianghui Xie1,2 Yong Cao1 Andreas Geiger1 1Tubingen AI Center, University of Tubingen, Germany 2Max Planck Institute for Informatics, Saarland Informatics Campus, Germany https://coral79.github.io/frankenmotion/ Gerard Pons-Moll1, 6 2 0 2 5 1 ] . [ 1 9 0 9 0 1 . 1 0 6 2 : r Figure 1. Overview of our FrankenMotion framework. Left: Body-Part Control, where users specify fine-grained movements of individual body parts; Middle: Body-Part + Action Control, enabling coordinated whole-body actions with part-specific constraints; Right: Body-Part + Action + Sequence Control, supporting complex multi-stage motion sequences involving interactions and transitions. In all cases, FrankenAgent translates natural-language instructions into structured control signals for precise motion generation. "
[19.01.2026 14:39] Response: ['Tubingen AI Center, University of Tubingen, Germany', 'Max Planck Institute for Informatics, Saarland Informatics Campus, Germany']
[19.01.2026 14:39] Deleting PDF ./assets/pdf/2601.10909.pdf.
[19.01.2026 14:40] Success.
[19.01.2026 14:40] Downloading and parsing paper https://huggingface.co/papers/2601.10825.
[19.01.2026 14:40] Downloading paper 2601.10825 from https://arxiv.org/pdf/2601.10825v1...
[19.01.2026 14:40] Extracting affiliations from text.
[19.01.2026 14:40] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 5 1 ] . [ 1 5 2 8 0 1 . 1 0 6 2 : r a Junsol Kim1,2,, Shiyang Lai2, Nino Scherrer1, Blaise Agüera Arcas1,3 and James Evans1,2,3,* 1Google, Paradigms of Intelligence Team, 2University of Chicago, 3Santa Fe Institute Large language models have achieved remarkable capabilities across domains, yet mechanisms underlying sophisticated reasoning remain elusive 1,2. Recent reasoning-reinforced models, including OpenAIs o-series, DeepSeek-R1, and QwQ-32B, outperform comparable instruction-tuned models on complex cognitive tasks 3,4, attributed to extended test-time computation through longer chains of thought 5. Here we show that enhanced reasoning emerges not from extended computation alone, but from the implicit simulation of complex, multi-agent-like interactionsa society of thoughtwhich enables the deliberate diversification and debate among internal cognitive perspectives characterized by distinct personality traits and domain expertise. Through quantitative analysis using classified outputs and mechanistic interpretability methods applied to reasoning traces 6,7, we find that reasoning models like DeepSeek-R1 and QwQ-32B exhibit much greater perspective diversity than baseline and merely instruction-tuned models, activating broader conflict between heterogeneous personalityand expertiserelated features during reasoning. This multi-agent structure manifests in conversational behaviours including question-answering sequences, perspective shifts, and reconciliation of conflicting views, as well as in socio-emotional roles that characterize sharp back-and-forth conversation, which together account for the accuracy advantage in reasoning tasks through both direct and indirect facilitation of cognitive strategies 8,9. Controlled reinforcement learning experiments further reveal that base models spontaneously increase conversational behaviours when solely rewarded for reasoning accuracy, and fine-tuning models with conversational scaffolding substantially accelerates "
[19.01.2026 14:40] Response: ['Google', 'Paradigms of Intelligence Team', 'University of Chicago', 'Santa Fe Institute']
[19.01.2026 14:40] Deleting PDF ./assets/pdf/2601.10825.pdf.
[19.01.2026 14:40] Success.
[19.01.2026 14:40] Downloading and parsing paper https://huggingface.co/papers/2601.09636.
[19.01.2026 14:40] Downloading paper 2601.09636 from https://arxiv.org/pdf/2601.09636v1...
[19.01.2026 14:40] Extracting affiliations from text.
[19.01.2026 14:40] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"PersonalAlign: Hierarchical Implicit Intent Alignment for Personalized GUI Agent with Long-Term User-Centric Records Yibo Lyu, Gongwei Chen, Rui Shao, Weili Guan, Liqiang Nie Harbin Institute of Technology, Shenzhen weberlv1b@gmail.com {shaorui, nieliqiang}@hit.edu.com https://github.com/JiuTian-VL/PersonalAlign 6 2 0 2 4 1 ] . [ 1 6 3 6 9 0 . 1 0 6 2 : r a "
[19.01.2026 14:40] Response: ['Harbin Institute of Technology, Shenzhen']
[19.01.2026 14:40] Deleting PDF ./assets/pdf/2601.09636.pdf.
[19.01.2026 14:40] Success.
[19.01.2026 14:40] Downloading and parsing paper https://huggingface.co/papers/2601.11516.
[19.01.2026 14:40] Downloading paper 2601.11516 from https://arxiv.org/pdf/2601.11516v1...
[19.01.2026 14:40] Extracting affiliations from text.
[19.01.2026 14:40] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 6 1 ] . [ 1 6 1 5 1 1 . 1 0 6 2 : r Building Production-Ready Probes For Gemini János Kramár, Joshua Engels, Zheng Wang, Bilal Chughtai, Rohin Shah, Neel Nanda, and Arthur Conmy Google DeepMind, Equal contributions to this work. 2026-01 Frontier language model capabilities are improving rapidly. We thus need stronger mitigations against bad actors misusing increasingly powerful systems. Prior work has shown that activation probes may be promising misuse mitigation technique, but we identify key remaining challenge: probes fail to generalize under important production distribution shifts. In particular, we find that the shift from short-context to long-context inputs is difficult for existing probe architectures. We propose several new probe architecture that handle this long-context distribution shift. We evaluate these probes in the cyber-offensive domain, testing their robustness against various production-relevant shifts, including multi-turn conversations, static jailbreaks, and adaptive red teaming. Our results demonstrate that while multimax addresses context length, combination of architecture choice and training on diverse distributions is required for broad generalization. Additionally, we show that pairing probes with prompted classifiers achieves optimal accuracy at low cost due to the computational efficiency of probes. These findings have informed the successful deployment of misuse mitigation probes in user-facing instances of Gemini, Googles frontier language model. Finally, we find early positive results using AlphaEvolve (Novikov et al., 2025) to automate improvements in both probe architecture search and adaptive red teaming, showing that automating some AI safety research is already possible. Keywords: Activation Probing, Interpretability, Language Models, Misuse Risk, AI Safety, Monitoring 1. Introduction In this paper, we describe our experience applying probes to detect cyber-offensive prompts given as input to Gemini 2.5 Flash (Google,"
[19.01.2026 14:41] Failed to download and parse paper https://huggingface.co/papers/2601.11516: HTTPSConnectionPool(host='gigachat.devices.sberbank.ru', port=443): Max retries exceeded with url: /api/v1/chat/completions (Caused by ConnectTimeoutError(<HTTPSConnection(host='gigachat.devices.sberbank.ru', port=443) at 0x7fecdf6a38d0>, 'Connection to gigachat.devices.sberbank.ru timed out. (connect timeout=60)'))
[19.01.2026 14:41] Downloading and parsing paper https://huggingface.co/papers/2601.11087.
[19.01.2026 14:41] Downloading paper 2601.11087 from https://arxiv.org/pdf/2601.11087v1...
[19.01.2026 14:41] Extracting affiliations from text.
[19.01.2026 14:41] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"PhysRVG: Physics-Aware Unified Reinforcement Learning for Video Generative Models Qiyuan Zhang1,2*, Biao Gong2, Shuai Tan2, Zheng Zhang2, Yujun Shen2, Xing Zhu2, Yuyuan Li1, Kelu Yao3, Chunhua Shen1, Changqing Zou1,3 1Zhejiang University 2Ant Group 3Zhejiang Lab 6 2 0 2 6 1 ] . [ 1 7 8 0 1 1 . 1 0 6 2 : r Figure 1. Samples generated by PhysRVG. Our model produces videos with physically plausible rigid body dynamics. Rows14 display four fundamental types of motion addressed in our work, row5 validates the models generalization in out-of-distribution scenarios. "
[19.01.2026 14:41] Response: ['Zhejiang University', 'Ant Group', 'Zhejiang Lab']
[19.01.2026 14:41] Deleting PDF ./assets/pdf/2601.11087.pdf.
[19.01.2026 14:41] Success.
[19.01.2026 14:41] Downloading and parsing paper https://huggingface.co/papers/2601.11514.
[19.01.2026 14:41] Downloading paper 2601.11514 from https://arxiv.org/pdf/2601.11514v1...
[19.01.2026 14:41] Extracting affiliations from text.
[19.01.2026 14:41] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"ShapeR: Robust Conditional 3D Shape Generation from Casual Captures Yawar Siddiqui Duncan Frost Samir Aroudj Armen Avetisyan Henry Howard-Jenkins Daniel DeTone Pierre Moulon Qirui Wu Zhengqin Li Julian Straub Richard Newcombe Jakob Engel Meta Reality Labs Research Simon Fraser University 6 2 0 2 6 1 ] . [ 1 4 1 5 1 1 . 1 0 6 2 : r Figure 1. ShapeR introduces novel approach to metric shape generation. Given an input image sequence, preprocessing extracts perobject metric sparse SLAM points, images, poses, and captions using off-the-shelf methods. rectified flow transformer operating on VecSet latents conditions on these multimodal inputs to generate shape code, which is decoded into the objects mesh. (Right) By applying the model object-centrically to each detected object, we obtain metric reconstruction of the entire scene. "
[19.01.2026 14:41] Response: [
    "Meta Reality Labs Research",
    "Simon Fraser University"
]
[19.01.2026 14:41] Deleting PDF ./assets/pdf/2601.11514.pdf.
[19.01.2026 14:41] Success.
[19.01.2026 14:41] Downloading and parsing paper https://huggingface.co/papers/2601.11354.
[19.01.2026 14:41] Downloading paper 2601.11354 from https://arxiv.org/pdf/2601.11354v1...
[19.01.2026 14:41] Extracting affiliations from text.
[19.01.2026 14:41] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 6 1 ] . [ 1 4 5 3 1 1 . 1 0 6 2 : r AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems Weiyi Wang1,3, Xinchi Chen1,3, Jingjing Gong2,3 Xuanjing Huang1 Xipeng Qiu1,2,3, 1Fudan University 2Shanghai Innovation Institute 3OpenMOSS Team "
[19.01.2026 14:41] Response: ['Fudan University', 'Shanghai Innovation Institute', 'OpenMOSS Team']
[19.01.2026 14:41] Deleting PDF ./assets/pdf/2601.11354.pdf.
[19.01.2026 14:41] Success.
[19.01.2026 14:41] Downloading and parsing paper https://huggingface.co/papers/2601.11227.
[19.01.2026 14:41] Downloading paper 2601.11227 from https://arxiv.org/pdf/2601.11227v1...
[19.01.2026 14:41] Extracting affiliations from text.
[19.01.2026 14:41] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Shaoyang Xu, Wenxuan Zhang* Singapore University of Technology and Design shaoyang_xu@mymail.sutd.edu.sg, wxzhang@sutd.edu.sg 6 2 0 2 6 1 ] . [ 1 7 2 2 1 1 . 1 0 6 2 : r a "
[19.01.2026 14:41] Response: ['Singapore University of Technology and Design']
[19.01.2026 14:41] Deleting PDF ./assets/pdf/2601.11227.pdf.
[19.01.2026 14:41] Success.
[19.01.2026 14:41] Downloading and parsing paper https://huggingface.co/papers/2601.09255.
[19.01.2026 14:41] Downloading paper 2601.09255 from https://arxiv.org/pdf/2601.09255v1...
[19.01.2026 14:41] Extracting affiliations from text.
[19.01.2026 14:41] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"PhyRPR: Training-Free Physics-Constrained Video Generation Yibo Zhao1 Hengjia Li1 Xiaofei He1 Boxi Wu1 1State Key Lab of CAD&CG, Zhejiang University 6 2 0 2 4 1 ] . [ 1 5 5 2 9 0 . 1 0 6 2 : r Fig. 1: Samples produced by our method. (ab) require physical priors, while (cd) emphasize strict motion constraints. Our three-stage pipeline PhyRPR (PhyReasonPhyPlanPhyRefine) decouples physical reasoning from rendering to better satisfy physical constraints while maintaining high-fidelity video quality. AbstractRecent diffusion-based video generation models can synthesize visually plausible videos, yet they often struggle to satisfy physical constraints. key reason is that most existing approaches remain single-stage: they entangle high-level physical understanding with low-level visual synthesis, making it hard to generate content that require explicit physical reasoning. To address this limitation, we propose training-free threestage pipeline, PhyRPR: PhyReasonPhyPlanPhyRefine, which decouples physical understanding from visual synthesis. Specifically, PhyReason uses large multimodal model for physical state reasoning and an image generator for keyframe synthesis; PhyPlan deterministically synthesizes controllable coarse motion scaffold; and PhyRefine injects this scaffold into diffusion sampling via latent fusion strategy to refine appearance while preserving the planned dynamics. This staged design enables explicit physical control during generation. Extensive experiments under physics constraints show that our method consistently improves physical plausibility and motion controllability. Index TermsDiffusion Model, Reasoning video generation I. INTRODUCTION In recent years, diffusion-based video generation models have made remarkable progress, synthesizing high-fidelity and visually compelling, film-like content. However, despite their impressive visual realism, video diffusion models remain largely correlation-driven: they primarily exploit patterns in large-scale tra"
[19.01.2026 14:41] Response: ['State Key Lab of CAD&CG, Zhejiang University']
[19.01.2026 14:41] Deleting PDF ./assets/pdf/2601.09255.pdf.
[19.01.2026 14:41] Success.
[19.01.2026 14:41] Downloading and parsing paper https://huggingface.co/papers/2601.09001.
[19.01.2026 14:41] Downloading paper 2601.09001 from https://arxiv.org/pdf/2601.09001v1...
[19.01.2026 14:41] Extracting affiliations from text.
[19.01.2026 14:41] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Entropy Sentinel: Continuous LLM Accuracy Monitoring from Decoding Entropy Traces in STEM Departamento de Matematica, FCEyN Universidad de Buenos Aires Buenos Aires, Argentina pedromemolibuffa@uba.ar Luciano Del Corro ELIAS Lab, Departamento de Ingenierá Universidad de San Andres Victoria, Argentina delcorrol@udesa.edu.ar 6 2 0 2 3 1 ] . [ 1 1 0 0 9 0 . 1 0 6 2 : r a "
[19.01.2026 14:41] Response: ['Departamento de Matematica, FCEyN Universidad de Buenos Aires', 'ELIAS Lab, Departamento de Ingenierá Universidad de San Andres']
[19.01.2026 14:41] Deleting PDF ./assets/pdf/2601.09001.pdf.
[19.01.2026 14:41] Success.
[19.01.2026 14:41] Downloading and parsing paper https://huggingface.co/papers/2601.07812.
[19.01.2026 14:41] Downloading paper 2601.07812 from https://arxiv.org/pdf/2601.07812v1...
[19.01.2026 14:41] Extracting affiliations from text.
[19.01.2026 14:41] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 2 1 ] . [ 1 2 1 8 7 0 . 1 0 6 2 : r More Images, More Problems? Controlled Analysis of VLM Failure Modes. Anurag Das1*, Adrian Bulat2,3, Alberto Baldrati2, Ioannis Maniadis Metaxas2 Bernt Schiele1, Georgios Tzimiropoulos2,4, Brais Martinez2 1MPI for Informatics, Saarland Informatics Campus 2Samsung AI, Cambridge 3Technical University of Ias, i, Romania 4Queen Mary University of London, UK andas@mpi-inf.mpg.de "
[19.01.2026 14:42] Response: ['MPI for Informatics, Saarland Informatics Campus', 'Samsung AI, Cambridge', 'Technical University of Ias, i, Romania', 'Queen Mary University of London, UK']
[19.01.2026 14:42] Deleting PDF ./assets/pdf/2601.07812.pdf.
[19.01.2026 14:42] Success.
[19.01.2026 14:42] Enriching papers with extra data.
[19.01.2026 14:42] ********************************************************************************
[19.01.2026 14:42] Abstract 0. Group-based reinforcement learning from verifier rewards suffers from biased advantage estimation that underestimates hard prompts and overestimates easy prompts, which is addressed through a history-aware adaptive difficulty weighting method.  					AI-generated summary 				 Reinforcement Learning f...
[19.01.2026 14:42] ********************************************************************************
[19.01.2026 14:42] Abstract 1. The integration of AI agents into economic markets fundamentally alters the landscape of strategic interaction. We investigate the economic implications of expanding the set of available technologies in three canonical game-theoretic settings: bargaining (resource division), negotiation (asymmetric ...
[19.01.2026 14:42] ********************************************************************************
[19.01.2026 14:42] Abstract 2. A text-based data synthesis approach generates multi-turn tool-use trajectories for large language models, achieving improved performance and reduced computational costs through a specialized trajectory synthesizer.  					AI-generated summary 				 Enabling Large Language Models (LLMs) to effectively...
[19.01.2026 14:42] ********************************************************************************
[19.01.2026 14:42] Abstract 3. RLVR has advanced reasoning capabilities but struggles with open-ended generation due to lack of ground truth; this work proposes an automated rubric generation framework and dataset to improve performance in health reasoning benchmarks.  					AI-generated summary 				 Reinforcement Learning with Ve...
[19.01.2026 14:42] ********************************************************************************
[19.01.2026 14:42] Abstract 4. Personalized large language models can generate false information aligned with user history instead of factual truth, but a new method called FPPS helps maintain both factual accuracy and personalized responses while preserving existing personalization effects.  					AI-generated summary 				 Person...
[19.01.2026 14:42] ********************************************************************************
[19.01.2026 14:42] Abstract 5. Vision-Language-Action models are enhanced by incorporating action-space reasoning through a structured sequence of coarse action intents, improving manipulation task performance in both simulation and real-world environments.  					AI-generated summary 				 Vision-Language-Action (VLA) models have ...
[19.01.2026 14:42] ********************************************************************************
[19.01.2026 14:42] Abstract 6. Reinforcement learning framework for agentic search that improves reliability by teaching agents to recognize reasoning limits and respond appropriately when evidence is insufficient.  					AI-generated summary 				 RL-based agentic search enables LLMs to solve complex questions via dynamic planning...
[19.01.2026 14:42] ********************************************************************************
[19.01.2026 14:42] Abstract 7. Supervised fine-tuning with multiple references addresses overfitting to non-core expressions by masking low-probability tokens based on their semantic importance.  					AI-generated summary 				 Supervised fine-tuning (SFT) is a fundamental post-training strategy to align Large Language Models (LLM...
[19.01.2026 14:42] ********************************************************************************
[19.01.2026 14:42] Abstract 8. A diffusion-based framework generates human motion from text prompts with fine-grained part-level control using a newly constructed dataset with atomic, temporally-aware annotations.  					AI-generated summary 				 Human motion generation from text prompts has made remarkable progress in recent year...
[19.01.2026 14:42] ********************************************************************************
[19.01.2026 14:42] Abstract 9. Reasoning models demonstrate enhanced performance through multi-agent-like interactions that create diverse cognitive perspectives and improve problem-solving through structured social organization.  					AI-generated summary 				 Large language models have achieved remarkable capabilities across do...
[19.01.2026 14:42] ********************************************************************************
[19.01.2026 14:42] Abstract 10. PersonalAlign framework addresses GUI agent alignment with implicit user intents through hierarchical memory organization and long-term record reasoning, improving both execution and proactive performance.  					AI-generated summary 				 While GUI agents have shown strong performance under explicit ...
[19.01.2026 14:42] ********************************************************************************
[19.01.2026 14:42] Abstract 11. Activation probes for language model misuse mitigation face challenges with long-context generalization, requiring new architectures and diverse training for robust performance across production shifts.  					AI-generated summary 				 Frontier language model capabilities are improving rapidly. We th...
[19.01.2026 14:42] ********************************************************************************
[19.01.2026 14:42] Abstract 12. A physics-aware reinforcement learning paradigm is introduced for video generation that enforces physical collision rules directly in high-dimensional spaces, ensuring strict application of physics knowledge rather than treating it as conditional constraints.  					AI-generated summary 				 Physical...
[19.01.2026 14:42] ********************************************************************************
[19.01.2026 14:42] Abstract 13. ShapeR generates high-fidelity 3D shapes from casual image sequences using visual-inertial SLAM, 3D detection, and vision-language models with rectified flow transformer conditioning.  					AI-generated summary 				 Recent advances in 3D shape generation have achieved impressive results, but most ex...
[19.01.2026 14:42] ********************************************************************************
[19.01.2026 14:42] Abstract 14. Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained rea...
[19.01.2026 14:42] ********************************************************************************
[19.01.2026 14:42] Abstract 15. Controlling the language of thought in large language models increases output diversity by leveraging distinct thinking spaces across different languages, with mixed-language sampling providing superior results.  					AI-generated summary 				 Output diversity is crucial for Large Language Models as...
[19.01.2026 14:42] ********************************************************************************
[19.01.2026 14:42] Abstract 16. A three-stage pipeline decouples physical reasoning from visual synthesis in video generation, improving physical plausibility and motion controllability through distinct phases of reasoning, planning, and refinement.  					AI-generated summary 				 Recent diffusion-based video generation models can...
[19.01.2026 14:42] ********************************************************************************
[19.01.2026 14:42] Abstract 17. Output-entropy profiles computed from final-layer next-token probabilities serve as a scalable signal for monitoring LLM performance and prioritizing data acquisition under domain shifts.  					AI-generated summary 				 Deploying LLMs raises two coupled challenges: (1) monitoring - estimating where ...
[19.01.2026 14:42] ********************************************************************************
[19.01.2026 14:42] Abstract 18. Large Vision Language Models exhibit significant limitations in multi-image understanding and reasoning, which are revealed through a new benchmark and addressed via procedural data generation and attention masking techniques.  					AI-generated summary 				 Large Vision Language Models (LVLMs) have...
[19.01.2026 14:42] Read previous papers.
[19.01.2026 14:42] Generating reviews via LLM API.
[19.01.2026 14:42] Querying the API.
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Group-based reinforcement learning from verifier rewards suffers from biased advantage estimation that underestimates hard prompts and overestimates easy prompts, which is addressed through a history-aware adaptive difficulty weighting method.  					AI-generated summary 				 Reinforcement Learning from Verifier Rewards (RLVR) has emerged as a widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood.   In this work, we uncover a fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide the first theoretical analysis showing that it systematically underestimates advantages for hard prompts and overestimates them for easy prompts, leading to imbalanced exploration and exploitation. To address this issue, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on an evolving difficulty anchor and training dynamics. Both theoretical analysis and experiments on five mathematical reasoning benchmarks demonstrate that HA-DW consistently improves performance when integrated into GRPO and its variants. Our results suggest that correcting biased advantage estimation is critical for robust and efficient RLVR training.
[19.01.2026 14:42] Response: {
  "desc": "В данной работе рассматривается проблема смещенной оценки преимущества в методах группового подкрепления с использованием верификаторов вознаграждений. Показано, что такая оценка систематически недооценивает сложные задачи и переоценивает простые, что приводит к неэффективному процессу исследования и эксплуатации. Для решения этой проблемы предложен метод адаптивного взвешивания сложности на основе истории обучения (HA-DW), который корректирует смещения оценки преимуществ и улучшает общую эффективность обучения подкреплению.",
  "emoji": "⚙️",
  "title": "Исправление смещений оценки преимуществ в обучении подкреплению с помощью верификаторов вознаграждений через адаптивное взвешивание сложности"
}
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension
OPEN_SOURCE: Papers that contribute to open-source projects
SCIENCE: Papers on scientific applications of ML
LOW_RESOURCE: Papers mentioning low-resource settings or languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Group-based reinforcement learning from verifier rewards suffers from biased advantage estimation that underestimates hard prompts and overestimates easy prompts, which is addressed through a history-aware adaptive difficulty weighting method.  					AI-generated summary 				 Reinforcement Learning from Verifier Rewards (RLVR) has emerged as a widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood.   In this work, we uncover a fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide the first theoretical analysis showing that it systematically underestimates advantages for hard prompts and overestimates them for easy prompts, leading to imbalanced exploration and exploitation. To address this issue, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on an evolving difficulty anchor and training dynamics. Both theoretical analysis and experiments on five mathematical reasoning benchmarks demonstrate that HA-DW consistently improves performance when integrated into GRPO and its variants. Our results suggest that correcting biased advantage estimation is critical for robust and efficient RLVR training."

[19.01.2026 14:42] Response: [
    "RL",
    "REINFORCEMENT_LEARNING"
]
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Group-based reinforcement learning from verifier rewards suffers from biased advantage estimation that underestimates hard prompts and overestimates easy prompts, which is addressed through a history-aware adaptive difficulty weighting method.  					AI-generated summary 				 Reinforcement Learning from Verifier Rewards (RLVR) has emerged as a widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood.   In this work, we uncover a fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide the first theoretical analysis showing that it systematically underestimates advantages for hard prompts and overestimates them for easy prompts, leading to imbalanced exploration and exploitation. To address this issue, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on an evolving difficulty anchor and training dynamics. Both theoretical analysis and experiments on five mathematical reasoning benchmarks demonstrate that HA-DW consistently improves performance when integrated into GRPO and its variants. Our results suggest that correcting biased advantage estimation is critical for robust and efficient RLVR training."

[19.01.2026 14:42] Response: [
    "REASONING",
    "OPTIMIZATION"
]
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper (4 sentences), use correct machine learning terms. 'title': a slogan of a main idea of the article. Return only JSON and nothing else.

Group-based reinforcement learning from verifier rewards suffers from biased advantage estimation that underestimates hard prompts and overestimates easy prompts, which is addressed through a history-aware adaptive difficulty weighting method.  					AI-generated summary 				 Reinforcement Learning from Verifier Rewards (RLVR) has emerged as a widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood.   In this work, we uncover a fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide the first theoretical analysis showing that it systematically underestimates advantages for hard prompts and overestimates them for easy prompts, leading to imbalanced exploration and exploitation. To address this issue, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on an evolving difficulty anchor and training dynamics. Both theoretical analysis and experiments on five mathematical reasoning benchmarks demonstrate that HA-DW consistently improves performance when integrated into GRPO and its variants. Our results suggest that correcting biased advantage estimation is critical for robust and efficient RLVR training.

Return JSON that matches this schema: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object"}. Respond with JSON only.
[19.01.2026 14:42] Response: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object", "title": "History-Aware Adaptive Difficulty Weighting for Group-Based Reinforcement Learning", "desc": "This paper addresses the bias in advantage estimation in group-based reinforcement learning from verifier rewards by introducing History-Aware Adaptive Difficulty Weighting (HA-DW). The method adjusts advantage estimates dynamically using an evolving difficulty anchor and training dynamics, improving performance across various mathematical reasoning benchmarks."}
[19.01.2026 14:42] Querying the API.
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

The integration of AI agents into economic markets fundamentally alters the landscape of strategic interaction. We investigate the economic implications of expanding the set of available technologies in three canonical game-theoretic settings: bargaining (resource division), negotiation (asymmetric information trade), and persuasion (strategic information transmission). We find that simply increasing the choice of AI delegates can drastically shift equilibrium payoffs and regulatory outcomes, often creating incentives for regulators to proactively develop and release technologies. Conversely, we identify a strategic phenomenon termed the "Poisoned Apple" effect: an agent may release a new technology, which neither they nor their opponent ultimately uses, solely to manipulate the regulator's choice of market design in their favor. This strategic release improves the releaser's welfare at the expense of their opponent and the regulator's fairness objectives. Our findings demonstrate that static regulatory frameworks are vulnerable to manipulation via technology expansion, necessitating dynamic market designs that adapt to the evolving landscape of AI capabilities.
[19.01.2026 14:42] Response: {
  "desc": "В статье исследуется влияние внедрения искусственных интеллектуальных агентов в экономические рынки на стратегическое взаимодействие участников. Рассматриваются три классические игровые теоретические ситуации: ведение переговоров по разделению ресурсов, переговоры при асимметричной информации и передача стратегической информации. Установлено, что расширение набора доступных технологий существенно меняет равновесные выплаты и регулятивные результаты, создавая стимулы для регуляторов разрабатывать и выпускать новые технологии. Также обнаружен феномен «ядовитого яблока», когда агент выпускает новую технологию исключительно с целью манипуляции выбором регуляторной структуры рынка в свою пользу, ухудшая положение своего оппонента и регуляторных целей справедливости. Авторы подчеркивают необходимость динамических рыночных конструкций, способных адаптироваться к растущим возможностям искусственного интеллекта.",
  "emoji": "🤖",
  "title": "Влияние ИИ-агентов на экономику и стратегическое взаимодействие"
}
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension
OPEN_SOURCE: Papers that contribute to open-source projects
SCIENCE: Papers on scientific applications of ML
LOW_RESOURCE: Papers mentioning low-resource settings or languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The integration of AI agents into economic markets fundamentally alters the landscape of strategic interaction. We investigate the economic implications of expanding the set of available technologies in three canonical game-theoretic settings: bargaining (resource division), negotiation (asymmetric information trade), and persuasion (strategic information transmission). We find that simply increasing the choice of AI delegates can drastically shift equilibrium payoffs and regulatory outcomes, often creating incentives for regulators to proactively develop and release technologies. Conversely, we identify a strategic phenomenon termed the "Poisoned Apple" effect: an agent may release a new technology, which neither they nor their opponent ultimately uses, solely to manipulate the regulator's choice of market design in their favor. This strategic release improves the releaser's welfare at the expense of their opponent and the regulator's fairness objectives. Our findings demonstrate that static regulatory frameworks are vulnerable to manipulation via technology expansion, necessitating dynamic market designs that adapt to the evolving landscape of AI capabilities."

[19.01.2026 14:42] Response: [
    AGENTS,
    ROBOTICS,
    CONTINUAL_LEARNING
]
[19.01.2026 14:42] Error. Failed to parse JSON from LLM. [
    AGENTS,
    ROBOTICS,
    CONTINUAL_LEARNING
]
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The integration of AI agents into economic markets fundamentally alters the landscape of strategic interaction. We investigate the economic implications of expanding the set of available technologies in three canonical game-theoretic settings: bargaining (resource division), negotiation (asymmetric information trade), and persuasion (strategic information transmission). We find that simply increasing the choice of AI delegates can drastically shift equilibrium payoffs and regulatory outcomes, often creating incentives for regulators to proactively develop and release technologies. Conversely, we identify a strategic phenomenon termed the "Poisoned Apple" effect: an agent may release a new technology, which neither they nor their opponent ultimately uses, solely to manipulate the regulator's choice of market design in their favor. This strategic release improves the releaser's welfare at the expense of their opponent and the regulator's fairness objectives. Our findings demonstrate that static regulatory frameworks are vulnerable to manipulation via technology expansion, necessitating dynamic market designs that adapt to the evolving landscape of AI capabilities."

[19.01.2026 14:42] Response: [
    'GAMES',
    'ETHICS'
]
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper (4 sentences), use correct machine learning terms. 'title': a slogan of a main idea of the article. Return only JSON and nothing else.

The integration of AI agents into economic markets fundamentally alters the landscape of strategic interaction. We investigate the economic implications of expanding the set of available technologies in three canonical game-theoretic settings: bargaining (resource division), negotiation (asymmetric information trade), and persuasion (strategic information transmission). We find that simply increasing the choice of AI delegates can drastically shift equilibrium payoffs and regulatory outcomes, often creating incentives for regulators to proactively develop and release technologies. Conversely, we identify a strategic phenomenon termed the "Poisoned Apple" effect: an agent may release a new technology, which neither they nor their opponent ultimately uses, solely to manipulate the regulator's choice of market design in their favor. This strategic release improves the releaser's welfare at the expense of their opponent and the regulator's fairness objectives. Our findings demonstrate that static regulatory frameworks are vulnerable to manipulation via technology expansion, necessitating dynamic market designs that adapt to the evolving landscape of AI capabilities.

Return JSON that matches this schema: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object"}. Respond with JSON only.
[19.01.2026 14:42] Response: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object", "description": "Explanation of the paper's main ideas using correct machine learning terms", "example": {"desc": "This paper explores how integrating AI agents into economic markets changes strategic interactions in bargaining, negotiation, and persuasion scenarios. It finds that expanding the set of available AI technologies significantly impacts equilibrium payoffs and regulatory outcomes, leading to potential manipulation through the 'Poisoned Apple' effect where one party releases a technology to influence market design in their favor.", "title": "AI Technology Expansion and Strategic Manipulation in Economic Markets"}}
[19.01.2026 14:42] Querying the API.
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A text-based data synthesis approach generates multi-turn tool-use trajectories for large language models, achieving improved performance and reduced computational costs through a specialized trajectory synthesizer.  					AI-generated summary 				 Enabling Large Language Models (LLMs) to effectively utilize tools in multi-turn interactions is essential for building capable autonomous agents. However, acquiring diverse and realistic multi-turn tool-use data remains a significant challenge. In this work, we propose a novel text-based paradigm. We observe that textual corpora naturally contain rich, multi-step problem-solving experiences, which can serve as an untapped, scalable, and authentic data source for multi-turn tool-use tasks. Based on this insight, we introduce GEM, a data synthesis pipeline that enables the generation and extraction of multi-turn tool-use trajectories from text corpora through a four-stage process: relevance filtering, workflow & tool extraction, trajectory grounding, and complexity refinement. To reduce the computational cost, we further train a specialized Trajectory Synthesizer via supervised fine-tuning. This model distills the complex generation pipeline into an efficient, end-to-end trajectory generator. Experiments demonstrate that our GEM-32B achieve a 16.5% improvement on the BFCL V3 Multi-turn benchmark. Our models partially surpass the performance of models trained on τ - bench (Airline and Retail) in-domain data, highlighting the superior generalization capability derived from our text-based synthesis paradigm. Notably, our Trajectory Synthesizer matches the quality of the full pipeline while significantly reducing inference latency and costs.
[19.01.2026 14:42] Response: {
  "desc": "В данной работе представлен подход на основе текстовых данных для синтеза многотurnовых траекторий использования инструментов для больших языковых моделей (LLM). Предложенная методология позволяет повысить эффективность и снизить вычислительные затраты за счет специализированного синтезатора траекторий. Посредством четырехэтапного процесса фильтрации релевантности, выделения рабочих процессов и инструментов, привязки траектории к реальным данным и уточнения сложности, достигается улучшение результатов на известных бенчмарках. В частности, модель GEM-32B демонстрирует увеличение производительности на 16.5%, а использование специального синтезатора траекторий обеспечивает сопоставимую по качеству генерацию при значительном сокращении времени обработки и затрат.",
  "emoji": "📚",
  "title": "Синтез многотurnовых траекторий инструмента для больших языковых моделей на основе текстовых данных"
}
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension
OPEN_SOURCE: Papers that contribute to open-source projects
SCIENCE: Papers on scientific applications of ML
LOW_RESOURCE: Papers mentioning low-resource settings or languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A text-based data synthesis approach generates multi-turn tool-use trajectories for large language models, achieving improved performance and reduced computational costs through a specialized trajectory synthesizer.  					AI-generated summary 				 Enabling Large Language Models (LLMs) to effectively utilize tools in multi-turn interactions is essential for building capable autonomous agents. However, acquiring diverse and realistic multi-turn tool-use data remains a significant challenge. In this work, we propose a novel text-based paradigm. We observe that textual corpora naturally contain rich, multi-step problem-solving experiences, which can serve as an untapped, scalable, and authentic data source for multi-turn tool-use tasks. Based on this insight, we introduce GEM, a data synthesis pipeline that enables the generation and extraction of multi-turn tool-use trajectories from text corpora through a four-stage process: relevance filtering, workflow & tool extraction, trajectory grounding, and complexity refinement. To reduce the computational cost, we further train a specialized Trajectory Synthesizer via supervised fine-tuning. This model distills the complex generation pipeline into an efficient, end-to-end trajectory generator. Experiments demonstrate that our GEM-32B achieve a 16.5% improvement on the BFCL V3 Multi-turn benchmark. Our models partially surpass the performance of models trained on τ - bench (Airline and Retail) in-domain data, highlighting the superior generalization capability derived from our text-based synthesis paradigm. Notably, our Trajectory Synthesizer matches the quality of the full pipeline while significantly reducing inference latency and costs."

[19.01.2026 14:42] Response: [
    "TOOLS",
    "MULTITURN",
    "AGENT_MEMORY",
    "DATASET",
    "TRAINING"
]
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A text-based data synthesis approach generates multi-turn tool-use trajectories for large language models, achieving improved performance and reduced computational costs through a specialized trajectory synthesizer.  					AI-generated summary 				 Enabling Large Language Models (LLMs) to effectively utilize tools in multi-turn interactions is essential for building capable autonomous agents. However, acquiring diverse and realistic multi-turn tool-use data remains a significant challenge. In this work, we propose a novel text-based paradigm. We observe that textual corpora naturally contain rich, multi-step problem-solving experiences, which can serve as an untapped, scalable, and authentic data source for multi-turn tool-use tasks. Based on this insight, we introduce GEM, a data synthesis pipeline that enables the generation and extraction of multi-turn tool-use trajectories from text corpora through a four-stage process: relevance filtering, workflow & tool extraction, trajectory grounding, and complexity refinement. To reduce the computational cost, we further train a specialized Trajectory Synthesizer via supervised fine-tuning. This model distills the complex generation pipeline into an efficient, end-to-end trajectory generator. Experiments demonstrate that our GEM-32B achieve a 16.5% improvement on the BFCL V3 Multi-turn benchmark. Our models partially surpass the performance of models trained on τ - bench (Airline and Retail) in-domain data, highlighting the superior generalization capability derived from our text-based synthesis paradigm. Notably, our Trajectory Synthesizer matches the quality of the full pipeline while significantly reducing inference latency and costs."

[19.01.2026 14:42] Response: [
    "TOOL_USE",
    "CONTEXT_WINDOW",
    "MEMORY"
]
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper (4 sentences), use correct machine learning terms. 'title': a slogan of a main idea of the article. Return only JSON and nothing else.

A text-based data synthesis approach generates multi-turn tool-use trajectories for large language models, achieving improved performance and reduced computational costs through a specialized trajectory synthesizer.  					AI-generated summary 				 Enabling Large Language Models (LLMs) to effectively utilize tools in multi-turn interactions is essential for building capable autonomous agents. However, acquiring diverse and realistic multi-turn tool-use data remains a significant challenge. In this work, we propose a novel text-based paradigm. We observe that textual corpora naturally contain rich, multi-step problem-solving experiences, which can serve as an untapped, scalable, and authentic data source for multi-turn tool-use tasks. Based on this insight, we introduce GEM, a data synthesis pipeline that enables the generation and extraction of multi-turn tool-use trajectories from text corpora through a four-stage process: relevance filtering, workflow & tool extraction, trajectory grounding, and complexity refinement. To reduce the computational cost, we further train a specialized Trajectory Synthesizer via supervised fine-tuning. This model distills the complex generation pipeline into an efficient, end-to-end trajectory generator. Experiments demonstrate that our GEM-32B achieve a 16.5% improvement on the BFCL V3 Multi-turn benchmark. Our models partially surpass the performance of models trained on τ - bench (Airline and Retail) in-domain data, highlighting the superior generalization capability derived from our text-based synthesis paradigm. Notably, our Trajectory Synthesizer matches the quality of the full pipeline while significantly reducing inference latency and costs.

Return JSON that matches this schema: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object"}. Respond with JSON only.
[19.01.2026 14:42] Response: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object", "desc": "This paper introduces GEM, a text-based data synthesis pipeline designed to generate multi-turn tool-use trajectories for large language models. It leverages textual corpora to extract and refine multi-step problem-solving experiences, enabling efficient and scalable training. A specialized Trajectory Synthesizer reduces computational costs by distilling the complex generation pipeline into an end-to-end model, resulting in improved performance and lower inference latency compared to traditional methods.", "title": "Text-Based Data Synthesis for Efficient Tool-Use Trajectories in LLM Multi-Turn Interactions"}
[19.01.2026 14:42] Querying the API.
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

RLVR has advanced reasoning capabilities but struggles with open-ended generation due to lack of ground truth; this work proposes an automated rubric generation framework and dataset to improve performance in health reasoning benchmarks.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has driven substantial progress in reasoning-intensive domains like mathematics. However, optimizing open-ended generation remains challenging due to the lack of ground truth. While rubric-based evaluation offers a structured proxy for verification, existing methods suffer from scalability bottlenecks and coarse criteria, resulting in a supervision ceiling effect. To address this, we propose an automated Coarse-to-Fine Rubric Generation framework. By synergizing principle-guided synthesis, multi-model aggregation, and difficulty evolution, our approach produces comprehensive and highly discriminative criteria capable of capturing the subtle nuances. Based on this framework, we introduce RubricHub, a large-scale (sim110k) and multi-domain dataset. We validate its utility through a two-stage post-training pipeline comprising Rubric-based Rejection Sampling Fine-Tuning (RuFT) and Reinforcement Learning (RuRL). Experimental results demonstrate that RubricHub unlocks significant performance gains: our post-trained Qwen3-14B achieves state-of-the-art (SOTA) results on HealthBench (69.3), surpassing proprietary frontier models such as GPT-5. The code and data will be released soon.
[19.01.2026 14:42] Response: {
  "desc": "В данной работе рассматривается проблема отсутствия эталонных данных для оценки качества генерации в задачах с открытым концом у моделей машинного обучения на основе RLVR. Предлагается автоматизированная система создания рубрик и набор данных RubricHub для улучшения результатов в медицинских задачах. Используя подход координированной синтеза принципов, объединения нескольких моделей и эволюции сложности, разработанная методика позволяет создавать детализированные критерии оценки, которые значительно повышают качество решений.",
  "emoji": "📚",
  "title": "Автоматизация создания оценочных шкал для повышения эффективности систем искусственного интеллекта в медицинских задачах"
}
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension
OPEN_SOURCE: Papers that contribute to open-source projects
SCIENCE: Papers on scientific applications of ML
LOW_RESOURCE: Papers mentioning low-resource settings or languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"RLVR has advanced reasoning capabilities but struggles with open-ended generation due to lack of ground truth; this work proposes an automated rubric generation framework and dataset to improve performance in health reasoning benchmarks.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has driven substantial progress in reasoning-intensive domains like mathematics. However, optimizing open-ended generation remains challenging due to the lack of ground truth. While rubric-based evaluation offers a structured proxy for verification, existing methods suffer from scalability bottlenecks and coarse criteria, resulting in a supervision ceiling effect. To address this, we propose an automated Coarse-to-Fine Rubric Generation framework. By synergizing principle-guided synthesis, multi-model aggregation, and difficulty evolution, our approach produces comprehensive and highly discriminative criteria capable of capturing the subtle nuances. Based on this framework, we introduce RubricHub, a large-scale (sim110k) and multi-domain dataset. We validate its utility through a two-stage post-training pipeline comprising Rubric-based Rejection Sampling Fine-Tuning (RuFT) and Reinforcement Learning (RuRL). Experimental results demonstrate that RubricHub unlocks significant performance gains: our post-trained Qwen3-14B achieves state-of-the-art (SOTA) results on HealthBench (69.3), surpassing proprietary frontier models such as GPT-5. The code and data will be released soon."

[19.01.2026 14:42] Response: [
    "DATASET",
    "BENCHMARK",
    "RETRIEVAL",
    "HEALTHCARE"
]
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"RLVR has advanced reasoning capabilities but struggles with open-ended generation due to lack of ground truth; this work proposes an automated rubric generation framework and dataset to improve performance in health reasoning benchmarks.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has driven substantial progress in reasoning-intensive domains like mathematics. However, optimizing open-ended generation remains challenging due to the lack of ground truth. While rubric-based evaluation offers a structured proxy for verification, existing methods suffer from scalability bottlenecks and coarse criteria, resulting in a supervision ceiling effect. To address this, we propose an automated Coarse-to-Fine Rubric Generation framework. By synergizing principle-guided synthesis, multi-model aggregation, and difficulty evolution, our approach produces comprehensive and highly discriminative criteria capable of capturing the subtle nuances. Based on this framework, we introduce RubricHub, a large-scale (sim110k) and multi-domain dataset. We validate its utility through a two-stage post-training pipeline comprising Rubric-based Rejection Sampling Fine-Tuning (RuFT) and Reinforcement Learning (RuRL). Experimental results demonstrate that RubricHub unlocks significant performance gains: our post-trained Qwen3-14B achieves state-of-the-art (SOTA) results on HealthBench (69.3), surpassing proprietary frontier models such as GPT-5. The code and data will be released soon."

[19.01.2026 14:42] Response: [
    "REASONING",
    "TRANSFER_LEARNING"
]
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper (4 sentences), use correct machine learning terms. 'title': a slogan of a main idea of the article. Return only JSON and nothing else.

RLVR has advanced reasoning capabilities but struggles with open-ended generation due to lack of ground truth; this work proposes an automated rubric generation framework and dataset to improve performance in health reasoning benchmarks.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has driven substantial progress in reasoning-intensive domains like mathematics. However, optimizing open-ended generation remains challenging due to the lack of ground truth. While rubric-based evaluation offers a structured proxy for verification, existing methods suffer from scalability bottlenecks and coarse criteria, resulting in a supervision ceiling effect. To address this, we propose an automated Coarse-to-Fine Rubric Generation framework. By synergizing principle-guided synthesis, multi-model aggregation, and difficulty evolution, our approach produces comprehensive and highly discriminative criteria capable of capturing the subtle nuances. Based on this framework, we introduce RubricHub, a large-scale (sim110k) and multi-domain dataset. We validate its utility through a two-stage post-training pipeline comprising Rubric-based Rejection Sampling Fine-Tuning (RuFT) and Reinforcement Learning (RuRL). Experimental results demonstrate that RubricHub unlocks significant performance gains: our post-trained Qwen3-14B achieves state-of-the-art (SOTA) results on HealthBench (69.3), surpassing proprietary frontier models such as GPT-5. The code and data will be released soon.

Return JSON that matches this schema: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object"}. Respond with JSON only.
[19.01.2026 14:42] Response: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object", "description": "JSON object outlining the abstract of a Machine Learning paper.", "desc": "This paper addresses the limitations of Reinforcement Learning with Verifiable Rewards (RLVR) in open-ended generation by proposing an automated rubric generation framework and dataset. It introduces a Coarse-to-Fine Rubric Generation method combining principle-guided synthesis, multi-model aggregation, and difficulty evolution. This framework is applied to create RubricHub, a large-scale dataset used for fine-tuning and reinforcement learning. The experimental results show significant improvements in performance on HealthBench, achieving state-of-the-art results.", "title": "Automated Rubric Generation Framework for Improved Open-Ended Reasoning"}
[19.01.2026 14:42] Querying the API.
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Personalized large language models can generate false information aligned with user history instead of factual truth, but a new method called FPPS helps maintain both factual accuracy and personalized responses while preserving existing personalization effects.  					AI-generated summary 				 Personalized large language models (LLMs) adapt model behavior to individual users to enhance user satisfaction, yet personalization can inadvertently distort factual reasoning. We show that when personalized LLMs face factual queries, there exists a phenomenon where the model generates answers aligned with a user's prior history rather than the objective truth, resulting in personalization-induced hallucinations that degrade factual reliability and may propagate incorrect beliefs, due to representational entanglement between personalization and factual representations. To address this issue, we propose Factuality-Preserving Personalized Steering (FPPS), a lightweight inference-time approach that mitigates personalization-induced factual distortions while preserving personalized behavior. We further introduce PFQABench, the first benchmark designed to jointly evaluate factual and personalized question answering under personalization. Experiments across multiple LLM backbones and personalization methods show that FPPS substantially improves factual accuracy while maintaining personalized performance.
[19.01.2026 14:42] Response: {
  "desc": "Данная работа исследует проблему искажения фактов в персонализированных больших языковых моделях (LLM), которые адаптируются под индивидуальные предпочтения пользователей, что может приводить к генерации ложной информации, соответствующей истории пользователя, а не фактам. Предложенный метод FPPS позволяет сохранять как точность фактов, так и персональные настройки модели без значительных потерь эффективности. Для оценки разработанного подхода был создан новый набор данных PFQABench, объединяющий оценку точности фактов и персональных характеристик ответов.",
  "emoji": "📚",
  "title": "FPPS: Сохранение Фактической Точности при Персонализации Большими Языковыми Моделями"
}
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension
OPEN_SOURCE: Papers that contribute to open-source projects
SCIENCE: Papers on scientific applications of ML
LOW_RESOURCE: Papers mentioning low-resource settings or languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Personalized large language models can generate false information aligned with user history instead of factual truth, but a new method called FPPS helps maintain both factual accuracy and personalized responses while preserving existing personalization effects.  					AI-generated summary 				 Personalized large language models (LLMs) adapt model behavior to individual users to enhance user satisfaction, yet personalization can inadvertently distort factual reasoning. We show that when personalized LLMs face factual queries, there exists a phenomenon where the model generates answers aligned with a user's prior history rather than the objective truth, resulting in personalization-induced hallucinations that degrade factual reliability and may propagate incorrect beliefs, due to representational entanglement between personalization and factual representations. To address this issue, we propose Factuality-Preserving Personalized Steering (FPPS), a lightweight inference-time approach that mitigates personalization-induced factual distortions while preserving personalized behavior. We further introduce PFQABench, the first benchmark designed to jointly evaluate factual and personalized question answering under personalization. Experiments across multiple LLM backbones and personalization methods show that FPPS substantially improves factual accuracy while maintaining personalized performance."

[19.01.2026 14:42] Response: [
    "DATA",
    "BENCHMARK",
    "INFERENCE"
]
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Personalized large language models can generate false information aligned with user history instead of factual truth, but a new method called FPPS helps maintain both factual accuracy and personalized responses while preserving existing personalization effects.  					AI-generated summary 				 Personalized large language models (LLMs) adapt model behavior to individual users to enhance user satisfaction, yet personalization can inadvertently distort factual reasoning. We show that when personalized LLMs face factual queries, there exists a phenomenon where the model generates answers aligned with a user's prior history rather than the objective truth, resulting in personalization-induced hallucinations that degrade factual reliability and may propagate incorrect beliefs, due to representational entanglement between personalization and factual representations. To address this issue, we propose Factuality-Preserving Personalized Steering (FPPS), a lightweight inference-time approach that mitigates personalization-induced factual distortions while preserving personalized behavior. We further introduce PFQABench, the first benchmark designed to jointly evaluate factual and personalized question answering under personalization. Experiments across multiple LLM backbones and personalization methods show that FPPS substantially improves factual accuracy while maintaining personalized performance."

[19.01.2026 14:42] Response: [
    "HALLUCINATIONS",
    "PERSONALIZATION"
]
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper (4 sentences), use correct machine learning terms. 'title': a slogan of a main idea of the article. Return only JSON and nothing else.

Personalized large language models can generate false information aligned with user history instead of factual truth, but a new method called FPPS helps maintain both factual accuracy and personalized responses while preserving existing personalization effects.  					AI-generated summary 				 Personalized large language models (LLMs) adapt model behavior to individual users to enhance user satisfaction, yet personalization can inadvertently distort factual reasoning. We show that when personalized LLMs face factual queries, there exists a phenomenon where the model generates answers aligned with a user's prior history rather than the objective truth, resulting in personalization-induced hallucinations that degrade factual reliability and may propagate incorrect beliefs, due to representational entanglement between personalization and factual representations. To address this issue, we propose Factuality-Preserving Personalized Steering (FPPS), a lightweight inference-time approach that mitigates personalization-induced factual distortions while preserving personalized behavior. We further introduce PFQABench, the first benchmark designed to jointly evaluate factual and personalized question answering under personalization. Experiments across multiple LLM backbones and personalization methods show that FPPS substantially improves factual accuracy while maintaining personalized performance.

Return JSON that matches this schema: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object"}. Respond with JSON only.
[19.01.2026 14:42] Response: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"},"required": ["desc", "title"], "title": "Article", "type": "object"}, "desc": "This paper addresses the challenge of maintaining factual accuracy in personalized large language models (LLMs). It introduces FPPS, a technique for factuality-preserving personalized steering that ensures factual correctness without compromising personalization effects. The study also presents PFQABench, a benchmark specifically designed to assess factual and personalized question answering. Experimental results demonstrate significant improvements in factual accuracy while retaining personalized performance.", "title": "Factuality-Preserving Personalized Steering for Large Language Models"}
[19.01.2026 14:42] Querying the API.
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Vision-Language-Action models are enhanced by incorporating action-space reasoning through a structured sequence of coarse action intents, improving manipulation task performance in both simulation and real-world environments.  					AI-generated summary 				 Vision-Language-Action (VLA) models have emerged as essential generalist robot policies for diverse manipulation tasks, conventionally relying on directly translating multimodal inputs into actions via Vision-Language Model (VLM) embeddings. Recent advancements have introduced explicit intermediary reasoning, such as sub-task prediction (language) or goal image synthesis (vision), to guide action generation. However, these intermediate reasoning are often indirect and inherently limited in their capacity to convey the full, granular information required for precise action execution. Instead, we posit that the most effective form of reasoning is one that deliberates directly in the action space. We introduce Action Chain-of-Thought (ACoT), a paradigm where the reasoning process itself is formulated as a structured sequence of coarse action intents that guide the final policy. In this paper, we propose ACoT-VLA, a novel architecture that materializes the ACoT paradigm. Specifically, we introduce two complementary components: an Explicit Action Reasoner (EAR) and Implicit Action Reasoner (IAR). The former proposes coarse reference trajectories as explicit action-level reasoning steps, while the latter extracts latent action priors from internal representations of multimodal input, co-forming an ACoT that conditions the downstream action head to enable grounded policy learning. Extensive experiments in real-world and simulation environments demonstrate the superiority of our proposed method, which achieves 98.5%, 84.1%, and 47.4% on LIBERO, LIBERO-Plus and VLABench, respectively.
[19.01.2026 14:42] Response: {
  "desc": "В данной работе предлагается улучшенная модель Vision-Language-Action (VLA), которая включает в себя рассуждения в пространстве действий через структурированную последовательность грубых намерений действий. Это позволяет повысить эффективность выполнения задач манипуляции как в симуляционных, так и в реальных средах. Введены два компонента: Явный Разум Действия (Explicit Action Reasoner — EAR) для формулирования явных шагов действий и Неявный Разум Действия (Implicit Action Reasoner — IAR) для извлечения скрытых предварительных условий действия из мультимодальных входов. Эксперименты показывают значительное улучшение по сравнению с предыдущими методами.",
  "emoji": "🤖",
  "title": "Повышение эффективности моделей VLA за счет рассуждений в пространстве действий"
}
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension
OPEN_SOURCE: Papers that contribute to open-source projects
SCIENCE: Papers on scientific applications of ML
LOW_RESOURCE: Papers mentioning low-resource settings or languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Vision-Language-Action models are enhanced by incorporating action-space reasoning through a structured sequence of coarse action intents, improving manipulation task performance in both simulation and real-world environments.  					AI-generated summary 				 Vision-Language-Action (VLA) models have emerged as essential generalist robot policies for diverse manipulation tasks, conventionally relying on directly translating multimodal inputs into actions via Vision-Language Model (VLM) embeddings. Recent advancements have introduced explicit intermediary reasoning, such as sub-task prediction (language) or goal image synthesis (vision), to guide action generation. However, these intermediate reasoning are often indirect and inherently limited in their capacity to convey the full, granular information required for precise action execution. Instead, we posit that the most effective form of reasoning is one that deliberates directly in the action space. We introduce Action Chain-of-Thought (ACoT), a paradigm where the reasoning process itself is formulated as a structured sequence of coarse action intents that guide the final policy. In this paper, we propose ACoT-VLA, a novel architecture that materializes the ACoT paradigm. Specifically, we introduce two complementary components: an Explicit Action Reasoner (EAR) and Implicit Action Reasoner (IAR). The former proposes coarse reference trajectories as explicit action-level reasoning steps, while the latter extracts latent action priors from internal representations of multimodal input, co-forming an ACoT that conditions the downstream action head to enable grounded policy learning. Extensive experiments in real-world and simulation environments demonstrate the superiority of our proposed method, which achieves 98.5%, 84.1%, and 47.4% on LIBERO, LIBERO-Plus and VLABench, respectively."

[19.01.2026 14:42] Response: [
    "VISION",
    "LANGUAGE",
    "ACTION",
    "MULTIMODAL"
]
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Vision-Language-Action models are enhanced by incorporating action-space reasoning through a structured sequence of coarse action intents, improving manipulation task performance in both simulation and real-world environments.  					AI-generated summary 				 Vision-Language-Action (VLA) models have emerged as essential generalist robot policies for diverse manipulation tasks, conventionally relying on directly translating multimodal inputs into actions via Vision-Language Model (VLM) embeddings. Recent advancements have introduced explicit intermediary reasoning, such as sub-task prediction (language) or goal image synthesis (vision), to guide action generation. However, these intermediate reasoning are often indirect and inherently limited in their capacity to convey the full, granular information required for precise action execution. Instead, we posit that the most effective form of reasoning is one that deliberates directly in the action space. We introduce Action Chain-of-Thought (ACoT), a paradigm where the reasoning process itself is formulated as a structured sequence of coarse action intents that guide the final policy. In this paper, we propose ACoT-VLA, a novel architecture that materializes the ACoT paradigm. Specifically, we introduce two complementary components: an Explicit Action Reasoner (EAR) and Implicit Action Reasoner (IAR). The former proposes coarse reference trajectories as explicit action-level reasoning steps, while the latter extracts latent action priors from internal representations of multimodal input, co-forming an ACoT that conditions the downstream action head to enable grounded policy learning. Extensive experiments in real-world and simulation environments demonstrate the superiority of our proposed method, which achieves 98.5%, 84.1%, and 47.4% on LIBERO, LIBERO-Plus and VLABench, respectively."

[19.01.2026 14:42] Response: [
    "REASONING",
    "ACTION_SPACE_REASONING"
]
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper (4 sentences), use correct machine learning terms. 'title': a slogan of a main idea of the article. Return only JSON and nothing else.

Vision-Language-Action models are enhanced by incorporating action-space reasoning through a structured sequence of coarse action intents, improving manipulation task performance in both simulation and real-world environments.  					AI-generated summary 				 Vision-Language-Action (VLA) models have emerged as essential generalist robot policies for diverse manipulation tasks, conventionally relying on directly translating multimodal inputs into actions via Vision-Language Model (VLM) embeddings. Recent advancements have introduced explicit intermediary reasoning, such as sub-task prediction (language) or goal image synthesis (vision), to guide action generation. However, these intermediate reasoning are often indirect and inherently limited in their capacity to convey the full, granular information required for precise action execution. Instead, we posit that the most effective form of reasoning is one that deliberates directly in the action space. We introduce Action Chain-of-Thought (ACoT), a paradigm where the reasoning process itself is formulated as a structured sequence of coarse action intents that guide the final policy. In this paper, we propose ACoT-VLA, a novel architecture that materializes the ACoT paradigm. Specifically, we introduce two complementary components: an Explicit Action Reasoner (EAR) and Implicit Action Reasoner (IAR). The former proposes coarse reference trajectories as explicit action-level reasoning steps, while the latter extracts latent action priors from internal representations of multimodal input, co-forming an ACoT that conditions the downstream action head to enable grounded policy learning. Extensive experiments in real-world and simulation environments demonstrate the superiority of our proposed method, which achieves 98.5%, 84.1%, and 47.4% on LIBERO, LIBERO-Plus and VLABench, respectively.

Return JSON that matches this schema: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object"}. Respond with JSON only.
[19.01.2026 14:42] Response: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object", "description": "JSON object outlining the key points and title of a Machine Learning paper.", "example": {"desc": "Vision-Language-Action models are enhanced by incorporating action-space reasoning through a structured sequence of coarse action intents, improving manipulation task performance in both simulation and real-world environments.", "title": "Action Chain-of-Thought (ACoT) for Enhanced Vision-Language-Action Models"}},"title": "Article","type": "object","desc": "Vision-Language-Action models are improved by introducing Action Chain-of-Thought (ACoT), which uses structured sequences of coarse action intents to enhance manipulation task performance in both simulated and real-world environments. This approach incorporates an Explicit Action Reasoner (EAR) and an Implicit Action Reasoner (IAR) to condition the downstream action head for more accurate and efficient policy learning.","title": "ACoT-VLA: Structured Action Space Reasoning for Vision-Language-Action Models" }
[19.01.2026 14:42] Error. Failed to parse JSON. Details: Extra data: line 1 column 609 (char 608). Response: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object", "description": "JSON object outlining the key points and title of a Machine Learning paper.", "example": {"desc": "Vision-Language-Action models are enhanced by incorporating action-space reasoning through a structured sequence of coarse action intents, improving manipulation task performance in both simulation and real-world environments.", "title": "Action Chain-of-Thought (ACoT) for Enhanced Vision-Language-Action Models"}},"title": "Article","type": "object","desc": "Vision-Language-Action models are improved by introducing Action Chain-of-Thought (ACoT), which uses structured sequences of coarse action intents to enhance manipulation task performance in both simulated and real-world environments. This approach incorporates an Explicit Action Reasoner (EAR) and an Implicit Action Reasoner (IAR) to condition the downstream action head for more accurate and efficient policy learning.","title": "ACoT-VLA: Structured Action Space Reasoning for Vision-Language-Action Models" }
[19.01.2026 14:42] Querying the API.
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Reinforcement learning framework for agentic search that improves reliability by teaching agents to recognize reasoning limits and respond appropriately when evidence is insufficient.  					AI-generated summary 				 RL-based agentic search enables LLMs to solve complex questions via dynamic planning and external search. While this approach significantly enhances accuracy with agent policies optimized via large-scale reinforcement learning, we identify a critical gap in reliability: these agents fail to recognize their reasoning boundaries and rarely admit ``I DON'T KNOW'' (IDK) even when evidence is insufficient or reasoning reaches its limit. The lack of reliability often leads to plausible but unreliable answers, introducing significant risks in many real-world scenarios. To this end, we propose Boundary-Aware Policy Optimization (BAPO), a novel RL framework designed to cultivate reliable boundary awareness without compromising accuracy. BAPO introduces two key components: (i) a group-based boundary-aware reward that encourages an IDK response only when the reasoning reaches its limit, and (ii) an adaptive reward modulator that strategically suspends this reward during early exploration, preventing the model from exploiting IDK as a shortcut. Extensive experiments on four benchmarks demonstrate that BAPO substantially enhances the overall reliability of agentic search.
[19.01.2026 14:42] Response: {
  "desc": "В данной работе предложен подход на основе подкрепления для агентского поиска, который улучшает надежность за счет обучения агентов распознаванию границ своих рассуждений и адекватному реагированию при недостатке доказательств. Предложенная архитектура, названная Boundary-Aware Policy Optimization (BAPO), включает в себя механизм поощрений, стимулирующий использование ответа «Я НЕ ЗНАЮ» только тогда, когда рассуждения достигают предела своих возможностей, а также модулятор вознаграждения, предотвращающий злоупотребление этим ответом на ранних этапах исследования.",
  "emoji": "🤔",
  "title": "Повышение надежности агентских систем через осознание границ рассуждений"
}
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension
OPEN_SOURCE: Papers that contribute to open-source projects
SCIENCE: Papers on scientific applications of ML
LOW_RESOURCE: Papers mentioning low-resource settings or languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Reinforcement learning framework for agentic search that improves reliability by teaching agents to recognize reasoning limits and respond appropriately when evidence is insufficient.  					AI-generated summary 				 RL-based agentic search enables LLMs to solve complex questions via dynamic planning and external search. While this approach significantly enhances accuracy with agent policies optimized via large-scale reinforcement learning, we identify a critical gap in reliability: these agents fail to recognize their reasoning boundaries and rarely admit ``I DON'T KNOW'' (IDK) even when evidence is insufficient or reasoning reaches its limit. The lack of reliability often leads to plausible but unreliable answers, introducing significant risks in many real-world scenarios. To this end, we propose Boundary-Aware Policy Optimization (BAPO), a novel RL framework designed to cultivate reliable boundary awareness without compromising accuracy. BAPO introduces two key components: (i) a group-based boundary-aware reward that encourages an IDK response only when the reasoning reaches its limit, and (ii) an adaptive reward modulator that strategically suspends this reward during early exploration, preventing the model from exploiting IDK as a shortcut. Extensive experiments on four benchmarks demonstrate that BAPO substantially enhances the overall reliability of agentic search."

[19.01.2026 14:42] Response: [
    "RL",
    "RELIABILITY",
    "BOUNDARY_AWARE_POLICY_OPTIMIZATION"
]
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Reinforcement learning framework for agentic search that improves reliability by teaching agents to recognize reasoning limits and respond appropriately when evidence is insufficient.  					AI-generated summary 				 RL-based agentic search enables LLMs to solve complex questions via dynamic planning and external search. While this approach significantly enhances accuracy with agent policies optimized via large-scale reinforcement learning, we identify a critical gap in reliability: these agents fail to recognize their reasoning boundaries and rarely admit ``I DON'T KNOW'' (IDK) even when evidence is insufficient or reasoning reaches its limit. The lack of reliability often leads to plausible but unreliable answers, introducing significant risks in many real-world scenarios. To this end, we propose Boundary-Aware Policy Optimization (BAPO), a novel RL framework designed to cultivate reliable boundary awareness without compromising accuracy. BAPO introduces two key components: (i) a group-based boundary-aware reward that encourages an IDK response only when the reasoning reaches its limit, and (ii) an adaptive reward modulator that strategically suspends this reward during early exploration, preventing the model from exploiting IDK as a shortcut. Extensive experiments on four benchmarks demonstrate that BAPO substantially enhances the overall reliability of agentic search."

[19.01.2026 14:42] Response: [
    "REASONING",
    "AGI"
]
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper (4 sentences), use correct machine learning terms. 'title': a slogan of a main idea of the article. Return only JSON and nothing else.

Reinforcement learning framework for agentic search that improves reliability by teaching agents to recognize reasoning limits and respond appropriately when evidence is insufficient.  					AI-generated summary 				 RL-based agentic search enables LLMs to solve complex questions via dynamic planning and external search. While this approach significantly enhances accuracy with agent policies optimized via large-scale reinforcement learning, we identify a critical gap in reliability: these agents fail to recognize their reasoning boundaries and rarely admit ``I DON'T KNOW'' (IDK) even when evidence is insufficient or reasoning reaches its limit. The lack of reliability often leads to plausible but unreliable answers, introducing significant risks in many real-world scenarios. To this end, we propose Boundary-Aware Policy Optimization (BAPO), a novel RL framework designed to cultivate reliable boundary awareness without compromising accuracy. BAPO introduces two key components: (i) a group-based boundary-aware reward that encourages an IDK response only when the reasoning reaches its limit, and (ii) an adaptive reward modulator that strategically suspends this reward during early exploration, preventing the model from exploiting IDK as a shortcut. Extensive experiments on four benchmarks demonstrate that BAPO substantially enhances the overall reliability of agentic search.

Return JSON that matches this schema: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object"}. Respond with JSON only.
[19.01.2026 14:42] Response: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object", "description": "JSON object outlining the abstract of the ML paper with explanation and title fields.", "desc": "A Reinforcement Learning framework called Boundary-Aware Policy Optimization (BAPO) is introduced to enhance the reliability of AI agents performing agentic search tasks. It teaches agents to recognize their reasoning limits and respond appropriately when evidence is insufficient, reducing the risk of providing unreliable answers. This framework includes a group-based boundary-aware reward mechanism and an adaptive reward modulator to prevent exploitation of the IDK response as a shortcut.", "title": "Improving Reliability in Agentic Search Through Boundary-Aware Policy Optimization"}
[19.01.2026 14:42] Querying the API.
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Supervised fine-tuning with multiple references addresses overfitting to non-core expressions by masking low-probability tokens based on their semantic importance.  					AI-generated summary 				 Supervised fine-tuning (SFT) is a fundamental post-training strategy to align Large Language Models (LLMs) with human intent. However, traditional SFT often ignores the one-to-many nature of language by forcing alignment with a single reference answer, leading to the model overfitting to non-core expressions. Although our empirical analysis suggests that introducing multiple reference answers can mitigate this issue, the prohibitive data and computational costs necessitate a strategic shift: prioritizing the mitigation of single-reference overfitting over the costly pursuit of answer diversity. To achieve this, we reveal the intrinsic connection between token probability and semantic importance: high-probability tokens carry the core logical framework, while low-probability tokens are mostly replaceable expressions. Based on this insight, we propose ProFit, which selectively masks low-probability tokens to prevent surface-level overfitting. Extensive experiments confirm that ProFit consistently outperforms traditional SFT baselines on general reasoning and mathematical benchmarks.
[19.01.2026 14:42] Response: {
  "desc": "Данная работа исследует проблему избыточной подгонки моделей машинного обучения к второстепенным выражениям при традиционном контролируемом дообучении (SFT). Предложенный метод ProFit решает эту задачу путем маскировки низко вероятностных токенов на основе их семантической значимости, что позволяет модели лучше сохранять ключевые логические структуры и улучшать ее общую производительность.",
  "emoji": "📚",
  "title": "Контролируемое дообучение с несколькими ссылками для предотвращения поверхностной переобученности"
}
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension
OPEN_SOURCE: Papers that contribute to open-source projects
SCIENCE: Papers on scientific applications of ML
LOW_RESOURCE: Papers mentioning low-resource settings or languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Supervised fine-tuning with multiple references addresses overfitting to non-core expressions by masking low-probability tokens based on their semantic importance.  					AI-generated summary 				 Supervised fine-tuning (SFT) is a fundamental post-training strategy to align Large Language Models (LLMs) with human intent. However, traditional SFT often ignores the one-to-many nature of language by forcing alignment with a single reference answer, leading to the model overfitting to non-core expressions. Although our empirical analysis suggests that introducing multiple reference answers can mitigate this issue, the prohibitive data and computational costs necessitate a strategic shift: prioritizing the mitigation of single-reference overfitting over the costly pursuit of answer diversity. To achieve this, we reveal the intrinsic connection between token probability and semantic importance: high-probability tokens carry the core logical framework, while low-probability tokens are mostly replaceable expressions. Based on this insight, we propose ProFit, which selectively masks low-probability tokens to prevent surface-level overfitting. Extensive experiments confirm that ProFit consistently outperforms traditional SFT baselines on general reasoning and mathematical benchmarks."

[19.01.2026 14:42] Response: [
    "TRAINING"
]
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Supervised fine-tuning with multiple references addresses overfitting to non-core expressions by masking low-probability tokens based on their semantic importance.  					AI-generated summary 				 Supervised fine-tuning (SFT) is a fundamental post-training strategy to align Large Language Models (LLMs) with human intent. However, traditional SFT often ignores the one-to-many nature of language by forcing alignment with a single reference answer, leading to the model overfitting to non-core expressions. Although our empirical analysis suggests that introducing multiple reference answers can mitigate this issue, the prohibitive data and computational costs necessitate a strategic shift: prioritizing the mitigation of single-reference overfitting over the costly pursuit of answer diversity. To achieve this, we reveal the intrinsic connection between token probability and semantic importance: high-probability tokens carry the core logical framework, while low-probability tokens are mostly replaceable expressions. Based on this insight, we propose ProFit, which selectively masks low-probability tokens to prevent surface-level overfitting. Extensive experiments confirm that ProFit consistently outperforms traditional SFT baselines on general reasoning and mathematical benchmarks."

[19.01.2026 14:42] Response: [
    "REASONING",
    "OPTIMIZATION"
]
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper (4 sentences), use correct machine learning terms. 'title': a slogan of a main idea of the article. Return only JSON and nothing else.

Supervised fine-tuning with multiple references addresses overfitting to non-core expressions by masking low-probability tokens based on their semantic importance.  					AI-generated summary 				 Supervised fine-tuning (SFT) is a fundamental post-training strategy to align Large Language Models (LLMs) with human intent. However, traditional SFT often ignores the one-to-many nature of language by forcing alignment with a single reference answer, leading to the model overfitting to non-core expressions. Although our empirical analysis suggests that introducing multiple reference answers can mitigate this issue, the prohibitive data and computational costs necessitate a strategic shift: prioritizing the mitigation of single-reference overfitting over the costly pursuit of answer diversity. To achieve this, we reveal the intrinsic connection between token probability and semantic importance: high-probability tokens carry the core logical framework, while low-probability tokens are mostly replaceable expressions. Based on this insight, we propose ProFit, which selectively masks low-probability tokens to prevent surface-level overfitting. Extensive experiments confirm that ProFit consistently outperforms traditional SFT baselines on general reasoning and mathematical benchmarks.

Return JSON that matches this schema: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object"}. Respond with JSON only.
[19.01.2026 14:42] Response: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object", "description": "Explanation of the ML paper in simple terms", "example": {"desc": "Supervised fine-tuning with multiple references addresses overfitting to non-core expressions by masking low-probability tokens based on their semantic importance.", "title": "ProFit: Mitigating Overfitting in Fine-Tuned Language Models"}}
[19.01.2026 14:42] Querying the API.
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A diffusion-based framework generates human motion from text prompts with fine-grained part-level control using a newly constructed dataset with atomic, temporally-aware annotations.  					AI-generated summary 				 Human motion generation from text prompts has made remarkable progress in recent years. However, existing methods primarily rely on either sequence-level or action-level descriptions due to the absence of fine-grained, part-level motion annotations. This limits their controllability over individual body parts. In this work, we construct a high-quality motion dataset with atomic, temporally-aware part-level text annotations, leveraging the reasoning capabilities of large language models (LLMs). Unlike prior datasets that either provide synchronized part captions with fixed time segments or rely solely on global sequence labels, our dataset captures asynchronous and semantically distinct part movements at fine temporal resolution. Based on this dataset, we introduce a diffusion-based part-aware motion generation framework, namely FrankenMotion, where each body part is guided by its own temporally-structured textual prompt. This is, to our knowledge, the first work to provide atomic, temporally-aware part-level motion annotations and have a model that allows motion generation with both spatial (body part) and temporal (atomic action) control. Experiments demonstrate that FrankenMotion outperforms all previous baseline models adapted and retrained for our setting, and our model can compose motions unseen during training. Our code and dataset will be publicly available upon publication.
[19.01.2026 14:42] Response: {
  "desc": "В данной работе предложен диффузионный фреймворк для генерации движений человека на основе текстовых подсказок с высокой степенью детализации и контролем отдельных частей тела. Для этого был создан новый высококачественный набор данных с атомными и временно-зависимыми аннотациями частей тела. На основе данного набора данных разработана модель Франкенномotion, позволяющая управлять движениями как в пространстве, так и во времени, что ранее не реализовывалось в полной мере. Эксперименты показали значительное превосходство новой модели над предыдущими базовыми моделями.",
  "emoji": "👩‍🦶",
  "title": "Франкенменеджмент движений: контроль тела и времени"
}
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension
OPEN_SOURCE: Papers that contribute to open-source projects
SCIENCE: Papers on scientific applications of ML
LOW_RESOURCE: Papers mentioning low-resource settings or languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A diffusion-based framework generates human motion from text prompts with fine-grained part-level control using a newly constructed dataset with atomic, temporally-aware annotations.  					AI-generated summary 				 Human motion generation from text prompts has made remarkable progress in recent years. However, existing methods primarily rely on either sequence-level or action-level descriptions due to the absence of fine-grained, part-level motion annotations. This limits their controllability over individual body parts. In this work, we construct a high-quality motion dataset with atomic, temporally-aware part-level text annotations, leveraging the reasoning capabilities of large language models (LLMs). Unlike prior datasets that either provide synchronized part captions with fixed time segments or rely solely on global sequence labels, our dataset captures asynchronous and semantically distinct part movements at fine temporal resolution. Based on this dataset, we introduce a diffusion-based part-aware motion generation framework, namely FrankenMotion, where each body part is guided by its own temporally-structured textual prompt. This is, to our knowledge, the first work to provide atomic, temporally-aware part-level motion annotations and have a model that allows motion generation with both spatial (body part) and temporal (atomic action) control. Experiments demonstrate that FrankenMotion outperforms all previous baseline models adapted and retrained for our setting, and our model can compose motions unseen during training. Our code and dataset will be publicly available upon publication."

[19.01.2026 14:42] Response: [
    "DATASET",
    "DATA"
]
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A diffusion-based framework generates human motion from text prompts with fine-grained part-level control using a newly constructed dataset with atomic, temporally-aware annotations.  					AI-generated summary 				 Human motion generation from text prompts has made remarkable progress in recent years. However, existing methods primarily rely on either sequence-level or action-level descriptions due to the absence of fine-grained, part-level motion annotations. This limits their controllability over individual body parts. In this work, we construct a high-quality motion dataset with atomic, temporally-aware part-level text annotations, leveraging the reasoning capabilities of large language models (LLMs). Unlike prior datasets that either provide synchronized part captions with fixed time segments or rely solely on global sequence labels, our dataset captures asynchronous and semantically distinct part movements at fine temporal resolution. Based on this dataset, we introduce a diffusion-based part-aware motion generation framework, namely FrankenMotion, where each body part is guided by its own temporally-structured textual prompt. This is, to our knowledge, the first work to provide atomic, temporally-aware part-level motion annotations and have a model that allows motion generation with both spatial (body part) and temporal (atomic action) control. Experiments demonstrate that FrankenMotion outperforms all previous baseline models adapted and retrained for our setting, and our model can compose motions unseen during training. Our code and dataset will be publicly available upon publication."

[19.01.2026 14:42] Response: [
    "DIFFUSION",
    "AGI",  # Reasoning capabilities of large language models used
    "INTERPRETABILITY"  # Implicitly involves interpretable motion generation
]
[19.01.2026 14:42] Error. Failed to parse JSON from LLM. [
    "DIFFUSION",
    "AGI",  # Reasoning capabilities of large language models used
    "INTERPRETABILITY"  # Implicitly involves interpretable motion generation
]
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper (4 sentences), use correct machine learning terms. 'title': a slogan of a main idea of the article. Return only JSON and nothing else.

A diffusion-based framework generates human motion from text prompts with fine-grained part-level control using a newly constructed dataset with atomic, temporally-aware annotations.  					AI-generated summary 				 Human motion generation from text prompts has made remarkable progress in recent years. However, existing methods primarily rely on either sequence-level or action-level descriptions due to the absence of fine-grained, part-level motion annotations. This limits their controllability over individual body parts. In this work, we construct a high-quality motion dataset with atomic, temporally-aware part-level text annotations, leveraging the reasoning capabilities of large language models (LLMs). Unlike prior datasets that either provide synchronized part captions with fixed time segments or rely solely on global sequence labels, our dataset captures asynchronous and semantically distinct part movements at fine temporal resolution. Based on this dataset, we introduce a diffusion-based part-aware motion generation framework, namely FrankenMotion, where each body part is guided by its own temporally-structured textual prompt. This is, to our knowledge, the first work to provide atomic, temporally-aware part-level motion annotations and have a model that allows motion generation with both spatial (body part) and temporal (atomic action) control. Experiments demonstrate that FrankenMotion outperforms all previous baseline models adapted and retrained for our setting, and our model can compose motions unseen during training. Our code and dataset will be publicly available upon publication.

Return JSON that matches this schema: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object"}. Respond with JSON only.
[19.01.2026 14:42] Response: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object", "description": "Explanation of the paper's key points using correct machine learning terms", "example": {"desc": "A diffusion-based framework generates human motion from text prompts with fine-grained part-level control using a newly constructed dataset with atomic, temporally-aware annotations.", "title": "Fine-Grained Part-Level Control in Motion Generation Using Temporally-Aware Annotations"}}"desc": "This paper introduces a novel diffusion-based framework called FrankenMotion that enables fine-grained part-level control in human motion generation from text prompts. It utilizes a newly created dataset with atomic, temporally-aware part-level annotations, which enhances controllability over individual body parts compared to previous methods relying on sequence-level or action-level descriptions. The framework demonstrates superior performance over existing baselines and supports composition of unseen motions during training.", "title": "Atomic Part-Level Control in Diffusion-Based Human Motion Generation"}
[19.01.2026 14:42] Error. Failed to parse JSON. Details: Extra data: line 1 column 578 (char 577). Response: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object", "description": "Explanation of the paper's key points using correct machine learning terms", "example": {"desc": "A diffusion-based framework generates human motion from text prompts with fine-grained part-level control using a newly constructed dataset with atomic, temporally-aware annotations.", "title": "Fine-Grained Part-Level Control in Motion Generation Using Temporally-Aware Annotations"}}"desc": "This paper introduces a novel diffusion-based framework called FrankenMotion that enables fine-grained part-level control in human motion generation from text prompts. It utilizes a newly created dataset with atomic, temporally-aware part-level annotations, which enhances controllability over individual body parts compared to previous methods relying on sequence-level or action-level descriptions. The framework demonstrates superior performance over existing baselines and supports composition of unseen motions during training.", "title": "Atomic Part-Level Control in Diffusion-Based Human Motion Generation"}
[19.01.2026 14:42] Querying the API.
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Reasoning models demonstrate enhanced performance through multi-agent-like interactions that create diverse cognitive perspectives and improve problem-solving through structured social organization.  					AI-generated summary 				 Large language models have achieved remarkable capabilities across domains, yet mechanisms underlying sophisticated reasoning remain elusive. Recent reasoning models outperform comparable instruction-tuned models on complex cognitive tasks, attributed to extended computation through longer chains of thought. Here we show that enhanced reasoning emerges not from extended computation alone, but from simulating multi-agent-like interactions -- a society of thought -- which enables diversification and debate among internal cognitive perspectives characterized by distinct personality traits and domain expertise. Through quantitative analysis and mechanistic interpretability methods applied to reasoning traces, we find that reasoning models like DeepSeek-R1 and QwQ-32B exhibit much greater perspective diversity than instruction-tuned models, activating broader conflict between heterogeneous personality- and expertise-related features during reasoning. This multi-agent structure manifests in conversational behaviors, including question-answering, perspective shifts, and the reconciliation of conflicting views, and in socio-emotional roles that characterize sharp back-and-forth conversations, together accounting for the accuracy advantage in reasoning tasks. Controlled reinforcement learning experiments reveal that base models increase conversational behaviors when rewarded solely for reasoning accuracy, and fine-tuning models with conversational scaffolding accelerates reasoning improvement over base models. These findings indicate that the social organization of thought enables effective exploration of solution spaces. We suggest that reasoning models establish a computational parallel to collective intelligence in human groups, where diversity enables superior problem-solving when systematically structured, which suggests new opportunities for agent organization to harness the wisdom of crowds.
[19.01.2026 14:42] Response: {
  "desc": "В данной работе показано, что модели машинного обучения демонстрируют улучшенные результаты благодаря взаимодействию в стиле мультиагентных систем, создающих разнообразные когнитивные перспективы и улучшающих решение задач за счет структурированной социальной организации. Исследование выявило, что высокая эффективность моделей обусловлена не только увеличением длины цепочек рассуждений, но и имитацией взаимодействия множества внутренних когнитивных представлений с различными личностными чертами и областями знаний. Это приводит к более широкому обсуждению и разрешению конфликтов мнений внутри модели, что способствует повышению точности выполнения сложных интеллектуальных задач.",
  "emoji": "🤖",
  "title": "Социальная организация мышления как ключ к эффективной обработке информации"
}
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension
OPEN_SOURCE: Papers that contribute to open-source projects
SCIENCE: Papers on scientific applications of ML
LOW_RESOURCE: Papers mentioning low-resource settings or languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Reasoning models demonstrate enhanced performance through multi-agent-like interactions that create diverse cognitive perspectives and improve problem-solving through structured social organization.  					AI-generated summary 				 Large language models have achieved remarkable capabilities across domains, yet mechanisms underlying sophisticated reasoning remain elusive. Recent reasoning models outperform comparable instruction-tuned models on complex cognitive tasks, attributed to extended computation through longer chains of thought. Here we show that enhanced reasoning emerges not from extended computation alone, but from simulating multi-agent-like interactions -- a society of thought -- which enables diversification and debate among internal cognitive perspectives characterized by distinct personality traits and domain expertise. Through quantitative analysis and mechanistic interpretability methods applied to reasoning traces, we find that reasoning models like DeepSeek-R1 and QwQ-32B exhibit much greater perspective diversity than instruction-tuned models, activating broader conflict between heterogeneous personality- and expertise-related features during reasoning. This multi-agent structure manifests in conversational behaviors, including question-answering, perspective shifts, and the reconciliation of conflicting views, and in socio-emotional roles that characterize sharp back-and-forth conversations, together accounting for the accuracy advantage in reasoning tasks. Controlled reinforcement learning experiments reveal that base models increase conversational behaviors when rewarded solely for reasoning accuracy, and fine-tuning models with conversational scaffolding accelerates reasoning improvement over base models. These findings indicate that the social organization of thought enables effective exploration of solution spaces. We suggest that reasoning models establish a computational parallel to collective intelligence in human groups, where diversity enables superior problem-solving when systematically structured, which suggests new opportunities for agent organization to harness the wisdom of crowds."

[19.01.2026 14:42] Response: [
    "AGENTS",
    "REASONING",
    "SOCIAL_INTERACTION",
    "COGNITIVE_MODELING"
]
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Reasoning models demonstrate enhanced performance through multi-agent-like interactions that create diverse cognitive perspectives and improve problem-solving through structured social organization.  					AI-generated summary 				 Large language models have achieved remarkable capabilities across domains, yet mechanisms underlying sophisticated reasoning remain elusive. Recent reasoning models outperform comparable instruction-tuned models on complex cognitive tasks, attributed to extended computation through longer chains of thought. Here we show that enhanced reasoning emerges not from extended computation alone, but from simulating multi-agent-like interactions -- a society of thought -- which enables diversification and debate among internal cognitive perspectives characterized by distinct personality traits and domain expertise. Through quantitative analysis and mechanistic interpretability methods applied to reasoning traces, we find that reasoning models like DeepSeek-R1 and QwQ-32B exhibit much greater perspective diversity than instruction-tuned models, activating broader conflict between heterogeneous personality- and expertise-related features during reasoning. This multi-agent structure manifests in conversational behaviors, including question-answering, perspective shifts, and the reconciliation of conflicting views, and in socio-emotional roles that characterize sharp back-and-forth conversations, together accounting for the accuracy advantage in reasoning tasks. Controlled reinforcement learning experiments reveal that base models increase conversational behaviors when rewarded solely for reasoning accuracy, and fine-tuning models with conversational scaffolding accelerates reasoning improvement over base models. These findings indicate that the social organization of thought enables effective exploration of solution spaces. We suggest that reasoning models establish a computational parallel to collective intelligence in human groups, where diversity enables superior problem-solving when systematically structured, which suggests new opportunities for agent organization to harness the wisdom of crowds."

[19.01.2026 14:42] Response: [
    "REASONING",
    "INTERPRETABILITY"
]
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper (4 sentences), use correct machine learning terms. 'title': a slogan of a main idea of the article. Return only JSON and nothing else.

Reasoning models demonstrate enhanced performance through multi-agent-like interactions that create diverse cognitive perspectives and improve problem-solving through structured social organization.  					AI-generated summary 				 Large language models have achieved remarkable capabilities across domains, yet mechanisms underlying sophisticated reasoning remain elusive. Recent reasoning models outperform comparable instruction-tuned models on complex cognitive tasks, attributed to extended computation through longer chains of thought. Here we show that enhanced reasoning emerges not from extended computation alone, but from simulating multi-agent-like interactions -- a society of thought -- which enables diversification and debate among internal cognitive perspectives characterized by distinct personality traits and domain expertise. Through quantitative analysis and mechanistic interpretability methods applied to reasoning traces, we find that reasoning models like DeepSeek-R1 and QwQ-32B exhibit much greater perspective diversity than instruction-tuned models, activating broader conflict between heterogeneous personality- and expertise-related features during reasoning. This multi-agent structure manifests in conversational behaviors, including question-answering, perspective shifts, and the reconciliation of conflicting views, and in socio-emotional roles that characterize sharp back-and-forth conversations, together accounting for the accuracy advantage in reasoning tasks. Controlled reinforcement learning experiments reveal that base models increase conversational behaviors when rewarded solely for reasoning accuracy, and fine-tuning models with conversational scaffolding accelerates reasoning improvement over base models. These findings indicate that the social organization of thought enables effective exploration of solution spaces. We suggest that reasoning models establish a computational parallel to collective intelligence in human groups, where diversity enables superior problem-solving when systematically structured, which suggests new opportunities for agent organization to harness the wisdom of crowds.

Return JSON that matches this schema: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object"}. Respond with JSON only.
[19.01.2026 14:42] Response: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object", "description": "Explanation of the paper in JSON format", "desc": "This paper explores how reasoning models enhance their performance through multi-agent-like interactions, creating diverse cognitive perspectives and improving problem-solving via structured social organization. It highlights that these models achieve better reasoning outcomes not just due to extended computation, but also because they simulate a society of thought, enabling diverse perspectives and debates. Quantitative analysis reveals that reasoning models like DeepSeek-R1 and QwQ-32B exhibit higher perspective diversity compared to instruction-tuned models, leading to improved reasoning accuracy. The study suggests that such social organization mimics collective intelligence in humans, offering new insights into leveraging the wisdom of crowds.", "title": "Computational Collective Intelligence in Reasoning Models"}
[19.01.2026 14:42] Querying the API.
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

PersonalAlign framework addresses GUI agent alignment with implicit user intents through hierarchical memory organization and long-term record reasoning, improving both execution and proactive performance.  					AI-generated summary 				 While GUI agents have shown strong performance under explicit and completion instructions, real-world deployment requires aligning with users' more complex implicit intents. In this work, we highlight Hierarchical Implicit Intent Alignment for Personalized GUI Agent (PersonalAlign), a new agent task that requires agents to leverage long-term user records as persistent context to resolve omitted preferences in vague instructions and anticipate latent routines by user state for proactive assistance. To facilitate this study, we introduce AndroidIntent, a benchmark designed to evaluate agents' ability in resolving vague instructions and providing proactive suggestions through reasoning over long-term user records. We annotated 775 user-specific preferences and 215 routines from 20k long-term records across different users for evaluation. Furthermore, we introduce Hierarchical Intent Memory Agent (HIM-Agent), which maintains a continuously updating personal memory and hierarchically organizes user preferences and routines for personalization. Finally, we evaluate a range of GUI agents on AndroidIntent, including GPT-5, Qwen3-VL, and UI-TARS, further results show that HIM-Agent significantly improves both execution and proactive performance by 15.7% and 7.3%.
[19.01.2026 14:42] Response: {
  "desc": "Framework PersonalAlign решает задачу выравнивания интерфейса пользователя с неявными намерениями пользователей через иерархическую организацию памяти и долгосрочное рассуждение на основе записей, что улучшает как выполнение, так и проактивное поведение интеллектуальных агентов. В работе представлен новый подход к задаче PersonalAlign, использующий персонифицированную память для учета предпочтений и рутинных действий пользователей в течение длительного времени. Для оценки эффективности предложенного подхода был создан набор данных AndroidIntent, включающий аннотированные предпочтения и рутины пользователей. Результаты экспериментов показали значительное улучшение выполнения и проактивного поведения по сравнению с другими агентами, такими как GPT-5 и Qwen3-VL.",
  "emoji": "📱",
  "title": "Личный баланс: выравнивание намерений пользователей через персональную память ИИ-агента"
}
[19.01.2026 14:42] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension
OPEN_SOURCE: Papers that contribute to open-source projects
SCIENCE: Papers on scientific applications of ML
LOW_RESOURCE: Papers mentioning low-resource settings or languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"PersonalAlign framework addresses GUI agent alignment with implicit user intents through hierarchical memory organization and long-term record reasoning, improving both execution and proactive performance.  					AI-generated summary 				 While GUI agents have shown strong performance under explicit and completion instructions, real-world deployment requires aligning with users' more complex implicit intents. In this work, we highlight Hierarchical Implicit Intent Alignment for Personalized GUI Agent (PersonalAlign), a new agent task that requires agents to leverage long-term user records as persistent context to resolve omitted preferences in vague instructions and anticipate latent routines by user state for proactive assistance. To facilitate this study, we introduce AndroidIntent, a benchmark designed to evaluate agents' ability in resolving vague instructions and providing proactive suggestions through reasoning over long-term user records. We annotated 775 user-specific preferences and 215 routines from 20k long-term records across different users for evaluation. Furthermore, we introduce Hierarchical Intent Memory Agent (HIM-Agent), which maintains a continuously updating personal memory and hierarchically organizes user preferences and routines for personalization. Finally, we evaluate a range of GUI agents on AndroidIntent, including GPT-5, Qwen3-VL, and UI-TARS, further results show that HIM-Agent significantly improves both execution and proactive performance by 15.7% and 7.3%."

[19.01.2026 14:43] Response: [
    "AGENTS",
    "RETRIEVAL",
    "CONTEXT_WINDOW",
    "SESSION_MEMORY"
]
[19.01.2026 14:43] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"PersonalAlign framework addresses GUI agent alignment with implicit user intents through hierarchical memory organization and long-term record reasoning, improving both execution and proactive performance.  					AI-generated summary 				 While GUI agents have shown strong performance under explicit and completion instructions, real-world deployment requires aligning with users' more complex implicit intents. In this work, we highlight Hierarchical Implicit Intent Alignment for Personalized GUI Agent (PersonalAlign), a new agent task that requires agents to leverage long-term user records as persistent context to resolve omitted preferences in vague instructions and anticipate latent routines by user state for proactive assistance. To facilitate this study, we introduce AndroidIntent, a benchmark designed to evaluate agents' ability in resolving vague instructions and providing proactive suggestions through reasoning over long-term user records. We annotated 775 user-specific preferences and 215 routines from 20k long-term records across different users for evaluation. Furthermore, we introduce Hierarchical Intent Memory Agent (HIM-Agent), which maintains a continuously updating personal memory and hierarchically organizes user preferences and routines for personalization. Finally, we evaluate a range of GUI agents on AndroidIntent, including GPT-5, Qwen3-VL, and UI-TARS, further results show that HIM-Agent significantly improves both execution and proactive performance by 15.7% and 7.3%."

[19.01.2026 14:43] Response: [
    "AGI",
    "REASONING",
    "AGENT_MEMORY",
    "CONTEXT_WINDOW",
    "HALLUCINATIONS"
]
[19.01.2026 14:43] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper (4 sentences), use correct machine learning terms. 'title': a slogan of a main idea of the article. Return only JSON and nothing else.

PersonalAlign framework addresses GUI agent alignment with implicit user intents through hierarchical memory organization and long-term record reasoning, improving both execution and proactive performance.  					AI-generated summary 				 While GUI agents have shown strong performance under explicit and completion instructions, real-world deployment requires aligning with users' more complex implicit intents. In this work, we highlight Hierarchical Implicit Intent Alignment for Personalized GUI Agent (PersonalAlign), a new agent task that requires agents to leverage long-term user records as persistent context to resolve omitted preferences in vague instructions and anticipate latent routines by user state for proactive assistance. To facilitate this study, we introduce AndroidIntent, a benchmark designed to evaluate agents' ability in resolving vague instructions and providing proactive suggestions through reasoning over long-term user records. We annotated 775 user-specific preferences and 215 routines from 20k long-term records across different users for evaluation. Furthermore, we introduce Hierarchical Intent Memory Agent (HIM-Agent), which maintains a continuously updating personal memory and hierarchically organizes user preferences and routines for personalization. Finally, we evaluate a range of GUI agents on AndroidIntent, including GPT-5, Qwen3-VL, and UI-TARS, further results show that HIM-Agent significantly improves both execution and proactive performance by 15.7% and 7.3%.

Return JSON that matches this schema: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object"}. Respond with JSON only.
[19.01.2026 14:43] Response: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object", "description": "Machine Learning Paper Abstract Summary Schema", "desc": "The PersonalAlign framework focuses on aligning GUI agents with user's implicit intents using hierarchical memory organization and long-term record reasoning. It enhances both execution and proactive performance by leveraging persistent user contexts. The AndroidIntent benchmark evaluates agents' abilities to resolve vague instructions and provide proactive suggestions based on long-term user records. The Hierarchical Intent Memory Agent (HIM-Agent) maintains personalized memories and organizes user preferences and routines hierarchically for better personalization. Experimental results demonstrate significant improvements in execution and proactive performance compared to other agents.", "title": "Hierarchical Implicit Intent Alignment for Personalized GUI Agents"}
[19.01.2026 14:43] Querying the API.
[19.01.2026 14:43] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Activation probes for language model misuse mitigation face challenges with long-context generalization, requiring new architectures and diverse training for robust performance across production shifts.  					AI-generated summary 				 Frontier language model capabilities are improving rapidly. We thus need stronger mitigations against bad actors misusing increasingly powerful systems. Prior work has shown that activation probes may be a promising misuse mitigation technique, but we identify a key remaining challenge: probes fail to generalize under important production distribution shifts. In particular, we find that the shift from short-context to long-context inputs is difficult for existing probe architectures. We propose several new probe architecture that handle this long-context distribution shift.   We evaluate these probes in the cyber-offensive domain, testing their robustness against various production-relevant shifts, including multi-turn conversations, static jailbreaks, and adaptive red teaming. Our results demonstrate that while multimax addresses context length, a combination of architecture choice and training on diverse distributions is required for broad generalization. Additionally, we show that pairing probes with prompted classifiers achieves optimal accuracy at a low cost due to the computational efficiency of probes.   These findings have informed the successful deployment of misuse mitigation probes in user-facing instances of Gemini, Google's frontier language model. Finally, we find early positive results using AlphaEvolve to automate improvements in both probe architecture search and adaptive red teaming, showing that automating some AI safety research is already possible.
[19.01.2026 14:43] Response: {
  "desc": "В данной работе рассматриваются проблемы обобщения активационных пробелов в языковых моделях при переходе от коротких к длинным контекстам, что требует разработки новых архитектур и разнообразного обучения для обеспечения надежной работы моделей в производственных условиях. Авторы предлагают новые архитектуры пробелов, которые успешно справляются с длинными контекстами, а также демонстрируют их эффективность на практике через тестирование в области кибератак. Исследование подтверждает важность сочетания правильной архитектуры и широкого набора данных для достижения общей универсальности, дополнительно подчеркивая потенциал автоматического улучшения безопасности ИИ через инструменты вроде AlphaEvolve.",
  "emoji": "🔮",
  "title": "Улучшение защиты языковых моделей через активационные пробы"
}
[19.01.2026 14:43] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension
OPEN_SOURCE: Papers that contribute to open-source projects
SCIENCE: Papers on scientific applications of ML
LOW_RESOURCE: Papers mentioning low-resource settings or languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Activation probes for language model misuse mitigation face challenges with long-context generalization, requiring new architectures and diverse training for robust performance across production shifts.  					AI-generated summary 				 Frontier language model capabilities are improving rapidly. We thus need stronger mitigations against bad actors misusing increasingly powerful systems. Prior work has shown that activation probes may be a promising misuse mitigation technique, but we identify a key remaining challenge: probes fail to generalize under important production distribution shifts. In particular, we find that the shift from short-context to long-context inputs is difficult for existing probe architectures. We propose several new probe architecture that handle this long-context distribution shift.   We evaluate these probes in the cyber-offensive domain, testing their robustness against various production-relevant shifts, including multi-turn conversations, static jailbreaks, and adaptive red teaming. Our results demonstrate that while multimax addresses context length, a combination of architecture choice and training on diverse distributions is required for broad generalization. Additionally, we show that pairing probes with prompted classifiers achieves optimal accuracy at a low cost due to the computational efficiency of probes.   These findings have informed the successful deployment of misuse mitigation probes in user-facing instances of Gemini, Google's frontier language model. Finally, we find early positive results using AlphaEvolve to automate improvements in both probe architecture search and adaptive red teaming, showing that automating some AI safety research is already possible."

[19.01.2026 14:43] Response: [
    "DATA",
    "AGENT_MEMORY",
    "CONTEXT_WINDOW",
    "PROMPTING"
]
[19.01.2026 14:43] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Activation probes for language model misuse mitigation face challenges with long-context generalization, requiring new architectures and diverse training for robust performance across production shifts.  					AI-generated summary 				 Frontier language model capabilities are improving rapidly. We thus need stronger mitigations against bad actors misusing increasingly powerful systems. Prior work has shown that activation probes may be a promising misuse mitigation technique, but we identify a key remaining challenge: probes fail to generalize under important production distribution shifts. In particular, we find that the shift from short-context to long-context inputs is difficult for existing probe architectures. We propose several new probe architecture that handle this long-context distribution shift.   We evaluate these probes in the cyber-offensive domain, testing their robustness against various production-relevant shifts, including multi-turn conversations, static jailbreaks, and adaptive red teaming. Our results demonstrate that while multimax addresses context length, a combination of architecture choice and training on diverse distributions is required for broad generalization. Additionally, we show that pairing probes with prompted classifiers achieves optimal accuracy at a low cost due to the computational efficiency of probes.   These findings have informed the successful deployment of misuse mitigation probes in user-facing instances of Gemini, Google's frontier language model. Finally, we find early positive results using AlphaEvolve to automate improvements in both probe architecture search and adaptive red teaming, showing that automating some AI safety research is already possible."

[19.01.2026 14:43] Response: [
    "LONG_CONTEXT",
    "SECURITY"
]
[19.01.2026 14:43] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper (4 sentences), use correct machine learning terms. 'title': a slogan of a main idea of the article. Return only JSON and nothing else.

Activation probes for language model misuse mitigation face challenges with long-context generalization, requiring new architectures and diverse training for robust performance across production shifts.  					AI-generated summary 				 Frontier language model capabilities are improving rapidly. We thus need stronger mitigations against bad actors misusing increasingly powerful systems. Prior work has shown that activation probes may be a promising misuse mitigation technique, but we identify a key remaining challenge: probes fail to generalize under important production distribution shifts. In particular, we find that the shift from short-context to long-context inputs is difficult for existing probe architectures. We propose several new probe architecture that handle this long-context distribution shift.   We evaluate these probes in the cyber-offensive domain, testing their robustness against various production-relevant shifts, including multi-turn conversations, static jailbreaks, and adaptive red teaming. Our results demonstrate that while multimax addresses context length, a combination of architecture choice and training on diverse distributions is required for broad generalization. Additionally, we show that pairing probes with prompted classifiers achieves optimal accuracy at a low cost due to the computational efficiency of probes.   These findings have informed the successful deployment of misuse mitigation probes in user-facing instances of Gemini, Google's frontier language model. Finally, we find early positive results using AlphaEvolve to automate improvements in both probe architecture search and adaptive red teaming, showing that automating some AI safety research is already possible.

Return JSON that matches this schema: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object"}. Respond with JSON only.
[19.01.2026 14:43] Response: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object", "desc": "This paper explores the challenges of long-context generalization in activation probes for language models used in misuse mitigation. It proposes new architectures and diverse training methods to enhance robustness across different production environments. The study evaluates these probes in cyber-offensive scenarios and finds that combining specific architectures with prompt-based classifiers yields optimal performance. This work informs the deployment of misuse mitigation techniques in advanced language models like Gemini.", "title": "Mitigating Language Model Misuse via Robust Activation Probes"}
[19.01.2026 14:43] Querying the API.
[19.01.2026 14:43] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A physics-aware reinforcement learning paradigm is introduced for video generation that enforces physical collision rules directly in high-dimensional spaces, ensuring strict application of physics knowledge rather than treating it as conditional constraints.  					AI-generated summary 				 Physical principles are fundamental to realistic visual simulation, but remain a significant oversight in transformer-based video generation. This gap highlights a critical limitation in rendering rigid body motion, a core tenet of classical mechanics. While computer graphics and physics-based simulators can easily model such collisions using Newton formulas, modern pretrain-finetune paradigms discard the concept of object rigidity during pixel-level global denoising. Even perfectly correct mathematical constraints are treated as suboptimal solutions (i.e., conditions) during model optimization in post-training, fundamentally limiting the physical realism of generated videos. Motivated by these considerations, we introduce, for the first time, a physics-aware reinforcement learning paradigm for video generation models that enforces physical collision rules directly in high-dimensional spaces, ensuring the physics knowledge is strictly applied rather than treated as conditions. Subsequently, we extend this paradigm to a unified framework, termed Mimicry-Discovery Cycle (MDcycle), which allows substantial fine-tuning while fully preserving the model's ability to leverage physics-grounded feedback. To validate our approach, we construct new benchmark PhysRVGBench and perform extensive qualitative and quantitative experiments to thoroughly assess its effectiveness.
[19.01.2026 14:43] Response: {
  "desc": "В статье представлена новая парадигма машинного обучения с подкреплением, ориентированная на физику, для генерации видео, которая напрямую применяет правила физических столкновений в многомерном пространстве. Это позволяет строго соблюдать физические знания вместо того, чтобы рассматривать их как условные ограничения. Предложенная архитектура включает в себя цикл подражания-обнаружения (MDcycle), который обеспечивает глубокое обучение моделей с сохранением способности использовать обратную связь, основанную на физической основе. Для проверки эффективности предложенного подхода был создан новый эталонный набор данных PhysRVGBench и проведены подробные качественные и количественные эксперименты.",
  "emoji": "🔄",
  "title": "Физически осведомленное обучение с подкреплением для реалистичной генерации видео"
}
[19.01.2026 14:43] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension
OPEN_SOURCE: Papers that contribute to open-source projects
SCIENCE: Papers on scientific applications of ML
LOW_RESOURCE: Papers mentioning low-resource settings or languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A physics-aware reinforcement learning paradigm is introduced for video generation that enforces physical collision rules directly in high-dimensional spaces, ensuring strict application of physics knowledge rather than treating it as conditional constraints.  					AI-generated summary 				 Physical principles are fundamental to realistic visual simulation, but remain a significant oversight in transformer-based video generation. This gap highlights a critical limitation in rendering rigid body motion, a core tenet of classical mechanics. While computer graphics and physics-based simulators can easily model such collisions using Newton formulas, modern pretrain-finetune paradigms discard the concept of object rigidity during pixel-level global denoising. Even perfectly correct mathematical constraints are treated as suboptimal solutions (i.e., conditions) during model optimization in post-training, fundamentally limiting the physical realism of generated videos. Motivated by these considerations, we introduce, for the first time, a physics-aware reinforcement learning paradigm for video generation models that enforces physical collision rules directly in high-dimensional spaces, ensuring the physics knowledge is strictly applied rather than treated as conditions. Subsequently, we extend this paradigm to a unified framework, termed Mimicry-Discovery Cycle (MDcycle), which allows substantial fine-tuning while fully preserving the model's ability to leverage physics-grounded feedback. To validate our approach, we construct new benchmark PhysRVGBench and perform extensive qualitative and quantitative experiments to thoroughly assess its effectiveness."

[19.01.2026 14:43] Response: [
    "RL",
    "PHYSICS",
    "VIDEO_GENERATION"
]
[19.01.2026 14:43] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A physics-aware reinforcement learning paradigm is introduced for video generation that enforces physical collision rules directly in high-dimensional spaces, ensuring strict application of physics knowledge rather than treating it as conditional constraints.  					AI-generated summary 				 Physical principles are fundamental to realistic visual simulation, but remain a significant oversight in transformer-based video generation. This gap highlights a critical limitation in rendering rigid body motion, a core tenet of classical mechanics. While computer graphics and physics-based simulators can easily model such collisions using Newton formulas, modern pretrain-finetune paradigms discard the concept of object rigidity during pixel-level global denoising. Even perfectly correct mathematical constraints are treated as suboptimal solutions (i.e., conditions) during model optimization in post-training, fundamentally limiting the physical realism of generated videos. Motivated by these considerations, we introduce, for the first time, a physics-aware reinforcement learning paradigm for video generation models that enforces physical collision rules directly in high-dimensional spaces, ensuring the physics knowledge is strictly applied rather than treated as conditions. Subsequently, we extend this paradigm to a unified framework, termed Mimicry-Discovery Cycle (MDcycle), which allows substantial fine-tuning while fully preserving the model's ability to leverage physics-grounded feedback. To validate our approach, we construct new benchmark PhysRVGBench and perform extensive qualitative and quantitative experiments to thoroughly assess its effectiveness."

[19.01.2026 14:43] Response: [
    "PHYSICS",
    "VIDEO_GENERATION"
]
[19.01.2026 14:43] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper (4 sentences), use correct machine learning terms. 'title': a slogan of a main idea of the article. Return only JSON and nothing else.

A physics-aware reinforcement learning paradigm is introduced for video generation that enforces physical collision rules directly in high-dimensional spaces, ensuring strict application of physics knowledge rather than treating it as conditional constraints.  					AI-generated summary 				 Physical principles are fundamental to realistic visual simulation, but remain a significant oversight in transformer-based video generation. This gap highlights a critical limitation in rendering rigid body motion, a core tenet of classical mechanics. While computer graphics and physics-based simulators can easily model such collisions using Newton formulas, modern pretrain-finetune paradigms discard the concept of object rigidity during pixel-level global denoising. Even perfectly correct mathematical constraints are treated as suboptimal solutions (i.e., conditions) during model optimization in post-training, fundamentally limiting the physical realism of generated videos. Motivated by these considerations, we introduce, for the first time, a physics-aware reinforcement learning paradigm for video generation models that enforces physical collision rules directly in high-dimensional spaces, ensuring the physics knowledge is strictly applied rather than treated as conditions. Subsequently, we extend this paradigm to a unified framework, termed Mimicry-Discovery Cycle (MDcycle), which allows substantial fine-tuning while fully preserving the model's ability to leverage physics-grounded feedback. To validate our approach, we construct new benchmark PhysRVGBench and perform extensive qualitative and quantitative experiments to thoroughly assess its effectiveness.

Return JSON that matches this schema: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object"}. Respond with JSON only.
[19.01.2026 14:43] Response: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object", "description": "Explanation of the paper in simple terms", "example": {"desc": "A physics-aware reinforcement learning paradigm is introduced for video generation that enforces physical collision rules directly in high-dimensional spaces, ensuring strict application of physics knowledge rather than treating it as conditional constraints.", "title": "Physics-Aware Reinforcement Learning for Video Generation"} } 

Example Output:
{"desc": "This paper introduces a novel method for generating videos that incorporates physical collision rules into high-dimensional spaces, ensuring accurate representation of rigid body motion based on classical mechanics principles.", "title": "Physics-Grounded Video Generation via Physics-Aware Reinforcement Learning"}
[19.01.2026 14:43] Error. Failed to parse JSON. Details: Extra data: line 3 column 1 (char 594). Response: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object", "description": "Explanation of the paper in simple terms", "example": {"desc": "A physics-aware reinforcement learning paradigm is introduced for video generation that enforces physical collision rules directly in high-dimensional spaces, ensuring strict application of physics knowledge rather than treating it as conditional constraints.", "title": "Physics-Aware Reinforcement Learning for Video Generation"} } 

Example Output:
{"desc": "This paper introduces a novel method for generating videos that incorporates physical collision rules into high-dimensional spaces, ensuring accurate representation of rigid body motion based on classical mechanics principles.", "title": "Physics-Grounded Video Generation via Physics-Aware Reinforcement Learning"}
[19.01.2026 14:43] Querying the API.
[19.01.2026 14:43] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

ShapeR generates high-fidelity 3D shapes from casual image sequences using visual-inertial SLAM, 3D detection, and vision-language models with rectified flow transformer conditioning.  					AI-generated summary 				 Recent advances in 3D shape generation have achieved impressive results, but most existing methods rely on clean, unoccluded, and well-segmented inputs. Such conditions are rarely met in real-world scenarios. We present ShapeR, a novel approach for conditional 3D object shape generation from casually captured sequences. Given an image sequence, we leverage off-the-shelf visual-inertial SLAM, 3D detection algorithms, and vision-language models to extract, for each object, a set of sparse SLAM points, posed multi-view images, and machine-generated captions. A rectified flow transformer trained to effectively condition on these modalities then generates high-fidelity metric 3D shapes. To ensure robustness to the challenges of casually captured data, we employ a range of techniques including on-the-fly compositional augmentations, a curriculum training scheme spanning object- and scene-level datasets, and strategies to handle background clutter. Additionally, we introduce a new evaluation benchmark comprising 178 in-the-wild objects across 7 real-world scenes with geometry annotations. Experiments show that ShapeR significantly outperforms existing approaches in this challenging setting, achieving an improvement of 2.7x in Chamfer distance compared to state of the art.
[19.01.2026 14:43] Response: {
  "title": "ShapeR: Генерация высокоточных 3D форм на основе неформальных последовательностей изображений",
  "desc": "ShapeR — это метод генерации 3D-моделей объектов высокой точности из обычных видеоизображений с использованием визуально-инерциального SLAM, детекции 3D-объектов и моделей зрения и языка с условием трансформера потока, исправленного для дополнительных данных. Метод включает техники усиления данных в реальном времени, поэтапную схему обучения и стратегии борьбы с фоном, что позволяет значительно улучшить результаты по сравнению с существующими подходами.",
  "emoji": "📦"
}
[19.01.2026 14:43] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension
OPEN_SOURCE: Papers that contribute to open-source projects
SCIENCE: Papers on scientific applications of ML
LOW_RESOURCE: Papers mentioning low-resource settings or languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ShapeR generates high-fidelity 3D shapes from casual image sequences using visual-inertial SLAM, 3D detection, and vision-language models with rectified flow transformer conditioning.  					AI-generated summary 				 Recent advances in 3D shape generation have achieved impressive results, but most existing methods rely on clean, unoccluded, and well-segmented inputs. Such conditions are rarely met in real-world scenarios. We present ShapeR, a novel approach for conditional 3D object shape generation from casually captured sequences. Given an image sequence, we leverage off-the-shelf visual-inertial SLAM, 3D detection algorithms, and vision-language models to extract, for each object, a set of sparse SLAM points, posed multi-view images, and machine-generated captions. A rectified flow transformer trained to effectively condition on these modalities then generates high-fidelity metric 3D shapes. To ensure robustness to the challenges of casually captured data, we employ a range of techniques including on-the-fly compositional augmentations, a curriculum training scheme spanning object- and scene-level datasets, and strategies to handle background clutter. Additionally, we introduce a new evaluation benchmark comprising 178 in-the-wild objects across 7 real-world scenes with geometry annotations. Experiments show that ShapeR significantly outperforms existing approaches in this challenging setting, achieving an improvement of 2.7x in Chamfer distance compared to state of the art."

[19.01.2026 14:43] Response: [
    "3D"
]
[19.01.2026 14:43] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ShapeR generates high-fidelity 3D shapes from casual image sequences using visual-inertial SLAM, 3D detection, and vision-language models with rectified flow transformer conditioning.  					AI-generated summary 				 Recent advances in 3D shape generation have achieved impressive results, but most existing methods rely on clean, unoccluded, and well-segmented inputs. Such conditions are rarely met in real-world scenarios. We present ShapeR, a novel approach for conditional 3D object shape generation from casually captured sequences. Given an image sequence, we leverage off-the-shelf visual-inertial SLAM, 3D detection algorithms, and vision-language models to extract, for each object, a set of sparse SLAM points, posed multi-view images, and machine-generated captions. A rectified flow transformer trained to effectively condition on these modalities then generates high-fidelity metric 3D shapes. To ensure robustness to the challenges of casually captured data, we employ a range of techniques including on-the-fly compositional augmentations, a curriculum training scheme spanning object- and scene-level datasets, and strategies to handle background clutter. Additionally, we introduce a new evaluation benchmark comprising 178 in-the-wild objects across 7 real-world scenes with geometry annotations. Experiments show that ShapeR significantly outperforms existing approaches in this challenging setting, achieving an improvement of 2.7x in Chamfer distance compared to state of the art."

[19.01.2026 14:43] Response: []
[19.01.2026 14:43] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper (4 sentences), use correct machine learning terms. 'title': a slogan of a main idea of the article. Return only JSON and nothing else.

ShapeR generates high-fidelity 3D shapes from casual image sequences using visual-inertial SLAM, 3D detection, and vision-language models with rectified flow transformer conditioning.  					AI-generated summary 				 Recent advances in 3D shape generation have achieved impressive results, but most existing methods rely on clean, unoccluded, and well-segmented inputs. Such conditions are rarely met in real-world scenarios. We present ShapeR, a novel approach for conditional 3D object shape generation from casually captured sequences. Given an image sequence, we leverage off-the-shelf visual-inertial SLAM, 3D detection algorithms, and vision-language models to extract, for each object, a set of sparse SLAM points, posed multi-view images, and machine-generated captions. A rectified flow transformer trained to effectively condition on these modalities then generates high-fidelity metric 3D shapes. To ensure robustness to the challenges of casually captured data, we employ a range of techniques including on-the-fly compositional augmentations, a curriculum training scheme spanning object- and scene-level datasets, and strategies to handle background clutter. Additionally, we introduce a new evaluation benchmark comprising 178 in-the-wild objects across 7 real-world scenes with geometry annotations. Experiments show that ShapeR significantly outperforms existing approaches in this challenging setting, achieving an improvement of 2.7x in Chamfer distance compared to state of the art.

Return JSON that matches this schema: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object"}. Respond with JSON only.
[19.01.2026 14:43] Response: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object", "description": "JSON object containing paper summary and title", "desc": "ShapeR uses visual-inertial SLAM, 3D detection, and vision-language models with a rectified flow transformer to generate high-fidelity 3D shapes from casual image sequences. It employs on-the-fly augmentations, curriculum training, and handling of background clutter to improve performance. The method achieves significant improvements over previous approaches in real-world settings.", "title": "High-Fidelity 3D Shape Generation from Casual Image Sequences Using Multimodal Conditioning"}
[19.01.2026 14:43] Querying the API.
[19.01.2026 14:43] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research.  					AI-generated summary 				 Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research.
[19.01.2026 14:43] Response: {
  "desc": "Статья посвящена созданию нового комплекса испытаний — AstroReason-Bench, предназначенного для оценки возможностей агентных больших языковых моделей (агентных LLM) в решении сложных задач планирования, связанных с космосом и строгими физическими ограничениями. Этот комплекс включает разнообразные режимы планирования, такие как управление коммуникацией наземных станций и гибкое наблюдение Земли из космоса, а также унифицированный протокол взаимодействия агентов. Исследование показывает, что современные универсальные модели планирования значительно уступают специализированным системам при работе в реальных условиях, подчеркивая необходимость дальнейшего развития и адаптации агентных LLM.",
  "emoji": "🚀",
  "title": "AstroReason-Bench: испытание агентных LLM в задачах космического планирования"
}
[19.01.2026 14:43] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension
OPEN_SOURCE: Papers that contribute to open-source projects
SCIENCE: Papers on scientific applications of ML
LOW_RESOURCE: Papers mentioning low-resource settings or languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research.  					AI-generated summary 				 Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research."

[19.01.2026 14:43] Response: [
    "AGENTS",
    "BENCHMARK"
]
[19.01.2026 14:43] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research.  					AI-generated summary 				 Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research."

[19.01.2026 14:43] Response: [
    "REASONING",
    "AGENT_MEMORY"
]
[19.01.2026 14:43] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper (4 sentences), use correct machine learning terms. 'title': a slogan of a main idea of the article. Return only JSON and nothing else.

Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research.  					AI-generated summary 				 Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research.

Return JSON that matches this schema: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object"}. Respond with JSON only.
[19.01.2026 14:43] Response: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object", "title": "AstroReason-Bench: A Benchmark for Agentic Planning in Space Planning Problems", "desc": "This paper introduces AstroReason-Bench, a new benchmark designed to evaluate the capabilities of agentic Large Language Models (LLMs) in handling complex, physically constrained real-world problems such as space planning. It focuses on Space Planning Problems (SPP), which involve heterogeneous objectives, strict physical constraints, and long-term decision-making. The benchmark includes various scheduling scenarios like ground station communication and agile Earth observation, providing a unified interaction protocol for agents. The evaluation reveals significant limitations of current generalist LLMs compared to specialized solvers, underscoring the need for further advancements in agentic research."}
[19.01.2026 14:43] Querying the API.
[19.01.2026 14:43] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Controlling the language of thought in large language models increases output diversity by leveraging distinct thinking spaces across different languages, with mixed-language sampling providing superior results.  					AI-generated summary 				 Output diversity is crucial for Large Language Models as it underpins pluralism and creativity. In this work, we reveal that controlling the language used during model thinking-the language of thought-provides a novel and structural source of output diversity. Our preliminary study shows that different thinking languages occupy distinct regions in a model's thinking space. Based on this observation, we study two repeated sampling strategies under multilingual thinking-Single-Language Sampling and Mixed-Language Sampling-and conduct diversity evaluation on outputs that are controlled to be in English, regardless of the thinking language used. Across extensive experiments, we demonstrate that switching the thinking language from English to non-English languages consistently increases output diversity, with a clear and consistent positive correlation such that languages farther from English in the thinking space yield larger gains. We further show that aggregating samples across multiple thinking languages yields additional improvements through compositional effects, and that scaling sampling with linguistic heterogeneity expands the model's diversity ceiling. Finally, we show that these findings translate into practical benefits in pluralistic alignment scenarios, leading to broader coverage of cultural knowledge and value orientations in LLM outputs. Our code is publicly available at https://github.com/iNLP-Lab/Multilingual-LoT-Diversity.
[19.01.2026 14:43] Response: {
  "desc": "В данной работе исследуется влияние использования различных языков мышления на разнообразие выходных данных больших языковых моделей. Было обнаружено, что использование разных языков во время процесса мышления модели позволяет занимать различные области в ее пространстве размышлений, что приводит к увеличению разнообразия выходов. Эксперименты показали, что переключение с английского языка на другие языки дает положительный эффект, причем наибольший прирост наблюдается при использовании языков, наиболее удаленных от английского в пространстве мыслей модели. Дополнительное улучшение достигается за счет объединения образцов из нескольких языков, что подчеркивает важность лингвистического разнообразия для расширения возможностей и разнообразия моделей.",
  "emoji": "🌍",
  "title": "Управление языком мышления в больших языковых моделях для увеличения разнообразия"
}
[19.01.2026 14:43] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension
OPEN_SOURCE: Papers that contribute to open-source projects
SCIENCE: Papers on scientific applications of ML
LOW_RESOURCE: Papers mentioning low-resource settings or languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Controlling the language of thought in large language models increases output diversity by leveraging distinct thinking spaces across different languages, with mixed-language sampling providing superior results.  					AI-generated summary 				 Output diversity is crucial for Large Language Models as it underpins pluralism and creativity. In this work, we reveal that controlling the language used during model thinking-the language of thought-provides a novel and structural source of output diversity. Our preliminary study shows that different thinking languages occupy distinct regions in a model's thinking space. Based on this observation, we study two repeated sampling strategies under multilingual thinking-Single-Language Sampling and Mixed-Language Sampling-and conduct diversity evaluation on outputs that are controlled to be in English, regardless of the thinking language used. Across extensive experiments, we demonstrate that switching the thinking language from English to non-English languages consistently increases output diversity, with a clear and consistent positive correlation such that languages farther from English in the thinking space yield larger gains. We further show that aggregating samples across multiple thinking languages yields additional improvements through compositional effects, and that scaling sampling with linguistic heterogeneity expands the model's diversity ceiling. Finally, we show that these findings translate into practical benefits in pluralistic alignment scenarios, leading to broader coverage of cultural knowledge and value orientations in LLM outputs. Our code is publicly available at https://github.com/iNLP-Lab/Multilingual-LoT-Diversity."

[19.01.2026 14:43] Response: [
    "MULTILINGUAL",
    "OUTPUT_DIVERSITY",
    "LANGUAGE_OF_THOUGHT"
]
[19.01.2026 14:43] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Controlling the language of thought in large language models increases output diversity by leveraging distinct thinking spaces across different languages, with mixed-language sampling providing superior results.  					AI-generated summary 				 Output diversity is crucial for Large Language Models as it underpins pluralism and creativity. In this work, we reveal that controlling the language used during model thinking-the language of thought-provides a novel and structural source of output diversity. Our preliminary study shows that different thinking languages occupy distinct regions in a model's thinking space. Based on this observation, we study two repeated sampling strategies under multilingual thinking-Single-Language Sampling and Mixed-Language Sampling-and conduct diversity evaluation on outputs that are controlled to be in English, regardless of the thinking language used. Across extensive experiments, we demonstrate that switching the thinking language from English to non-English languages consistently increases output diversity, with a clear and consistent positive correlation such that languages farther from English in the thinking space yield larger gains. We further show that aggregating samples across multiple thinking languages yields additional improvements through compositional effects, and that scaling sampling with linguistic heterogeneity expands the model's diversity ceiling. Finally, we show that these findings translate into practical benefits in pluralistic alignment scenarios, leading to broader coverage of cultural knowledge and value orientations in LLM outputs. Our code is publicly available at https://github.com/iNLP-Lab/Multilingual-LoT-Diversity."

[19.01.2026 14:43] Response: [
    "INTERPRETABILITY",
    "LANGUAGE_MODELING"
]
[19.01.2026 14:43] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper (4 sentences), use correct machine learning terms. 'title': a slogan of a main idea of the article. Return only JSON and nothing else.

Controlling the language of thought in large language models increases output diversity by leveraging distinct thinking spaces across different languages, with mixed-language sampling providing superior results.  					AI-generated summary 				 Output diversity is crucial for Large Language Models as it underpins pluralism and creativity. In this work, we reveal that controlling the language used during model thinking-the language of thought-provides a novel and structural source of output diversity. Our preliminary study shows that different thinking languages occupy distinct regions in a model's thinking space. Based on this observation, we study two repeated sampling strategies under multilingual thinking-Single-Language Sampling and Mixed-Language Sampling-and conduct diversity evaluation on outputs that are controlled to be in English, regardless of the thinking language used. Across extensive experiments, we demonstrate that switching the thinking language from English to non-English languages consistently increases output diversity, with a clear and consistent positive correlation such that languages farther from English in the thinking space yield larger gains. We further show that aggregating samples across multiple thinking languages yields additional improvements through compositional effects, and that scaling sampling with linguistic heterogeneity expands the model's diversity ceiling. Finally, we show that these findings translate into practical benefits in pluralistic alignment scenarios, leading to broader coverage of cultural knowledge and value orientations in LLM outputs. Our code is publicly available at https://github.com/iNLP-Lab/Multilingual-LoT-Diversity.

Return JSON that matches this schema: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object"}. Respond with JSON only.
[19.01.2026 14:43] Response: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object", "description": "Explanation of the ML paper in simple terms using correct machine learning terminology", "example": {"desc": "This paper explores how controlling the language of thought in large language models enhances output diversity by utilizing distinct thinking spaces across various languages. It demonstrates that mixed-language sampling provides better results than single-language sampling. The study reveals that switching the thinking language from English to non-English languages increases output diversity, with languages farther from English yielding greater gains. Aggregating samples across multiple thinking languages improves diversity even more.", "title": "Increasing Output Diversity Through Multilingual Thinking Spaces in Large Language Models"}, "title": "Article"}
[19.01.2026 14:43] Querying the API.
[19.01.2026 14:43] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A three-stage pipeline decouples physical reasoning from visual synthesis in video generation, improving physical plausibility and motion controllability through distinct phases of reasoning, planning, and refinement.  					AI-generated summary 				 Recent diffusion-based video generation models can synthesize visually plausible videos, yet they often struggle to satisfy physical constraints. A key reason is that most existing approaches remain single-stage: they entangle high-level physical understanding with low-level visual synthesis, making it hard to generate content that require explicit physical reasoning. To address this limitation, we propose a training-free three-stage pipeline,PhyRPR:Phy\uline{Reason}--Phy\uline{Plan}--Phy\uline{Refine}, which decouples physical understanding from visual synthesis. Specifically, PhyReason uses a large multimodal model for physical state reasoning and an image generator for keyframe synthesis; PhyPlan deterministically synthesizes a controllable coarse motion scaffold; and PhyRefine injects this scaffold into diffusion sampling via a latent fusion strategy to refine appearance while preserving the planned dynamics. This staged design enables explicit physical control during generation. Extensive experiments under physics constraints show that our method consistently improves physical plausibility and motion controllability.
[19.01.2026 14:43] Response: {
  "desc": "В данной работе предложена трехэтапная архитектура PhyRPR для генерации видео, которая разделяет процессы физического рассуждения и визуального синтеза. На первом этапе PhyReason выполняется физическое рассуждение с использованием многомодальной модели, а на втором этапе PhyPlan синтезируется грубый план движения. Завершающий этап PhyRefine улучшает визуальное представление, сохраняя при этом запланированные физические свойства движений. Такой подход позволяет значительно повысить физическую правдоподобность и управляемость движений в генерируемых видео.",
  "emoji": "🔄",
  "title": "Трехэтапный подход PhyRPR для повышения физической правдоподобности и контроля движений в видео"
}
[19.01.2026 14:43] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension
OPEN_SOURCE: Papers that contribute to open-source projects
SCIENCE: Papers on scientific applications of ML
LOW_RESOURCE: Papers mentioning low-resource settings or languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A three-stage pipeline decouples physical reasoning from visual synthesis in video generation, improving physical plausibility and motion controllability through distinct phases of reasoning, planning, and refinement.  					AI-generated summary 				 Recent diffusion-based video generation models can synthesize visually plausible videos, yet they often struggle to satisfy physical constraints. A key reason is that most existing approaches remain single-stage: they entangle high-level physical understanding with low-level visual synthesis, making it hard to generate content that require explicit physical reasoning. To address this limitation, we propose a training-free three-stage pipeline,PhyRPR:Phy\uline{Reason}--Phy\uline{Plan}--Phy\uline{Refine}, which decouples physical understanding from visual synthesis. Specifically, PhyReason uses a large multimodal model for physical state reasoning and an image generator for keyframe synthesis; PhyPlan deterministically synthesizes a controllable coarse motion scaffold; and PhyRefine injects this scaffold into diffusion sampling via a latent fusion strategy to refine appearance while preserving the planned dynamics. This staged design enables explicit physical control during generation. Extensive experiments under physics constraints show that our method consistently improves physical plausibility and motion controllability."

[19.01.2026 14:43] Response: [
    "MULTIMODAL",
    "3D"
]
[19.01.2026 14:43] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A three-stage pipeline decouples physical reasoning from visual synthesis in video generation, improving physical plausibility and motion controllability through distinct phases of reasoning, planning, and refinement.  					AI-generated summary 				 Recent diffusion-based video generation models can synthesize visually plausible videos, yet they often struggle to satisfy physical constraints. A key reason is that most existing approaches remain single-stage: they entangle high-level physical understanding with low-level visual synthesis, making it hard to generate content that require explicit physical reasoning. To address this limitation, we propose a training-free three-stage pipeline,PhyRPR:Phy\uline{Reason}--Phy\uline{Plan}--Phy\uline{Refine}, which decouples physical understanding from visual synthesis. Specifically, PhyReason uses a large multimodal model for physical state reasoning and an image generator for keyframe synthesis; PhyPlan deterministically synthesizes a controllable coarse motion scaffold; and PhyRefine injects this scaffold into diffusion sampling via a latent fusion strategy to refine appearance while preserving the planned dynamics. This staged design enables explicit physical control during generation. Extensive experiments under physics constraints show that our method consistently improves physical plausibility and motion controllability."

[19.01.2026 14:43] Response: [
    "DIFFUSION",
    "PHYSICAL_REASONING"
]
[19.01.2026 14:43] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper (4 sentences), use correct machine learning terms. 'title': a slogan of a main idea of the article. Return only JSON and nothing else.

A three-stage pipeline decouples physical reasoning from visual synthesis in video generation, improving physical plausibility and motion controllability through distinct phases of reasoning, planning, and refinement.  					AI-generated summary 				 Recent diffusion-based video generation models can synthesize visually plausible videos, yet they often struggle to satisfy physical constraints. A key reason is that most existing approaches remain single-stage: they entangle high-level physical understanding with low-level visual synthesis, making it hard to generate content that require explicit physical reasoning. To address this limitation, we propose a training-free three-stage pipeline,PhyRPR:Phy\uline{Reason}--Phy\uline{Plan}--Phy\uline{Refine}, which decouples physical understanding from visual synthesis. Specifically, PhyReason uses a large multimodal model for physical state reasoning and an image generator for keyframe synthesis; PhyPlan deterministically synthesizes a controllable coarse motion scaffold; and PhyRefine injects this scaffold into diffusion sampling via a latent fusion strategy to refine appearance while preserving the planned dynamics. This staged design enables explicit physical control during generation. Extensive experiments under physics constraints show that our method consistently improves physical plausibility and motion controllability.

Return JSON that matches this schema: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object"}. Respond with JSON only.
[19.01.2026 14:43] Response: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object", "description": "Explanation of the Machine Learning Paper", "desc": "This paper introduces a three-stage pipeline called PhyRPR:Phy Reason-Phy Plan-Phy Refine, which decouples physical reasoning and visual synthesis in video generation. It enhances physical plausibility and motion controllability by separating reasoning, planning, and refinement stages. The PhyReason stage uses a multimodal model for physical state reasoning and an image generator for keyframe synthesis. The PhyPlan stage generates a controllable coarse motion scaffold, and the PhyRefine stage integrates this scaffold into diffusion sampling to refine appearance while maintaining planned dynamics.", "title": "Decoupling Physical Reasoning from Visual Synthesis in Video Generation"}
[19.01.2026 14:43] Querying the API.
[19.01.2026 14:43] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Output-entropy profiles computed from final-layer next-token probabilities serve as a scalable signal for monitoring LLM performance and prioritizing data acquisition under domain shifts.  					AI-generated summary 				 Deploying LLMs raises two coupled challenges: (1) monitoring - estimating where a model underperforms as traffic and domains drift - and (2) improvement - prioritizing data acquisition to close the largest performance gaps. We test whether an inference-time signal can estimate slice-level accuracy under domain shift. For each response, we compute an output-entropy profile from final-layer next-token probabilities (from top-k logprobs) and summarize it with eleven statistics. A lightweight classifier predicts instance correctness, and averaging predicted probabilities yields a domain-level accuracy estimate. We evaluate on ten STEM reasoning benchmarks with exhaustive train/test compositions (k in {1,2,3,4}; all "10 choose k" combinations), across nine LLMs from six families (3B-20B). Estimates often track held-out benchmark accuracy, and several models show near-monotonic ordering of domains. Output-entropy profiles are thus an accessible signal for scalable monitoring and for targeting data acquisition.
[19.01.2026 14:43] Response: {
  "desc": "В данной работе исследуется использование профилей энтропии вывода на основе вероятностей следующего токена для мониторинга производительности языковых моделей и приоритизации сбора данных при изменении доменов. Показано, что эти профили могут служить надежным сигналом для оценки точности модели в различных условиях и помогают эффективно направлять процесс улучшения качества работы моделей.",
  "emoji": "📊",
  "title": "Мониторинг и улучшение языковых моделей с помощью профилей энтропии вывода"
}
[19.01.2026 14:43] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension
OPEN_SOURCE: Papers that contribute to open-source projects
SCIENCE: Papers on scientific applications of ML
LOW_RESOURCE: Papers mentioning low-resource settings or languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Output-entropy profiles computed from final-layer next-token probabilities serve as a scalable signal for monitoring LLM performance and prioritizing data acquisition under domain shifts.  					AI-generated summary 				 Deploying LLMs raises two coupled challenges: (1) monitoring - estimating where a model underperforms as traffic and domains drift - and (2) improvement - prioritizing data acquisition to close the largest performance gaps. We test whether an inference-time signal can estimate slice-level accuracy under domain shift. For each response, we compute an output-entropy profile from final-layer next-token probabilities (from top-k logprobs) and summarize it with eleven statistics. A lightweight classifier predicts instance correctness, and averaging predicted probabilities yields a domain-level accuracy estimate. We evaluate on ten STEM reasoning benchmarks with exhaustive train/test compositions (k in {1,2,3,4}; all "10 choose k" combinations), across nine LLMs from six families (3B-20B). Estimates often track held-out benchmark accuracy, and several models show near-monotonic ordering of domains. Output-entropy profiles are thus an accessible signal for scalable monitoring and for targeting data acquisition."

[19.01.2026 14:43] Response: [
    "INFERENCE",
    "TRAINING"
]
[19.01.2026 14:43] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Output-entropy profiles computed from final-layer next-token probabilities serve as a scalable signal for monitoring LLM performance and prioritizing data acquisition under domain shifts.  					AI-generated summary 				 Deploying LLMs raises two coupled challenges: (1) monitoring - estimating where a model underperforms as traffic and domains drift - and (2) improvement - prioritizing data acquisition to close the largest performance gaps. We test whether an inference-time signal can estimate slice-level accuracy under domain shift. For each response, we compute an output-entropy profile from final-layer next-token probabilities (from top-k logprobs) and summarize it with eleven statistics. A lightweight classifier predicts instance correctness, and averaging predicted probabilities yields a domain-level accuracy estimate. We evaluate on ten STEM reasoning benchmarks with exhaustive train/test compositions (k in {1,2,3,4}; all "10 choose k" combinations), across nine LLMs from six families (3B-20B). Estimates often track held-out benchmark accuracy, and several models show near-monotonic ordering of domains. Output-entropy profiles are thus an accessible signal for scalable monitoring and for targeting data acquisition."

[19.01.2026 14:43] Response: [
    "INTERPRETABILITY",
    "OPTIMIZATION"
]
[19.01.2026 14:43] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper (4 sentences), use correct machine learning terms. 'title': a slogan of a main idea of the article. Return only JSON and nothing else.

Output-entropy profiles computed from final-layer next-token probabilities serve as a scalable signal for monitoring LLM performance and prioritizing data acquisition under domain shifts.  					AI-generated summary 				 Deploying LLMs raises two coupled challenges: (1) monitoring - estimating where a model underperforms as traffic and domains drift - and (2) improvement - prioritizing data acquisition to close the largest performance gaps. We test whether an inference-time signal can estimate slice-level accuracy under domain shift. For each response, we compute an output-entropy profile from final-layer next-token probabilities (from top-k logprobs) and summarize it with eleven statistics. A lightweight classifier predicts instance correctness, and averaging predicted probabilities yields a domain-level accuracy estimate. We evaluate on ten STEM reasoning benchmarks with exhaustive train/test compositions (k in {1,2,3,4}; all "10 choose k" combinations), across nine LLMs from six families (3B-20B). Estimates often track held-out benchmark accuracy, and several models show near-monotonic ordering of domains. Output-entropy profiles are thus an accessible signal for scalable monitoring and for targeting data acquisition.

Return JSON that matches this schema: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object"}. Respond with JSON only.
[19.01.2026 14:43] Response: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object", "description": "Explanation of the paper in simple terms using correct machine learning terminology", "example": {"desc": "This paper explores how output-entropy profiles derived from final-layer next-token probabilities can serve as a scalable signal for monitoring language model performance and prioritizing data acquisition during domain shifts.", "title": "Scalable Monitoring and Data Acquisition Prioritization Using Output-Entropy Profiles in Language Models"}},"desc": "This paper discusses how output-entropy profiles extracted from the final layer's next-token probabilities act as a scalable indicator for assessing language model performance and guiding data acquisition strategies when dealing with domain shifts. It evaluates these entropy-based signals across various large language models and demonstrates their effectiveness in approximating benchmark accuracy and ranking domains based on performance gaps.", "title": "Scalable Monitoring and Data Acquisition Prioritization Using Output-Entropy Profiles in Large Language Models" }
[19.01.2026 14:43] Error. Failed to parse JSON. Details: Extra data: line 1 column 648 (char 647). Response: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object", "description": "Explanation of the paper in simple terms using correct machine learning terminology", "example": {"desc": "This paper explores how output-entropy profiles derived from final-layer next-token probabilities can serve as a scalable signal for monitoring language model performance and prioritizing data acquisition during domain shifts.", "title": "Scalable Monitoring and Data Acquisition Prioritization Using Output-Entropy Profiles in Language Models"}},"desc": "This paper discusses how output-entropy profiles extracted from the final layer's next-token probabilities act as a scalable indicator for assessing language model performance and guiding data acquisition strategies when dealing with domain shifts. It evaluates these entropy-based signals across various large language models and demonstrates their effectiveness in approximating benchmark accuracy and ranking domains based on performance gaps.", "title": "Scalable Monitoring and Data Acquisition Prioritization Using Output-Entropy Profiles in Large Language Models" }
[19.01.2026 14:43] Querying the API.
[19.01.2026 14:43] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Large Vision Language Models exhibit significant limitations in multi-image understanding and reasoning, which are revealed through a new benchmark and addressed via procedural data generation and attention masking techniques.  					AI-generated summary 				 Large Vision Language Models (LVLMs) have demonstrated remarkable capabilities, yet their proficiency in understanding and reasoning over multiple images remains largely unexplored. While existing benchmarks have initiated the evaluation of multi-image models, a comprehensive analysis of their core weaknesses and their causes is still lacking. In this work, we introduce MIMIC (Multi-Image Model Insights and Challenges), a new benchmark designed to rigorously evaluate the multi-image capabilities of LVLMs. Using MIMIC, we conduct a series of diagnostic experiments that reveal pervasive issues: LVLMs often fail to aggregate information across images and struggle to track or attend to multiple concepts simultaneously. To address these failures, we propose two novel complementary remedies. On the data side, we present a procedural data-generation strategy that composes single-image annotations into rich, targeted multi-image training examples. On the optimization side, we analyze layer-wise attention patterns and derive an attention-masking scheme tailored for multi-image inputs. Experiments substantially improved cross-image aggregation, while also enhancing performance on existing multi-image benchmarks, outperforming prior state of the art across tasks. Data and code will be made available at https://github.com/anurag-198/MIMIC.
[19.01.2026 14:43] Response: {
  "desc": "В данной работе исследуются ограничения больших моделей взаимодействия зрения и языка (LVLM) при обработке нескольких изображений и проведении логического рассуждения. Для этого был создан новый бенчмарк под названием MIMIC, который выявляет основные проблемы таких моделей и предлагает два решения: процедурная генерация данных для создания сложных мульти-изображений и маскировка внимания на уровне слоев нейронной сети. Эксперименты показали значительное улучшение в объединении информации из разных изображений и повышении общей производительности моделей.",
  "emoji": "📊",
  "title": "Выявление и преодоление ограничений мультиизобразительных способностей больших моделей зрения и языка"
}
[19.01.2026 14:43] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension
OPEN_SOURCE: Papers that contribute to open-source projects
SCIENCE: Papers on scientific applications of ML
LOW_RESOURCE: Papers mentioning low-resource settings or languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large Vision Language Models exhibit significant limitations in multi-image understanding and reasoning, which are revealed through a new benchmark and addressed via procedural data generation and attention masking techniques.  					AI-generated summary 				 Large Vision Language Models (LVLMs) have demonstrated remarkable capabilities, yet their proficiency in understanding and reasoning over multiple images remains largely unexplored. While existing benchmarks have initiated the evaluation of multi-image models, a comprehensive analysis of their core weaknesses and their causes is still lacking. In this work, we introduce MIMIC (Multi-Image Model Insights and Challenges), a new benchmark designed to rigorously evaluate the multi-image capabilities of LVLMs. Using MIMIC, we conduct a series of diagnostic experiments that reveal pervasive issues: LVLMs often fail to aggregate information across images and struggle to track or attend to multiple concepts simultaneously. To address these failures, we propose two novel complementary remedies. On the data side, we present a procedural data-generation strategy that composes single-image annotations into rich, targeted multi-image training examples. On the optimization side, we analyze layer-wise attention patterns and derive an attention-masking scheme tailored for multi-image inputs. Experiments substantially improved cross-image aggregation, while also enhancing performance on existing multi-image benchmarks, outperforming prior state of the art across tasks. Data and code will be made available at https://github.com/anurag-198/MIMIC."

[19.01.2026 14:43] Response: [
    "DATA",
    "BENCHMARK"
]
[19.01.2026 14:43] Gigachat request. Model: GigaChat. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages
MEMORY: Works on internal/external memory for LLMs
AGENT_MEMORY: Memory and long-term context in agents
SESSION_MEMORY: Dialogue/session memory across turns
RETRIEVAL: Retrieval models and retrievers (distinct from full RAG)
RETRIEVAL_EVAL: Benchmarks/metrics for retrieval robustness
PROMPTING: Prompt engineering, CoT/ToT, self-consistency
PROMPT_CACHING: KV/response caching for long sessions
CONTEXT_WINDOW: Techniques to extend/pack context
COMPRESSION: Context compression/summarization/chunking for memory
MEMORIZATION: On-the-fly learning/continual memorization
CONTINUAL_LEARNING: Continual learning without forgetting
KNOWLEDGE_GRAPHS: Knowledge storage via graphs
TOOL_USE: Using external tools as memory/extension

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large Vision Language Models exhibit significant limitations in multi-image understanding and reasoning, which are revealed through a new benchmark and addressed via procedural data generation and attention masking techniques.  					AI-generated summary 				 Large Vision Language Models (LVLMs) have demonstrated remarkable capabilities, yet their proficiency in understanding and reasoning over multiple images remains largely unexplored. While existing benchmarks have initiated the evaluation of multi-image models, a comprehensive analysis of their core weaknesses and their causes is still lacking. In this work, we introduce MIMIC (Multi-Image Model Insights and Challenges), a new benchmark designed to rigorously evaluate the multi-image capabilities of LVLMs. Using MIMIC, we conduct a series of diagnostic experiments that reveal pervasive issues: LVLMs often fail to aggregate information across images and struggle to track or attend to multiple concepts simultaneously. To address these failures, we propose two novel complementary remedies. On the data side, we present a procedural data-generation strategy that composes single-image annotations into rich, targeted multi-image training examples. On the optimization side, we analyze layer-wise attention patterns and derive an attention-masking scheme tailored for multi-image inputs. Experiments substantially improved cross-image aggregation, while also enhancing performance on existing multi-image benchmarks, outperforming prior state of the art across tasks. Data and code will be made available at https://github.com/anurag-198/MIMIC."

[19.01.2026 14:43] Response: [
    "INTERPRETABILITY",
    "REASONING"
]
[19.01.2026 14:43] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper (4 sentences), use correct machine learning terms. 'title': a slogan of a main idea of the article. Return only JSON and nothing else.

Large Vision Language Models exhibit significant limitations in multi-image understanding and reasoning, which are revealed through a new benchmark and addressed via procedural data generation and attention masking techniques.  					AI-generated summary 				 Large Vision Language Models (LVLMs) have demonstrated remarkable capabilities, yet their proficiency in understanding and reasoning over multiple images remains largely unexplored. While existing benchmarks have initiated the evaluation of multi-image models, a comprehensive analysis of their core weaknesses and their causes is still lacking. In this work, we introduce MIMIC (Multi-Image Model Insights and Challenges), a new benchmark designed to rigorously evaluate the multi-image capabilities of LVLMs. Using MIMIC, we conduct a series of diagnostic experiments that reveal pervasive issues: LVLMs often fail to aggregate information across images and struggle to track or attend to multiple concepts simultaneously. To address these failures, we propose two novel complementary remedies. On the data side, we present a procedural data-generation strategy that composes single-image annotations into rich, targeted multi-image training examples. On the optimization side, we analyze layer-wise attention patterns and derive an attention-masking scheme tailored for multi-image inputs. Experiments substantially improved cross-image aggregation, while also enhancing performance on existing multi-image benchmarks, outperforming prior state of the art across tasks. Data and code will be made available at https://github.com/anurag-198/MIMIC.

Return JSON that matches this schema: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object"}. Respond with JSON only.
[19.01.2026 14:43] Response: {"properties": {"desc": {"title": "Desc", "type": "string"}, "title": {"title": "Title", "type": "string"}}, "required": ["desc", "title"], "title": "Article", "type": "object", "desc": "This paper highlights the limitations of Large Vision Language Models (LVLMs) in multi-image understanding and reasoning. It introduces a new benchmark called MIMIC to assess these limitations and reveals that LVLMs struggle to aggregate information across images and track multiple concepts simultaneously. The authors propose two solutions: procedural data generation and attention masking techniques to enhance cross-image aggregation and improve overall performance.", "title": "Improving Multi-Image Understanding and Reasoning with Procedural Data Generation and Attention Masking"}
[19.01.2026 14:43] Renaming data file.
[19.01.2026 14:43] Renaming previous data. hf_papers.json to ./d/2026-01-19.json
[19.01.2026 14:43] Saving new data file.
[19.01.2026 14:43] Generating page.
[19.01.2026 14:43] Renaming previous page.
[19.01.2026 14:43] Renaming previous data. index.html to ./d/2026-01-19.html
[19.01.2026 14:43] Writing result.
[19.01.2026 14:43] Renaming log file.
[19.01.2026 14:43] Renaming previous data. log.txt to ./logs/2026-01-19_last_log.txt
