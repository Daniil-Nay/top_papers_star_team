[25.01.2026 21:18] Read previous papers.
[25.01.2026 21:18] Generating top page (month).
[25.01.2026 21:18] Writing top page (month).
[25.01.2026 22:15] Read previous papers.
[25.01.2026 22:15] Get feed.
[25.01.2026 22:15] Extract page data from URL. URL: https://huggingface.co/papers/2601.15876
[25.01.2026 22:15] Extract page data from URL. URL: https://huggingface.co/papers/2601.14724
[25.01.2026 22:15] Extract page data from URL. URL: https://huggingface.co/papers/2601.16206
[25.01.2026 22:15] Extract page data from URL. URL: https://huggingface.co/papers/2601.15165
[25.01.2026 22:15] Extract page data from URL. URL: https://huggingface.co/papers/2601.15197
[25.01.2026 22:15] Extract page data from URL. URL: https://huggingface.co/papers/2601.16208
[25.01.2026 22:15] Extract page data from URL. URL: https://huggingface.co/papers/2601.15892
[25.01.2026 22:15] Extract page data from URL. URL: https://huggingface.co/papers/2601.16093
[25.01.2026 22:15] Extract page data from URL. URL: https://huggingface.co/papers/2601.16175
[25.01.2026 22:15] Extract page data from URL. URL: https://huggingface.co/papers/2601.15621
[25.01.2026 22:15] Extract page data from URL. URL: https://huggingface.co/papers/2601.11868
[25.01.2026 22:15] Extract page data from URL. URL: https://huggingface.co/papers/2601.15369
[25.01.2026 22:15] Extract page data from URL. URL: https://huggingface.co/papers/2601.15727
[25.01.2026 22:15] Extract page data from URL. URL: https://huggingface.co/papers/2601.16125
[25.01.2026 22:15] Extract page data from URL. URL: https://huggingface.co/papers/2601.16148
[25.01.2026 22:15] Extract page data from URL. URL: https://huggingface.co/papers/2601.14255
[25.01.2026 22:15] Extract page data from URL. URL: https://huggingface.co/papers/2601.16163
[25.01.2026 22:15] Extract page data from URL. URL: https://huggingface.co/papers/2601.15224
[25.01.2026 22:15] Extract page data from URL. URL: https://huggingface.co/papers/2601.16192
[25.01.2026 22:15] Extract page data from URL. URL: https://huggingface.co/papers/2601.15703
[25.01.2026 22:15] Extract page data from URL. URL: https://huggingface.co/papers/2601.15778
[25.01.2026 22:15] Extract page data from URL. URL: https://huggingface.co/papers/2601.15690
[25.01.2026 22:15] Extract page data from URL. URL: https://huggingface.co/papers/2601.15549
[25.01.2026 22:15] Extract page data from URL. URL: https://huggingface.co/papers/2601.16134
[25.01.2026 22:15] Extract page data from URL. URL: https://huggingface.co/papers/2601.16004
[25.01.2026 22:15] Extract page data from URL. URL: https://huggingface.co/papers/2601.15440
[25.01.2026 22:15] Extract page data from URL. URL: https://huggingface.co/papers/2601.08118
[25.01.2026 22:15] Updating GitHub stars.
[25.01.2026 22:15] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[25.01.2026 22:15] No deleted papers detected.
[25.01.2026 22:15] Downloading and parsing papers (pdf, html). Total: 27.
[25.01.2026 22:15] Downloading and parsing paper https://huggingface.co/papers/2601.15876.
[25.01.2026 22:15] Downloading paper 2601.15876 from https://arxiv.org/pdf/2601.15876v1...
[25.01.2026 22:15] Extracting affiliations from text.
[25.01.2026 22:15] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience Taofeng Xue*,1, Chong Peng*,1, Mianqiu Huang*1,2, Linsen Guo1, Tiancheng Han1,3, Haozhe Wang1,4, Jianing Wang1, Xiaocheng Zhang1, Xin Yang1, Dengchang Zhao1, Jinrui Ding1, Xiandi Ma1, Yuchen Xie1, Peng Pei1, Xunliang Cai1, Xipeng Qiu2 1Meituan 2 Fudan University 3Tongji University 4The Hong Kong University of Science and Technology "
[25.01.2026 22:15] Failed to download and parse paper https://huggingface.co/papers/2601.15876: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:15] Downloading and parsing paper https://huggingface.co/papers/2601.14724.
[25.01.2026 22:15] Downloading paper 2601.14724 from https://arxiv.org/pdf/2601.14724v1...
[25.01.2026 22:15] Extracting affiliations from text.
[25.01.2026 22:15] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 1 2 ] . [ 1 4 2 7 4 1 . 1 0 6 2 : r HERMES: KV Cache as Hierarchical Memory for Haowei Zhang1, Shudong Yang1,2, Jinlan Fu1,3, See-Kiong Ng3 Xipeng Qiu1,2 1Fudan University, 2Shanghai Innovation Institute, 3National University of Singapore "
[25.01.2026 22:15] Failed to download and parse paper https://huggingface.co/papers/2601.14724: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:15] Downloading and parsing paper https://huggingface.co/papers/2601.16206.
[25.01.2026 22:15] Downloading paper 2601.16206 from https://arxiv.org/pdf/2601.16206v1...
[25.01.2026 22:15] Extracting affiliations from text.
[25.01.2026 22:15] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"LLM-in-Sandbox Elicits General Agentic Intelligence Daixuan Chengαβ Shaohan Huangβ Yuxian Guγ Huatong Songα Guoxin Chenα Li Dongβ Wayne Xin Zhaoα Ji-Rong Wenα Furu Weiβ αGSAI, Renmin University of China βMicrosoft Research γTsinghua University https://llm-in-sandbox.github.io "
[25.01.2026 22:15] Failed to download and parse paper https://huggingface.co/papers/2601.16206: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:15] Downloading and parsing paper https://huggingface.co/papers/2601.15165.
[25.01.2026 22:15] Downloading paper 2601.15165 from https://arxiv.org/pdf/2601.15165v1...
[25.01.2026 22:15] Extracting affiliations from text.
[25.01.2026 22:15] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models January 22, 2026 Zanlin Ni 1 , Shenzhi Wang 1, Yang Yue 1, Tianyu Yu 2, Weilin Zhao 2, Yeguo Hua 3, Tianyi Chen 3, Jun Song 4, Cheng Yu 4, Bo Zheng 4, Gao Huang 1 (cid:66) 1 LeapLab, Tsinghua University 3 Tsinghua University 2 NLPLab, Tsinghua University 4 Alibaba Group nzl22@mails.tsinghua.edu.cn, gaohuang@tsinghua.edu.cn Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL)1 to elicit the reasoning capability of dLLMs. In this paper, we reveal counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap 6 2 0 2 1 2 ] . [ 1 5 6 1 5 1 . 1 0 6 2 : r Figure 1: Less flexibility unlocks better reasoning potential. Left: We observe coun"
[25.01.2026 22:15] Failed to download and parse paper https://huggingface.co/papers/2601.15165: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:15] Downloading and parsing paper https://huggingface.co/papers/2601.15197.
[25.01.2026 22:15] Downloading paper 2601.15197 from https://arxiv.org/pdf/2601.15197v2...
[25.01.2026 22:15] Extracting affiliations from text.
[25.01.2026 22:15] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 2 2 ] . [ 2 7 9 1 5 1 . 1 0 6 2 : r 2026-01-23 Work in progress. BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries Shijie Lian1,2,* Bin Yu2,4,* Xiaopeng Lin2,5,* Laurence T. Yang6,1, Zhaolong Shen2,7 Changti Wu2,8 Yuzhuo Miao1,2 Cong Huang2,3 Kai Chen2,3,9, 1HUST 2ZGCA 3ZGCI 4HIT 5HKUST(GZ) 6ZZU 7BUAA 8ECNU 9DeepCybo https://github.com/ZGC-EmbodyAI/BayesianVLA "
[25.01.2026 22:15] Failed to download and parse paper https://huggingface.co/papers/2601.15197: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:15] Downloading and parsing paper https://huggingface.co/papers/2601.16208.
[25.01.2026 22:15] Downloading paper 2601.16208 from https://arxiv.org/pdf/2601.16208v1...
[25.01.2026 22:15] Extracting affiliations from text.
[25.01.2026 22:15] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders Shengbang Tong*, Boyang Zheng*, Ziteng Wang*, Bingda Tang, Nanye Ma, Ellis Brown, Jihan Yang, Rob Fergus, Yann LeCun, Saining Xie New York University Website Code Models Data 6 2 0 2 2 2 ] . [ 1 8 0 2 6 1 . 1 0 6 2 : r a "
[25.01.2026 22:15] Failed to download and parse paper https://huggingface.co/papers/2601.16208: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:15] Downloading and parsing paper https://huggingface.co/papers/2601.15892.
[25.01.2026 22:15] Downloading paper 2601.15892 from https://arxiv.org/pdf/2601.15892v1...
[25.01.2026 22:15] Extracting affiliations from text.
[25.01.2026 22:15] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model Chenghao Fan1,2, Wen Heng2, Bo Li2, Sichen Liu1, Yuxuan Song2, Jing Su2, Xiaoye Qu1, Kai Shen2, Wei Wei1 1Huazhong University of Science and Technology, 2ByteDance Seed "
[25.01.2026 22:15] Failed to download and parse paper https://huggingface.co/papers/2601.15892: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:15] Downloading and parsing paper https://huggingface.co/papers/2601.16093.
[25.01.2026 22:15] Downloading paper 2601.16093 from https://arxiv.org/pdf/2601.16093v1...
[25.01.2026 22:16] Extracting affiliations from text.
[25.01.2026 22:16] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"SAMTok: Representing Any Mask with Two Words Yikang Zhou1,2 , Tao Zhang1,2 , Dengxian Gong1 , Yuanzheng Wu1 , Ye Tian2 , Haochen Wang2 , Haobo Yuan2 , Jiacong Wang2 , Lu Qi1 , Hao Fei3 , Anran Wang2 , Zhuochen Wang2 , Yujing Wang2 , Cheng Chen2 , Shunping Ji1 , Xiangtai Li2 Wuhan University1 ByteDance2 NUS3 : Equal contributions : Corresponding Author Project Page: https://zhouyiks.github.io/projects/SAMTok/ "
[25.01.2026 22:16] Failed to download and parse paper https://huggingface.co/papers/2601.16093: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2601.16175.
[25.01.2026 22:16] Downloading paper 2601.16175 from https://arxiv.org/pdf/2601.16175v1...
[25.01.2026 22:16] Extracting affiliations from text.
[25.01.2026 22:16] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"1, Daniel Koceja 1, Xinhao Li 4, Federico Bianchi 5 Jed McCaleb3, Xiaolong Wang4, Jan Kautz2, Yejin Choi2, James Zou 1,5, Carlos Guestrin 1, Yu Sun 1, "
[25.01.2026 22:16] Failed to download and parse paper https://huggingface.co/papers/2601.16175: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2601.15621.
[25.01.2026 22:16] Downloading paper 2601.15621 from https://arxiv.org/pdf/2601.15621v1...
[25.01.2026 22:16] Extracting affiliations from text.
[25.01.2026 22:16] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 2 2 ] . [ 1 1 2 6 5 1 . 1 0 6 2 : r 2026-01-23 Qwen3-TTS Technical Report Qwen Team https://huggingface.co/collections/Qwen/qwen3-tts https://modelscope.cn/collections/Qwen/Qwen3-TTS https://github.com/QwenLM/Qwen3-TTS "
[25.01.2026 22:16] Failed to download and parse paper https://huggingface.co/papers/2601.15621: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2601.11868.
[25.01.2026 22:16] Downloading paper 2601.11868 from https://arxiv.org/pdf/2601.11868v1...
[25.01.2026 22:16] Extracting affiliations from text.
[25.01.2026 22:16] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 7 1 ] . [ 1 8 6 8 1 1 . 1 0 6 2 : r TERMINAL-BENCH: BENCHMARKING AGENTS ON HARD, REALISTIC TASKS IN COMMAND LINE INTERFACES Mike A. Merrill1,*, Alexander G. Shaw2,*, Nicholas Carlini3, Boxuan Li4, Harsh Raj5, Ivan Bercovich6, Lin Shi7, Jeong Yeon Shin1,8, Thomas Walshe9, E. Kelly Buchanan1 Junhong Shen10, Guanghao Ye11, Haowei Lin12, Jason Poulos4, Maoyu Wang4, Marianna Nezhurina13,14, Jenia Jitsev13,14, Di Lu15, Orfeas Menis Mastromichalakis16,17, Zhiwei Xu18, Zizhao Chen7, Yue Liu19,20, Robert Zhang21, Leon Liangyu Chen1, Anurag Kashyap22, Jan-Lucas Uslu1, Jeffrey Li23, Jianbo Wu4, Minghao Yan24, Song Bian24, Vedang Sharma4, Ke Sun4, Steven Dillmann1, Akshay Anand25, Andrew Lanpouthakoun1, Bardia Koopah25, Changran Hu26, Etash Guha1,23, Gabriel H. S. Dreiman4, Jiacheng Zhu4, Karl Krauth1, Li Zhong3, Niklas Muennighoff1, Robert Amanfu4, Shangyin Tan25, Shreyas Pimpalgaonkar27, Tushar Aggarwal1, Xiangning Lin10, Xin Lan28, Xuandong Zhao25, Yiqing Liang29, Yuanli Wang30, Zilong Wang31 Changzhi Zhou32, David Heineman33, Hange Liu4, Harsh Trivedi33, John Yang1, Junhong Lin11, Manish Shetty25, Michael Yang6, Nabil Omi23, Negin Raoof25, Shanda Li10, Terry Yue Zhuo34,35, Wuwei Lin4, Yiwei Dai7, Yuxin Wang36, Wenhao Chai37, Shang Zhou31, Dariush Wahdany38, Ziyu She39, Jiaming Hu30, Zhikang Dong40, Yuxuan Zhu41, Sasha Cui42, Ahson Saiyed43, Arinbjorn Kolbeinsson43, Jesse Hu 44, Christopher Michael Rytting2, Ryan Marten27, Yixin Wang18, Alex Dimakis25,27 Andy Konwinski2, Ludwig Schmidt1 1Stanford University, 2Laude Institute, 3Anthropic, 4Independent, 5Northeastern University, 6University of California, Santa Barbara, 7Cornell University, 8Snorkel AI, 9Reflection AI, 10Carnegie Mellon University, 11Massachusetts Institute of Technology, 12Peking University, 13LAION, 14JSC, FZJ, 15Tencent, 16National Technical University of Athens, 17Nerion, 18University of Michigan, 19National University of Singapore, 20Moonshot AI, 21University of Texas at Austin, 22Amazon, 23Univer"
[25.01.2026 22:16] Failed to download and parse paper https://huggingface.co/papers/2601.11868: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2601.15369.
[25.01.2026 22:16] Downloading paper 2601.15369 from https://arxiv.org/pdf/2601.15369v1...
[25.01.2026 22:16] Extracting affiliations from text.
[25.01.2026 22:16] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 1 2 ] . e [ 1 9 6 3 5 1 . 1 0 6 2 : r OpenVision 3 : Family of Unified Visual Encoder for Letian Zhang* 1 Huaxiu Yao3 Sucheng Ren* 2 Zeyu Zheng4 Yanqing Liu1 Xianhang Li1 Zeyu Wang1 Yuyin Zhou Weili Nie5 Guilin Liu5 Zhiding Yu5 Cihang Xie1 1UC Santa Cruz 2JHU 3UNC-Chapel Hill 4UC Berkeley 5NVIDIA Project Page: https://ucsc-vlaa.github.io/OpenVision3/ Figure 1. An overview of OpenVision 3s architecture design and performance highlight. Left panel: The architecture of OpenVision 3. We employ frozen VAE and trainable ViT as the unified tokenizer, which produces tokens that are fed simultaneously into both the generation and understanding branches. Middle panel: The learning objectives of the generation branch and the understanding branch. For the generation branch, we focus on high-quality, pixel-level image reconstruction; concurrently, the understanding branch is optimized via joint contrastive learning and captioning objectives. Right panel: The performance summarization shows that OpenVision 3 outperforms other unified tokenizers and semantics-based encoders in rFID and gFID, while remaining competitive with CLIP in multimodal understanding ability. Abstract This paper presents family of advanced vision encoder, named OpenVision 3, that learns single, unified visual representation that can serve both image understanding and image generation. Our core architecture is simple: we feed VAE-compressed image latents to ViT encoder and train its output to support two complementary roles. First, the encoder output is passed to the ViT-VAE decoder to reconstruct the original image, encouraging the representation to capture generative structure. Second, the same representation is optimized with contrastive learning and image-captioning objectives, strengthening semantic features. By jointly optimizing reconstructionand semantics-driven signals in shared latent space, the encoder learns representations that synergize and generalize well across both regimes. We validat"
[25.01.2026 22:16] Failed to download and parse paper https://huggingface.co/papers/2601.15369: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2601.15727.
[25.01.2026 22:16] Downloading paper 2601.15727 from https://arxiv.org/pdf/2601.15727v1...
[25.01.2026 22:16] Extracting affiliations from text.
[25.01.2026 22:16] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 2 2 ] . [ 1 7 2 7 5 1 . 1 0 6 2 : r a Yang Yu1 , Peiyu Zang1,2 , Chi Hsu Tsai1,3 , Haiming Wu1,4 , Yixin Shen1,5 , Jialing Zhang1,6 , Haoyu Wang1,7 , Zhiyou Xiao1,3 , Jingze Shi8 , Yuyu Luo8 , Wentao Zhang3 , Chunlei Men1 , Guang Liu1 and Yonghua Lin1 1Beijing Academy of Artificial Intelligence 2Beijing Normal University 3Peking University 4Beijing Institute of Technology 5Cornell University 6Beijing Jiaotong University 7Renmin University of China 8Hong Kong University of Science and Technology (Guangzhou) "
[25.01.2026 22:16] Failed to download and parse paper https://huggingface.co/papers/2601.15727: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2601.16125.
[25.01.2026 22:16] Downloading paper 2601.16125 from https://arxiv.org/pdf/2601.16125v1...
[25.01.2026 22:16] Extracting affiliations from text.
[25.01.2026 22:16] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 2 2 ] . [ 1 5 2 1 6 1 . 1 0 6 2 : r Rethinking Composed Image Retrieval Evaluation: Fine-Grained Benchmark from Image Editing Tingyu Song 123 Yanzhao Zhang 2 Mingxin Li 2 Zhuoning Guo42 Dingkun Long 2 Pengjun Xie 2 Siyue Zhang 5 Yilun Zhao6 Shu Wu13 1CASIA 2Tongyi Lab, Alibaba Group 3UCAS 4HKUST(GZ) 5NTU 6Yale "
[25.01.2026 22:16] Failed to download and parse paper https://huggingface.co/papers/2601.16125: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2601.16148.
[25.01.2026 22:16] Downloading paper 2601.16148 from https://arxiv.org/pdf/2601.16148v1...
[25.01.2026 22:16] Extracting affiliations from text.
[25.01.2026 22:16] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion Remy Sabathier1,3 1Meta Reality Labs David Novotny2 2SpAItial Niloy J. Mitra3 3University College London Tom Monnier https://remysabathier.github.io/actionmesh/ 6 2 0 2 2 2 ] . [ 1 8 4 1 6 1 . 1 0 6 2 : r Figure 1. ActionMesh. Our model generates 3D meshes in action from wide range of inputs such as text prompt, video, an image + an animation text prompt, or 3D mesh + an animation text prompt. Unlike previous approaches, our method is not only fast, but also rig-free and topology consistent. These properties are convenient in practice, e.g., they allow the seamless animation of complex 3D shapes like an octopus with maracas (top left) or the automatic transfer of the mesh texture throughout the animation (bottom right). "
[25.01.2026 22:16] Failed to download and parse paper https://huggingface.co/papers/2601.16148: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2601.14255.
[25.01.2026 22:16] Downloading paper 2601.14255 from https://arxiv.org/pdf/2601.14255v1...
[25.01.2026 22:16] Extracting affiliations from text.
[25.01.2026 22:16] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 0 2 ] . [ 1 5 5 2 4 1 . 1 0 6 2 : r VideoMaMa: Mask-Guided Video Matting via Generative Prior Sangbeom Lim1 Seoung Wug Oh2 Jiahui Huang2 Heeji Yoon3 Seungryong Kim Joon-Young Lee2 1Korea University 2Adobe Research 3KAIST AI https://cvlab-kaist.github.io/VideoMaMa Figure 1. We introduce Video Mask-to-Matte Model (VideoMaMa), diffusion-based model that generates high-quality alpha mattes from input binary segmentation masks obtained either from existing models such as SAM2 [30] or from ground-truth segmentation masks in existing datasets such as SA-V [30]. Examples shown highlights our VideoMaMas ability to capture fine-grained details including motion blur, and intricate boundary structures on natural video footage. "
[25.01.2026 22:16] Failed to download and parse paper https://huggingface.co/papers/2601.14255: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2601.16163.
[25.01.2026 22:16] Downloading paper 2601.16163 from https://arxiv.org/pdf/2601.16163v1...
[25.01.2026 22:16] Extracting affiliations from text.
[25.01.2026 22:16] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 2 2 ] . [ 1 3 6 1 6 1 . 1 0 6 2 : r COSMOS POLICY: FINE-TUNING VIDEO MODELS FOR VISUOMOTOR CONTROL AND PLANNING Moo Jin Kim1,2 Yihuai Gao1,2 Tsung-Yi Lin1 Yen-Chen Lin1 Yunhao Ge1 Grace Lam1 Percy Liang2 Shuran Song1,2 Ming-Yu Liu1 Chelsea Finn2 Jinwei Gu1 1NVIDIA 2Stanford University https://research.nvidia.com/labs/dir/cosmos-policy/ Figure 1: We present Cosmos Policy, state-of-the-art robot policy fine-tuned from the NVIDIA CosmosPredict2-2B video foundation model. Cosmos Policy handles multimodal inputs and multi-view camera images and predicts (1) robot action chunk, (2) future state (represented by robot proprioception and image observations), and (3) value (expected rewards-to-go at the future state). No architectural changes are made to the base video model, and all modalities are jointly modeled through the video diffusion learning objective. "
[25.01.2026 22:16] Failed to download and parse paper https://huggingface.co/papers/2601.16163: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2601.15224.
[25.01.2026 22:16] Downloading paper 2601.15224 from https://arxiv.org/pdf/2601.15224v1...
[25.01.2026 22:17] Extracting affiliations from text.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 1 2 ] . [ 1 4 2 2 5 1 . 1 0 6 2 : r PROGRESSLM: Towards Progress Reasoning in Vision-Language Models Jianshu Zhang1 Chengxuan Qian2 Haosen Sun1 Haoran Lu1 Dingcheng Wang1 Letian Xue1 Han Liu1 1Northwestern University (cid:128) Website Code (cid:242) Model ı Dataset 2Arcadia University Figure 1: Given task demonstration and single observation, the goal is to estimate how much of the task has already been completed. Direct prediction can often judge whether the task is unfinished, but struggles to assign well-calibrated progress score. Progress reasoning instead follows coarse-to-fine process: it first performs episodic retrieval to coarsely locate the observation along the demonstrated task, then applies mental simulation to imagine the transition from the retrieved anchor to the current observation, enabling fine-grained estimate of completed progress, which enables accurate and interpretable progress estimation. Abstract Estimating task progress requires reasoning over long-horizon dynamics rather than recognizing static visual content. While modern Vision-Language Models (VLMs) excel at describing what is visible, it remains unclear whether they can infer how far task has progressed from partial observations. To this end, we introduce PROGRESS-BENCH, benchmark for systematically evaluating progress reasoning in VLMs. Beyond benchmarking, we further explore humaninspired two-stage progress reasoning paradigm through both training-free prompting and training-based approach based on curated dataset PROGRESSLM-45K. Experiments on 14 VLMs show that most models are not yet ready for task progress estimation, exhibiting sensitivity to demonstration modality and viewpoint changes, as well as poor handling of unanswerable cases. While training-free prompting that enforces structured progress reasoning yields limited and model-dependent gains, the training-based PROGRESSLM-3B achieves consistent improvements even at small model scale, despite being trained on task s"
[25.01.2026 22:17] Failed to download and parse paper https://huggingface.co/papers/2601.15224: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Downloading and parsing paper https://huggingface.co/papers/2601.16192.
[25.01.2026 22:17] Downloading paper 2601.16192 from https://arxiv.org/pdf/2601.16192v1...
[25.01.2026 22:17] Extracting affiliations from text.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"360Anything: Geometry-Free Lifting of Images and Videos to 360 Ziyi Wu1,3, Daniel Watson1, Andrea Tagliasacchi2,3,, David J. Fleet1,3, Marcus A. Brubaker1, and Saurabh Saxena1 1 Google DeepMind 2 Simon Fraser University 3 University of Toronto Abstract. Lifting perspective images and videos to 360 panoramas enables immersive 3D world generation. Existing approaches often rely on explicit geometric alignment between the perspective and the equirectangular projection (ERP) space. Yet, this requires known camera metadata, obscuring the application to in-the-wild data where such calibration is typically absent or noisy. We propose 360Anything, geometry-free framework built upon pre-trained diffusion transformers. By treating the perspective input and the panorama target simply as token sequences, 360Anything learns the perspective-to-equirectangular mapping in purely data-driven way, eliminating the need for camera information. Our approach achieves state-of-the-art performance on both image and video perspective-to-360 generation, outperforming prior works that use ground-truth camera information. We also trace the root cause of the seam artifacts at ERP boundaries to zero-padding in the VAE encoder, and introduce Circular Latent Encoding to facilitate seamless generation. Finally, we show competitive results in zero-shot camera FoV and orientation estimation benchmarks, demonstrating 360Anythings deep geometric understanding and broader utility in computer vision tasks. Additional results are available at https://360anything.github.io. Keywords: Panorama generation Diffusion transformer Outpainting 6 2 0 2 2 2 ] . [ 1 2 9 1 6 1 . 1 0 6 2 : 1 r Generating photorealistic 3D worlds is an exciting and challenging frontier in generative modeling, offering transformative potential across robotics, AR/VR, and gaming. Recent years have witnessed significant advancements in this domain [36,66,75,81], largely propelled by the dramatic progress in visual generative models [5, 34"
[25.01.2026 22:17] Failed to download and parse paper https://huggingface.co/papers/2601.16192: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Downloading and parsing paper https://huggingface.co/papers/2601.15703.
[25.01.2026 22:17] Downloading paper 2601.15703 from https://arxiv.org/pdf/2601.15703v1...
[25.01.2026 22:17] Extracting affiliations from text.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Prafulla Kumar Choubey Kung-Hsiang Huang Caiming Xiong Chien-Sheng Wu Salesforce AI Research {jiaxin.zhang, pchoubey, kh.huang, cxiong, wu.jason}@salesforce.com 6 2 0 2 2 2 ] . [ 1 3 0 7 5 1 . 1 0 6 2 : r a "
[25.01.2026 22:17] Failed to download and parse paper https://huggingface.co/papers/2601.15703: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Downloading and parsing paper https://huggingface.co/papers/2601.15778.
[25.01.2026 22:17] Downloading paper 2601.15778 from https://arxiv.org/pdf/2601.15778v1...
[25.01.2026 22:17] Extracting affiliations from text.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 2 2 ] . [ 1 8 7 7 5 1 . 1 0 6 2 : r Preprint Salesforce AI Research Jiaxin Zhang Caiming Xiong Chien-Sheng Wu Salesforce AI Research {jiaxin.zhang, cxiong, wu.jason}@salesforce.com "
[25.01.2026 22:17] Failed to download and parse paper https://huggingface.co/papers/2601.15778: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Downloading and parsing paper https://huggingface.co/papers/2601.15690.
[25.01.2026 22:17] Downloading paper 2601.15690 from https://arxiv.org/pdf/2601.15690v1...
[25.01.2026 22:17] Extracting affiliations from text.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 2 2 ] . [ 1 0 9 6 5 1 . 1 0 6 2 : r From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models Jiaxin Zhang1, Wendi Cui2, Zhuohang Li3, Lifu Huang4, Bradley Malin3,5, Caiming Xiong1, Chien-Sheng Wu1 2Intuit 1Salesforce AI Research 3Vanderbilt University 5Vanderbilt University Medical Center 4 University of California, Davis "
[25.01.2026 22:17] Failed to download and parse paper https://huggingface.co/papers/2601.15690: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Downloading and parsing paper https://huggingface.co/papers/2601.15549.
[25.01.2026 22:17] Downloading paper 2601.15549 from https://arxiv.org/pdf/2601.15549v1...
[25.01.2026 22:17] Extracting affiliations from text.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 2 2 ] . [ 1 9 4 5 5 1 . 1 0 6 2 : r VIOLA: Towards Video In-Context Learning with Minimal Annotations Ryo Fujii1,2, Hideo Saito1,2, and Ryo Hachiuma3 1 Keio University 2 Keio AI Research Center 3 NVIDIA Abstract. Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs. Keywords: In-context learning Multimodal large language models Active learning Pseudo annotation Recent advances in Multimodal Large Language Models (MLLMs) [38,58,74] have notably enhan"
[25.01.2026 22:17] Failed to download and parse paper https://huggingface.co/papers/2601.15549: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Downloading and parsing paper https://huggingface.co/papers/2601.16134.
[25.01.2026 22:17] Downloading paper 2601.16134 from https://arxiv.org/pdf/2601.16134v1...
[25.01.2026 22:17] Extracting affiliations from text.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"LLM Prompt Evaluation for Educational Applications Langdon Holmes1,*, Adam Coscia2, Scott Crossley1, Joon Suh Choi1, and Wesley Morris1 1 Vanderbilt University, Nashville, Tennessee 2 Georgia Institute of Technology, Atlanta, Georgia Abstract As large language models (LLMs) become increasingly common in educational applications, there is growing need for evidence-based methods to design and evaluate LLM prompts that produce personalized and pedagogically aligned out-puts. This study presents generalizable, systematic approach for evaluating prompts, demonstrated through an analysis of LLM-generated follow-up questions in structured dialogue activity. Six prompt templates were designed and tested. The templates incorporated established prompt engineering patterns, with each prompt emphasizing distinct pedagogical strategies. The prompt templates were compared through tournament-style evaluation framework that can be adapted for other educational applications. The tournament employed the Glicko2 rating system with eight judges evaluating question pairs across three dimensions: format, dialogue support, and appropriateness for learners. Data was sourced from 120 authentic user interactions across three distinct educational deployments. Results showed that single prompt related to strategic reading out-performed other templates with win probabilities ranging from 81% to 100% in pairwise comparisons. This prompt combined persona and context manager pat-terns and was designed to support metacognitive learning strategies such as self-directed learning. The methodology showcases how educational technology researchers can systematically evaluate and improve prompt designs, moving be-yond ad-hoc prompt engineering toward evidence-based prompt development for educational applications. Keywords Prompt Engineering, Large Language Models, Intelligent Tutoring Systems, Reading Comprehension 1. Introduction Large language models (LLMs) have become increasingly integral to education"
[25.01.2026 22:17] Failed to download and parse paper https://huggingface.co/papers/2601.16134: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Downloading and parsing paper https://huggingface.co/papers/2601.16004.
[25.01.2026 22:17] Downloading paper 2601.16004 from https://arxiv.org/pdf/2601.16004v1...
[25.01.2026 22:17] Extracting affiliations from text.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Wigners Friend as Circuit: Inter-Branch Communication Witness Benchmarks on Superconducting Quantum Hardware Christopher Altman * January 2026 Abstract We implement and benchmark on IBM Quantum hardware the circuit family proposed by Violaris (2026) for estimating operational inter-branch communication witnesses, i.e., correlations in classical measurement records produced by compiled Wigners-friend-style circuits. Concretely, we realize five-qubit instance of the protocol as an inter-register message-transfer pattern in the circuit (not physical signaling), and evaluate its behavior under realistic device noise and compilation constraints. The circuit encodes branch-conditioned evolution in which an observer subsystem (Wigners friend) evolves differently depending on the state of control qubit Q, followed by controlled transfer operation that probes correlations between branch-conditioned histories (conditional measurement contexts within single compiled circuit). We evaluate coherence-sensitive witnesses on specified four-qubit subset. Executing on the ibm fez backend (N = 20,000 shots), we obtain population-based visibility = 0.8771 0.0034, coherence witnesses WX = 0.8398 0.0038 and WY = 0.8107 0.0041, + 2 and phase-sensitive magnitude Cmag = (W 2 )1/2 = 1.1673 0.0040. This work does not claim to test or discriminate among interpretations of quantum mechanics. Unitary interpretations that reproduce standard quantum predictions remain operationally equivalent for these circuits. Instead, we frame the experiment as publicly reproducible constraint pipeline: given parameterized family of non-ideal channels (e.g., dephasing or collapselike perturbations) inserted at specified circuit locations, the coherence-sensitive witnesses define detectability thresholds relative to calibrated device noise. The accompanying reproducibility bundle provides full provenance (job IDs, calibration snapshots, software versions) enabling independent verification and extension. The Wign"
[25.01.2026 22:17] Failed to download and parse paper https://huggingface.co/papers/2601.16004: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Downloading and parsing paper https://huggingface.co/papers/2601.15440.
[25.01.2026 22:17] Downloading paper 2601.15440 from https://arxiv.org/pdf/2601.15440v1...
[25.01.2026 22:17] Extracting affiliations from text.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Numba-Accelerated 2D Diffusion-Limited Aggregation: Implementation and Fractal Characterization Sandy H. S. Herho1,2,, Faiz R. Fajary3, Iwan P. Anwar4, Faruq Khadami4, Nurjanna J. Trilaksono3, Rusmawan Suwarman3, and Dasapta E. Irawan5 6 2 0 2 1 2 ] . n [ 1 0 4 4 5 1 . 1 0 6 2 : r 1Ronin Institute for Independent Scholarship, Sacramento, CA 95816, USA 2School of Systems Science and Industrial Engineering, State University of New York, Binghamton, NY 13902, USA 3Atmospheric Science Research Group, Bandung Institute of Technology, Bandung 40132, Indonesia 4Applied and Environmental Oceanography Research Group, Bandung Institute of Technology, Bandung 40132, Indonesia 5Applied Geology Research Group, Bandung Institute of Technology, Bandung 40132, Indonesia e-mail: sandy.herho@ronininstitute.org Abstract We present dla-ideal-solver, high-performance framework for simulating two-dimensional DiffusionLimited Aggregation (DLA) using Numba-accelerated Python. By leveraging just-in-time (JIT) compilation, we achieve computational throughput comparable to legacy static implementations while retaining high-level flexibility. We investigate the Laplacian growth instability across varying injection geometries and walker concentrations. Our analysis confirms the robustness of the standard fractal dimension Df 1.71 for dilute regimes, consistent with the Witten-Sander universality class. However, we report distinct crossover to Eden-like compact growth (Df 1.87) in high-density environments, attributed to the saturation of the screening length. Beyond standard mass-radius scaling, we employ generalized Renyi dimensions and lacunarity metrics to quantify the monofractal character and spatial heterogeneity of the aggregates. This work establishes reproducible, open-source testbed for exploring phase transitions in non-equilibrium statistical mechanics. Keywords: Diffusion-Limited Aggregation, fractal dimension, Laplacian growth, Monte Carlo simulation, pattern formation Investigati"
[25.01.2026 22:17] Failed to download and parse paper https://huggingface.co/papers/2601.15440: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Downloading and parsing paper https://huggingface.co/papers/2601.08118.
[25.01.2026 22:17] Downloading paper 2601.08118 from https://arxiv.org/pdf/2601.08118v1...
[25.01.2026 22:17] Extracting affiliations from text.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"MIRRORBENCH: AN EXTENSIBLE FRAMEWORK TO EVALUATE USER-PROXY AGENTS FOR HUMAN-LIKENESS 6 2 0 2 3 1 ] . [ 1 8 1 1 8 0 . 1 0 6 2 : r Ashutosh Hathidara 1 Julien Yu 1 Vaishali Senthil 1 Sebastian Schreiber 1 Anil Babu Ankisettipalli 1 ABSTRACT Large language models (LLMs) are increasingly used as human simulators, both for evaluating conversational systems and for generating fine-tuning data. However, naive act-as-a-user" prompting often yields verbose, unrealistic utterances, underscoring the need for principled evaluation of so-called user proxy agents. We present MIRRORBENCH, reproducible, extensible benchmarking framework that evaluates user proxies solely on their ability to produce human-like user utterances across diverse conversational tasks, explicitly decoupled from downstream task success. MIRRORBENCH features modular execution engine with typed interfaces, metadatadriven registries, multi-backend support, caching, and robust observability. The system supports pluggable user proxies, datasets, tasks, and metrics, enabling researchers to evaluate arbitrary simulators under uniform, variance-aware harness. We include three lexical-diversity metrics (MATTR, YULES K, and HD-D) and three LLM-judgebased metrics (GTEVAL, PAIRWISE INDISTINGUISHABILITY, and RUBRIC-AND-REASON). Across four open datasets, MIRRORBENCH yields variance-aware results and reveals systematic gaps between user proxies and real human users. The framework is open source and includes simple command-line interface for running experiments, managing configurations and caching, and generating reports. The framework can be accessed at https://github.com/SAP/mirrorbench 1 INTRODUCTION & RELATED WORK The idea of simulating users to train and evaluate dialogue agents has long history in NLP (Balog & Zhai, 2023). Early user simulators were typically goal-driven or rulebased, following predefined agendas of dialogue acts to mimic human behavior (Schatzmann & Young, 2009). Recent work has moved towards larg"
[25.01.2026 22:17] Failed to download and parse paper https://huggingface.co/papers/2601.08118: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Enriching papers with extra data.
[25.01.2026 22:17] ********************************************************************************
[25.01.2026 22:17] Abstract 0. EvoCUA introduces an evolutionary approach to computer-use agents that combines autonomous task generation with policy optimization to achieve superior performance in complex, long-horizon tasks.  					AI-generated summary 				 The development of native computer-use agents (CUA) represents a signifi...
[25.01.2026 22:17] ********************************************************************************
[25.01.2026 22:17] Abstract 1. HERMES is a training-free architecture that enables real-time video stream understanding by utilizing a hierarchical memory framework based on KV cache reuse, achieving faster response times and maintained accuracy even with reduced video token input.  					AI-generated summary 				 Recent advanceme...
[25.01.2026 22:17] ********************************************************************************
[25.01.2026 22:17] Abstract 2. LLM-in-Sandbox enables large language models to perform general intelligence tasks across diverse domains by allowing them to explore a code sandbox environment, achieving robust generalization without additional training.  					AI-generated summary 				 We introduce LLM-in-Sandbox, enabling LLMs to...
[25.01.2026 22:17] ********************************************************************************
[25.01.2026 22:17] Abstract 3. Arbitrary order generation in diffusion large language models limits reasoning capability by causing premature solution space collapse, making standard policy optimization more effective.  					AI-generated summary 				 Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint...
[25.01.2026 22:17] ********************************************************************************
[25.01.2026 22:17] Abstract 4. BayesianVLA addresses language-action grounding issues in robot manipulation by using Bayesian decomposition to prevent information collapse and improve out-of-distribution generalization.  					AI-generated summary 				 Vision-Language-Action (VLA) models have shown promise in robot manipulation bu...
[25.01.2026 22:17] ********************************************************************************
[25.01.2026 22:17] Abstract 5. Representation Autoencoders (RAEs) demonstrate superior performance over VAEs in large-scale text-to-image generation, showing improved stability, faster convergence, and better quality while enabling unified multimodal reasoning in shared representation spaces.  					AI-generated summary 				 Repre...
[25.01.2026 22:17] ********************************************************************************
[25.01.2026 22:17] Abstract 6. Stable-DiffCoder demonstrates superior code modeling performance compared to autoregressive baselines through block diffusion continual pretraining and efficient training mechanisms.  					AI-generated summary 				 Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation a...
[25.01.2026 22:17] ********************************************************************************
[25.01.2026 22:17] Abstract 7. SAMTok enables pixel-wise capabilities in multi-modal LLMs through discrete mask tokenization and standard training methods, achieving state-of-the-art performance on various vision-language tasks.  					AI-generated summary 				 Pixel-wise capabilities are essential for building interactive intelli...
[25.01.2026 22:17] ********************************************************************************
[25.01.2026 22:17] Abstract 8. Test-time training enables AI systems to discover optimal solutions for specific scientific problems through continual learning focused on individual challenges rather than generalization.  					AI-generated summary 				 How can we use AI to discover a new state of the art for a scientific problem? ...
[25.01.2026 22:17] ********************************************************************************
[25.01.2026 22:17] Abstract 9. The Qwen3-TTS series presents advanced multilingual text-to-speech models with voice cloning and controllable speech generation capabilities, utilizing dual-track LM architecture and specialized speech tokenizers for efficient streaming synthesis.  					AI-generated summary 				 In this report, we p...
[25.01.2026 22:17] ********************************************************************************
[25.01.2026 22:17] Abstract 10. Terminal-Bench 2.0 presents a challenging benchmark with 89 terminal-based tasks to evaluate AI agents' capabilities in real-world scenarios.  					AI-generated summary 				 AI agents may soon become capable of autonomously completing valuable, long-horizon tasks in diverse domains. Current benchmar...
[25.01.2026 22:17] ********************************************************************************
[25.01.2026 22:17] Abstract 11. An advanced vision encoder named OpenVision 3 learns a unified visual representation for both image understanding and generation by combining VAE-compressed image latents with ViT architecture and joint optimization of reconstruction and semantic signals.  					AI-generated summary 				 This paper p...
[25.01.2026 22:17] ********************************************************************************
[25.01.2026 22:17] Abstract 12. Large language models and agent-based systems are being leveraged to automate kernel generation and optimization, addressing the scalability challenges in hardware-specific code development through structured approaches and systematic benchmarking.  					AI-generated summary 				 The performance of ...
[25.01.2026 22:17] ********************************************************************************
[25.01.2026 22:17] Abstract 13. A novel fine-grained composed image retrieval benchmark is introduced through image editing techniques, revealing significant capability gaps in existing multimodal models and exposing limitations of current benchmarks.  					AI-generated summary 				 Composed Image Retrieval (CIR) is a pivotal and ...
[25.01.2026 22:17] ********************************************************************************
[25.01.2026 22:17] Abstract 14. ActionMesh generates animated 3D meshes by extending 3D diffusion models with a temporal axis, producing high-quality, rig-free animations from various input types while maintaining topological consistency.  					AI-generated summary 				 Generating animated 3D objects is at the heart of many applic...
[25.01.2026 22:17] ********************************************************************************
[25.01.2026 22:17] Abstract 15. VideoMaMa converts coarse masks to accurate alpha mattes using pretrained video diffusion models, enabling zero-shot generalization and scalable pseudo-labeling for video matting.  					AI-generated summary 				 Generalizing video matting models to real-world videos remains a significant challenge d...
[25.01.2026 22:17] ********************************************************************************
[25.01.2026 22:17] Abstract 16. A pretrained video model is adapted into a robot policy through single-stage post-training, enabling direct action generation and planning capabilities without architectural modifications.  					AI-generated summary 				 Recent video generation models demonstrate remarkable ability to capture comple...
[25.01.2026 22:17] ********************************************************************************
[25.01.2026 22:17] Abstract 17. Vision-Language Models struggle with task progress estimation from partial observations, requiring new approaches like ProgressLM-45K and ProgressLM-3B to improve reasoning capabilities.  					AI-generated summary 				 Estimating task progress requires reasoning over long-horizon dynamics rather tha...
[25.01.2026 22:17] ********************************************************************************
[25.01.2026 22:17] Abstract 18. A geometry-free framework using pre-trained diffusion transformers lifts perspective images and videos to 360° panoramas without requiring camera metadata, achieving state-of-the-art performance through token sequence processing and addressing seam artifacts via circular latent encoding.  					AI-ge...
[25.01.2026 22:17] ********************************************************************************
[25.01.2026 22:17] Abstract 19. A unified dual-process framework transforms verbalized uncertainty into active control signals for improved reasoning reliability in AI agents.  					AI-generated summary 				 Although AI agents have demonstrated impressive capabilities in long-horizon reasoning, their reliability is severely hamper...
[25.01.2026 22:17] ********************************************************************************
[25.01.2026 22:17] Abstract 20. Agentic confidence calibration addresses limitations of static calibration methods by introducing a trajectory-based diagnostic framework that improves reliability across diverse AI agent systems.  					AI-generated summary 				 AI agents are rapidly advancing from passive language models to autonom...
[25.01.2026 22:17] ********************************************************************************
[25.01.2026 22:17] Abstract 21. Large language models face reliability challenges that are being addressed through uncertainty as an active control signal across advanced reasoning, autonomous agents, and reinforcement learning, supported by theoretical frameworks like Bayesian methods and conformal prediction.  					AI-generated ...
[25.01.2026 22:17] ********************************************************************************
[25.01.2026 22:17] Abstract 22. VIOLA is a label-efficient framework that combines minimal expert supervision with abundant unlabeled data to enable effective multimodal large language model adaptation in low-resource video domains through density-uncertainty-weighted sampling and confidence-aware retrieval mechanisms.  					AI-ge...
[25.01.2026 22:17] ********************************************************************************
[25.01.2026 22:17] Abstract 23. A systematic evaluation framework using tournament-style testing and Glicko2 rating system demonstrates effective prompt engineering for educational LLM applications, prioritizing metacognitive learning strategies.  					AI-generated summary 				 As large language models (LLMs) become increasingly c...
[25.01.2026 22:17] ********************************************************************************
[25.01.2026 22:17] Abstract 24. Implementation and benchmarking of quantum circuits for estimating operational inter-branch communication witnesses on IBM Quantum hardware demonstrates visibility and coherence witness measurements under realistic device conditions.  					AI-generated summary 				 We implement and benchmark on IBM ...
[25.01.2026 22:17] ********************************************************************************
[25.01.2026 22:17] Abstract 25. We present dla-ideal-solver, a high-performance framework for simulating two-dimensional Diffusion-Limited Aggregation (DLA) using Numba-accelerated Python. By leveraging just-in-time (JIT) compilation, we achieve computational throughput comparable to legacy static implementations while retaining h...
[25.01.2026 22:17] ********************************************************************************
[25.01.2026 22:17] Abstract 26. Large language models are evaluated as user simulators through a reproducible benchmark framework that assesses their ability to generate human-like conversational responses across diverse tasks.  					AI-generated summary 				 Large language models (LLMs) are increasingly used as human simulators, ...
[25.01.2026 22:17] Read previous papers.
[25.01.2026 22:17] Generating reviews via LLM API.
[25.01.2026 22:17] Querying the API.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

EvoCUA introduces an evolutionary approach to computer-use agents that combines autonomous task generation with policy optimization to achieve superior performance in complex, long-horizon tasks.  					AI-generated summary 				 The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.
[25.01.2026 22:17] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Querying the API.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

HERMES is a training-free architecture that enables real-time video stream understanding by utilizing a hierarchical memory framework based on KV cache reuse, achieving faster response times and maintained accuracy even with reduced video token input.  					AI-generated summary 				 Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant improvement in offline video understanding. However, extending these capabilities to streaming video inputs, remains challenging, as existing models struggle to simultaneously maintain stable understanding performance, real-time responses, and low GPU memory overhead. To address this challenge, we propose HERMES, a novel training-free architecture for real-time and accurate understanding of video streams. Based on a mechanistic attention investigation, we conceptualize KV cache as a hierarchical memory framework that encapsulates video information across multiple granularities. During inference, HERMES reuses a compact KV cache, enabling efficient streaming understanding under resource constraints. Notably, HERMES requires no auxiliary computations upon the arrival of user queries, thereby guaranteeing real-time responses for continuous video stream interactions, which achieves 10times faster TTFT compared to prior SOTA. Even when reducing video tokens by up to 68% compared with uniform sampling, HERMES achieves superior or comparable accuracy across all benchmarks, with up to 11.4% gains on streaming datasets.
[25.01.2026 22:17] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Querying the API.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

LLM-in-Sandbox enables large language models to perform general intelligence tasks across diverse domains by allowing them to explore a code sandbox environment, achieving robust generalization without additional training.  					AI-generated summary 				 We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.
[25.01.2026 22:17] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Querying the API.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Arbitrary order generation in diffusion large language models limits reasoning capability by causing premature solution space collapse, making standard policy optimization more effective.  					AI-generated summary 				 Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap
[25.01.2026 22:17] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Querying the API.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

BayesianVLA addresses language-action grounding issues in robot manipulation by using Bayesian decomposition to prevent information collapse and improve out-of-distribution generalization.  					AI-generated summary 				 Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior p(a mid v) and a language-conditioned posterior π(a mid v, ell). We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.
[25.01.2026 22:17] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Querying the API.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Representation Autoencoders (RAEs) demonstrate superior performance over VAEs in large-scale text-to-image generation, showing improved stability, faster convergence, and better quality while enabling unified multimodal reasoning in shared representation spaces.  					AI-generated summary 				 Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.
[25.01.2026 22:17] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Querying the API.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Stable-DiffCoder demonstrates superior code modeling performance compared to autoregressive baselines through block diffusion continual pretraining and efficient training mechanisms.  					AI-generated summary 				 Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.
[25.01.2026 22:17] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Querying the API.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

SAMTok enables pixel-wise capabilities in multi-modal LLMs through discrete mask tokenization and standard training methods, achieving state-of-the-art performance on various vision-language tasks.  					AI-generated summary 				 Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.
[25.01.2026 22:17] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Querying the API.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Test-time training enables AI systems to discover optimal solutions for specific scientific problems through continual learning focused on individual challenges rather than generalization.  					AI-generated summary 				 How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to 2times faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.
[25.01.2026 22:17] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Querying the API.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

The Qwen3-TTS series presents advanced multilingual text-to-speech models with voice cloning and controllable speech generation capabilities, utilizing dual-track LM architecture and specialized speech tokenizers for efficient streaming synthesis.  					AI-generated summary 				 In this report, we present the Qwen3-TTS series, a family of advanced multilingual, controllable, robust, and streaming text-to-speech models. Qwen3-TTS supports state-of-the-art 3-second voice cloning and description-based control, allowing both the creation of entirely novel voices and fine-grained manipulation over the output speech. Trained on over 5 million hours of speech data spanning 10 languages, Qwen3-TTS adopts a dual-track LM architecture for real-time synthesis, coupled with two speech tokenizers: 1) Qwen-TTS-Tokenizer-25Hz is a single-codebook codec emphasizing semantic content, which offers seamlessly integration with Qwen-Audio and enables streaming waveform reconstruction via a block-wise DiT. 2) Qwen-TTS-Tokenizer-12Hz achieves extreme bitrate reduction and ultra-low-latency streaming, enabling immediate first-packet emission (97,ms) through its 12.5 Hz, 16-layer multi-codebook design and a lightweight causal ConvNet. Extensive experiments indicate state-of-the-art performance across diverse objective and subjective benchmark (e.g., TTS multilingual test set, InstructTTSEval, and our long speech test set). To facilitate community research and development, we release both tokenizers and models under the Apache 2.0 license.
[25.01.2026 22:17] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Querying the API.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Terminal-Bench 2.0 presents a challenging benchmark with 89 terminal-based tasks to evaluate AI agents' capabilities in real-world scenarios.  					AI-generated summary 				 AI agents may soon become capable of autonomously completing valuable, long-horizon tasks in diverse domains. Current benchmarks either do not measure real-world tasks, or are not sufficiently difficult to meaningfully measure frontier models. To this end, we present Terminal-Bench 2.0: a carefully curated hard benchmark composed of 89 tasks in computer terminal environments inspired by problems from real workflows. Each task features a unique environment, human-written solution, and comprehensive tests for verification. We show that frontier models and agents score less than 65\% on the benchmark and conduct an error analysis to identify areas for model and agent improvement. We publish the dataset and evaluation harness to assist developers and researchers in future work at https://www.tbench.ai/ .
[25.01.2026 22:17] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Querying the API.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

An advanced vision encoder named OpenVision 3 learns a unified visual representation for both image understanding and generation by combining VAE-compressed image latents with ViT architecture and joint optimization of reconstruction and semantic signals.  					AI-generated summary 				 This paper presents a family of advanced vision encoder, named OpenVision 3, that learns a single, unified visual representation that can serve both image understanding and image generation. Our core architecture is simple: we feed VAE-compressed image latents to a ViT encoder and train its output to support two complementary roles. First, the encoder output is passed to the ViT-VAE decoder to reconstruct the original image, encouraging the representation to capture generative structure. Second, the same representation is optimized with contrastive learning and image-captioning objectives, strengthening semantic features. By jointly optimizing reconstruction- and semantics-driven signals in a shared latent space, the encoder learns representations that synergize and generalize well across both regimes. We validate this unified design through extensive downstream evaluations with the encoder frozen. For multimodal understanding, we plug the encoder into the LLaVA-1.5 framework: it performs comparably with a standard CLIP vision encoder (e.g., 62.4 vs 62.2 on SeedBench, and 83.7 vs 82.9 on POPE). For generation, we test it under the RAE framework: ours substantially surpasses the standard CLIP-based encoder (e.g., gFID: 1.89 vs 2.54 on ImageNet). We hope this work can spur future research on unified modeling.
[25.01.2026 22:17] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Querying the API.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Large language models and agent-based systems are being leveraged to automate kernel generation and optimization, addressing the scalability challenges in hardware-specific code development through structured approaches and systematic benchmarking.  					AI-generated summary 				 The performance of modern AI systems is fundamentally constrained by the quality of their underlying kernels, which translate high-level algorithmic semantics into low-level hardware operations. Achieving near-optimal kernels requires expert-level understanding of hardware architectures and programming models, making kernel engineering a critical but notoriously time-consuming and non-scalable process. Recent advances in large language models (LLMs) and LLM-based agents have opened new possibilities for automating kernel generation and optimization. LLMs are well-suited to compress expert-level kernel knowledge that is difficult to formalize, while agentic systems further enable scalable optimization by casting kernel development as an iterative, feedback-driven loop. Rapid progress has been made in this area. However, the field remains fragmented, lacking a systematic perspective for LLM-driven kernel generation. This survey addresses this gap by providing a structured overview of existing approaches, spanning LLM-based approaches and agentic optimization workflows, and systematically compiling the datasets and benchmarks that underpin learning and evaluation in this domain. Moreover, key open challenges and future research directions are further outlined, aiming to establish a comprehensive reference for the next generation of automated kernel optimization. To keep track of this field, we maintain an open-source GitHub repository at https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation.
[25.01.2026 22:17] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Querying the API.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A novel fine-grained composed image retrieval benchmark is introduced through image editing techniques, revealing significant capability gaps in existing multimodal models and exposing limitations of current benchmarks.  					AI-generated summary 				 Composed Image Retrieval (CIR) is a pivotal and complex task in multimodal understanding. Current CIR benchmarks typically feature limited query categories and fail to capture the diverse requirements of real-world scenarios. To bridge this evaluation gap, we leverage image editing to achieve precise control over modification types and content, enabling a pipeline for synthesizing queries across a broad spectrum of categories. Using this pipeline, we construct EDIR, a novel fine-grained CIR benchmark. EDIR encompasses 5,000 high-quality queries structured across five main categories and fifteen subcategories. Our comprehensive evaluation of 13 multimodal embedding models reveals a significant capability gap; even state-of-the-art models (e.g., RzenEmbed and GME) struggle to perform consistently across all subcategories, highlighting the rigorous nature of our benchmark. Through comparative analysis, we further uncover inherent limitations in existing benchmarks, such as modality biases and insufficient categorical coverage. Furthermore, an in-domain training experiment demonstrates the feasibility of our benchmark. This experiment clarifies the task challenges by distinguishing between categories that are solvable with targeted data and those that expose intrinsic limitations of current model architectures.
[25.01.2026 22:17] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Querying the API.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

ActionMesh generates animated 3D meshes by extending 3D diffusion models with a temporal axis, producing high-quality, rig-free animations from various input types while maintaining topological consistency.  					AI-generated summary 				 Generating animated 3D objects is at the heart of many applications, yet most advanced works are typically difficult to apply in practice because of their limited setup, their long runtime, or their limited quality. We introduce ActionMesh, a generative model that predicts production-ready 3D meshes "in action" in a feed-forward manner. Drawing inspiration from early video models, our key insight is to modify existing 3D diffusion models to include a temporal axis, resulting in a framework we dubbed "temporal 3D diffusion". Specifically, we first adapt the 3D diffusion stage to generate a sequence of synchronized latents representing time-varying and independent 3D shapes. Second, we design a temporal 3D autoencoder that translates a sequence of independent shapes into the corresponding deformations of a pre-defined reference shape, allowing us to build an animation. Combining these two components, ActionMesh generates animated 3D meshes from different inputs like a monocular video, a text description, or even a 3D mesh with a text prompt describing its animation. Besides, compared to previous approaches, our method is fast and produces results that are rig-free and topology consistent, hence enabling rapid iteration and seamless applications like texturing and retargeting. We evaluate our model on standard video-to-4D benchmarks (Consistent4D, Objaverse) and report state-of-the-art performances on both geometric accuracy and temporal consistency, demonstrating that our model can deliver animated 3D meshes with unprecedented speed and quality.
[25.01.2026 22:17] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Querying the API.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

VideoMaMa converts coarse masks to accurate alpha mattes using pretrained video diffusion models, enabling zero-shot generalization and scalable pseudo-labeling for video matting.  					AI-generated summary 				 Generalizing video matting models to real-world videos remains a significant challenge due to the scarcity of labeled data. To address this, we present Video Mask-to-Matte Model (VideoMaMa) that converts coarse segmentation masks into pixel accurate alpha mattes, by leveraging pretrained video diffusion models. VideoMaMa demonstrates strong zero-shot generalization to real-world footage, even though it is trained solely on synthetic data. Building on this capability, we develop a scalable pseudo-labeling pipeline for large-scale video matting and construct the Matting Anything in Video (MA-V) dataset, which offers high-quality matting annotations for more than 50K real-world videos spanning diverse scenes and motions. To validate the effectiveness of this dataset, we fine-tune the SAM2 model on MA-V to obtain SAM2-Matte, which outperforms the same model trained on existing matting datasets in terms of robustness on in-the-wild videos. These findings emphasize the importance of large-scale pseudo-labeled video matting and showcase how generative priors and accessible segmentation cues can drive scalable progress in video matting research.
[25.01.2026 22:17] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Querying the API.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A pretrained video model is adapted into a robot policy through single-stage post-training, enabling direct action generation and planning capabilities without architectural modifications.  					AI-generated summary 				 Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model's latent diffusion process, harnessing the model's pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/
[25.01.2026 22:17] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Querying the API.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Vision-Language Models struggle with task progress estimation from partial observations, requiring new approaches like ProgressLM-45K and ProgressLM-3B to improve reasoning capabilities.  					AI-generated summary 				 Estimating task progress requires reasoning over long-horizon dynamics rather than recognizing static visual content. While modern Vision-Language Models (VLMs) excel at describing what is visible, it remains unclear whether they can infer how far a task has progressed from partial observations. To this end, we introduce Progress-Bench, a benchmark for systematically evaluating progress reasoning in VLMs. Beyond benchmarking, we further explore a human-inspired two-stage progress reasoning paradigm through both training-free prompting and training-based approach based on curated dataset ProgressLM-45K. Experiments on 14 VLMs show that most models are not yet ready for task progress estimation, exhibiting sensitivity to demonstration modality and viewpoint changes, as well as poor handling of unanswerable cases. While training-free prompting that enforces structured progress reasoning yields limited and model-dependent gains, the training-based ProgressLM-3B achieves consistent improvements even at a small model scale, despite being trained on a task set fully disjoint from the evaluation tasks. Further analyses reveal characteristic error patterns and clarify when and why progress reasoning succeeds or fails.
[25.01.2026 22:17] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Querying the API.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A geometry-free framework using pre-trained diffusion transformers lifts perspective images and videos to 360° panoramas without requiring camera metadata, achieving state-of-the-art performance through token sequence processing and addressing seam artifacts via circular latent encoding.  					AI-generated summary 				 Lifting perspective images and videos to 360° panoramas enables immersive 3D world generation. Existing approaches often rely on explicit geometric alignment between the perspective and the equirectangular projection (ERP) space. Yet, this requires known camera metadata, obscuring the application to in-the-wild data where such calibration is typically absent or noisy. We propose 360Anything, a geometry-free framework built upon pre-trained diffusion transformers. By treating the perspective input and the panorama target simply as token sequences, 360Anything learns the perspective-to-equirectangular mapping in a purely data-driven way, eliminating the need for camera information. Our approach achieves state-of-the-art performance on both image and video perspective-to-360° generation, outperforming prior works that use ground-truth camera information. We also trace the root cause of the seam artifacts at ERP boundaries to zero-padding in the VAE encoder, and introduce Circular Latent Encoding to facilitate seamless generation. Finally, we show competitive results in zero-shot camera FoV and orientation estimation benchmarks, demonstrating 360Anything's deep geometric understanding and broader utility in computer vision tasks. Additional results are available at https://360anything.github.io/.
[25.01.2026 22:17] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Querying the API.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A unified dual-process framework transforms verbalized uncertainty into active control signals for improved reasoning reliability in AI agents.  					AI-generated summary 				 Although AI agents have demonstrated impressive capabilities in long-horizon reasoning, their reliability is severely hampered by the ``Spiral of Hallucination,'' where early epistemic errors propagate irreversibly. Existing methods face a dilemma: uncertainty quantification (UQ) methods typically act as passive sensors, only diagnosing risks without addressing them, while self-reflection mechanisms suffer from continuous or aimless corrections. To bridge this gap, we propose a unified Dual-Process Agentic UQ (AUQ) framework that transforms verbalized uncertainty into active, bi-directional control signals. Our architecture comprises two complementary mechanisms: System 1 (Uncertainty-Aware Memory, UAM), which implicitly propagates verbalized confidence and semantic explanations to prevent blind decision-making; and System 2 (Uncertainty-Aware Reflection, UAR), which utilizes these explanations as rational cues to trigger targeted inference-time resolution only when necessary. This enables the agent to balance efficient execution and deep deliberation dynamically. Extensive experiments on closed-loop benchmarks and open-ended deep research tasks demonstrate that our training-free approach achieves superior performance and trajectory-level calibration. We believe this principled framework AUQ represents a significant step towards reliable agents.
[25.01.2026 22:17] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Querying the API.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Agentic confidence calibration addresses limitations of static calibration methods by introducing a trajectory-based diagnostic framework that improves reliability across diverse AI agent systems.  					AI-generated summary 				 AI agents are rapidly advancing from passive language models to autonomous systems executing complex, multi-step tasks. Yet their overconfidence in failure remains a fundamental barrier to deployment in high-stakes settings. Existing calibration methods, built for static single-turn outputs, cannot address the unique challenges of agentic systems, such as compounding errors along trajectories, uncertainty from external tools, and opaque failure modes. To address these challenges, we introduce, for the first time, the problem of Agentic Confidence Calibration and propose Holistic Trajectory Calibration (HTC), a novel diagnostic framework that extracts rich process-level features ranging from macro dynamics to micro stability across an agent's entire trajectory. Powered by a simple, interpretable model, HTC consistently surpasses strong baselines in both calibration and discrimination, across eight benchmarks, multiple LLMs, and diverse agent frameworks. Beyond performance, HTC delivers three essential advances: it provides interpretability by revealing the signals behind failure, enables transferability by applying across domains without retraining, and achieves generalization through a General Agent Calibrator (GAC) that achieves the best calibration (lowest ECE) on the out-of-domain GAIA benchmark. Together, these contributions establish a new process-centric paradigm for confidence calibration, providing a framework for diagnosing and enhancing the reliability of AI agents.
[25.01.2026 22:17] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Querying the API.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Large language models face reliability challenges that are being addressed through uncertainty as an active control signal across advanced reasoning, autonomous agents, and reinforcement learning, supported by theoretical frameworks like Bayesian methods and conformal prediction.  					AI-generated summary 				 While Large Language Models (LLMs) show remarkable capabilities, their unreliability remains a critical barrier to deployment in high-stakes domains. This survey charts a functional evolution in addressing this challenge: the evolution of uncertainty from a passive diagnostic metric to an active control signal guiding real-time model behavior. We demonstrate how uncertainty is leveraged as an active control signal across three frontiers: in advanced reasoning to optimize computation and trigger self-correction; in autonomous agents to govern metacognitive decisions about tool use and information seeking; and in reinforcement learning to mitigate reward hacking and enable self-improvement via intrinsic rewards. By grounding these advancements in emerging theoretical frameworks like Bayesian methods and Conformal Prediction, we provide a unified perspective on this transformative trend. This survey provides a comprehensive overview, critical analysis, and practical design patterns, arguing that mastering the new trend of uncertainty is essential for building the next generation of scalable, reliable, and trustworthy AI.
[25.01.2026 22:17] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Querying the API.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

VIOLA is a label-efficient framework that combines minimal expert supervision with abundant unlabeled data to enable effective multimodal large language model adaptation in low-resource video domains through density-uncertainty-weighted sampling and confidence-aware retrieval mechanisms.  					AI-generated summary 				 Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers a training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts' annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), a label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of a strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct a hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on a composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs.
[25.01.2026 22:17] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Querying the API.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A systematic evaluation framework using tournament-style testing and Glicko2 rating system demonstrates effective prompt engineering for educational LLM applications, prioritizing metacognitive learning strategies.  					AI-generated summary 				 As large language models (LLMs) become increasingly common in educational applications, there is a growing need for evidence-based methods to design and evaluate LLM prompts that produce personalized and pedagogically aligned out-puts. This study presents a generalizable, systematic approach for evaluating prompts, demonstrated through an analysis of LLM-generated follow-up questions in a structured dialogue activity. Six prompt templates were designed and tested. The templates incorporated established prompt engineering patterns, with each prompt emphasizing distinct pedagogical strategies. The prompt templates were compared through a tournament-style evaluation framework that can be adapted for other educational applications. The tournament employed the Glicko2 rating system with eight judges evaluating question pairs across three dimensions: format, dialogue support, and appropriateness for learners. Data was sourced from 120 authentic user interactions across three distinct educational deployments. Results showed that a single prompt related to strategic reading out-performed other templates with win probabilities ranging from 81% to 100% in pairwise comparisons. This prompt combined persona and context manager pat-terns and was designed to support metacognitive learning strategies such as self-directed learning. The methodology showcases how educational technology re- searchers can systematically evaluate and improve prompt designs, moving beyond ad-hoc prompt engineering toward evidence-based prompt development for educational applications.
[25.01.2026 22:17] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Querying the API.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Implementation and benchmarking of quantum circuits for estimating operational inter-branch communication witnesses on IBM Quantum hardware demonstrates visibility and coherence witness measurements under realistic device conditions.  					AI-generated summary 				 We implement and benchmark on IBM Quantum hardware the circuit family proposed by Violaris for estimating operational inter-branch communication witnesses, defined as correlations in classical measurement records produced by compiled Wigner's-friend-style circuits. We realize a five-qubit instance of the protocol as an inter-register message-transfer pattern within a single circuit, rather than physical signaling, and evaluate its behavior under realistic device noise and compilation constraints. The circuit encodes branch-conditioned evolution of an observer subsystem whose dynamics depend on a control qubit, followed by a controlled transfer operation that probes correlations between conditional measurement contexts.   Executing on the ibm_fez backend with 20000 shots, we observe population-based visibility of 0.877, coherence witnesses of 0.840 and -0.811 along orthogonal axes, and a phase-sensitive magnitude of approximately 1.17. While the visibility metric is insensitive to some classes of dephasing, the coherence witnesses provide complementary sensitivity to off-diagonal noise.   This work does not test or discriminate among interpretations of quantum mechanics. Instead, it provides a reproducible operational constraint pipeline for evaluating detectability of non-ideal channels relative to calibrated device noise.
[25.01.2026 22:17] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Querying the API.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

We present dla-ideal-solver, a high-performance framework for simulating two-dimensional Diffusion-Limited Aggregation (DLA) using Numba-accelerated Python. By leveraging just-in-time (JIT) compilation, we achieve computational throughput comparable to legacy static implementations while retaining high-level flexibility. We investigate the Laplacian growth instability across varying injection geometries and walker concentrations. Our analysis confirms the robustness of the standard fractal dimension D_f approx 1.71 for dilute regimes, consistent with the Witten-Sander universality class. However, we report a distinct crossover to Eden-like compact growth (D_f approx 1.87) in high-density environments, attributed to the saturation of the screening length. Beyond standard mass-radius scaling, we employ generalized Rényi dimensions and lacunarity metrics to quantify the monofractal character and spatial heterogeneity of the aggregates. This work establishes a reproducible, open-source testbed for exploring phase transitions in non-equilibrium statistical mechanics.
[25.01.2026 22:17] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Querying the API.
[25.01.2026 22:17] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Large language models are evaluated as user simulators through a reproducible benchmark framework that assesses their ability to generate human-like conversational responses across diverse tasks.  					AI-generated summary 				 Large language models (LLMs) are increasingly used as human simulators, both for evaluating conversational systems and for generating fine-tuning data. However, naive "act-as-a-user" prompting often yields verbose, unrealistic utterances, underscoring the need for principled evaluation of so-called user proxy agents. We present MIRRORBENCH, a reproducible, extensible benchmarking framework that evaluates user proxies solely on their ability to produce human-like user utterances across diverse conversational tasks, explicitly decoupled from downstream task success. MIRRORBENCH features a modular execution engine with typed interfaces, metadata-driven registries, multi-backend support, caching, and robust observability. The system supports pluggable user proxies, datasets, tasks, and metrics, enabling researchers to evaluate arbitrary simulators under a uniform, variance-aware harness. We include three lexical-diversity metrics (MATTR, YULE'S K, and HD-D) and three LLM-judge-based metrics (GTEval, Pairwise Indistinguishability, and Rubric-and-Reason). Across four open datasets, MIRRORBENCH yields variance-aware results and reveals systematic gaps between user proxies and real human users. The framework is open source and includes a simple command-line interface for running experiments, managing configurations and caching, and generating reports. The framework can be accessed at https://github.com/SAP/mirrorbench.
[25.01.2026 22:17] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[25.01.2026 22:17] Renaming data file.
[25.01.2026 22:17] Renaming previous data. hf_papers.json to ./d/2026-01-23.json
[25.01.2026 22:17] Saving new data file.
[25.01.2026 22:17] Generating page.
[25.01.2026 22:17] Renaming previous page.
[25.01.2026 22:17] Renaming previous data. index.html to ./d/2026-01-23.html
[25.01.2026 22:17] Writing result.
[25.01.2026 22:17] Renaming log file.
[25.01.2026 22:17] Renaming previous data. log.txt to ./logs/2026-01-25_last_log.txt
