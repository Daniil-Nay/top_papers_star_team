[27.01.2026 03:46] Read previous papers.
[27.01.2026 03:46] Generating top page (month).
[27.01.2026 03:46] Writing top page (month).
[27.01.2026 04:42] Read previous papers.
[27.01.2026 04:42] Get feed.
[27.01.2026 04:42] Extract page data from URL. URL: https://huggingface.co/papers/2601.17737
[27.01.2026 04:42] Extract page data from URL. URL: https://huggingface.co/papers/2601.17058
[27.01.2026 04:42] Extract page data from URL. URL: https://huggingface.co/papers/2601.17367
[27.01.2026 04:42] Extract page data from URL. URL: https://huggingface.co/papers/2601.18577
[27.01.2026 04:42] Extract page data from URL. URL: https://huggingface.co/papers/2601.17027
[27.01.2026 04:42] Extract page data from URL. URL: https://huggingface.co/papers/2601.17124
[27.01.2026 04:42] Extract page data from URL. URL: https://huggingface.co/papers/2601.18184
[27.01.2026 04:42] Extract page data from URL. URL: https://huggingface.co/papers/2601.17761
[27.01.2026 04:42] Extract page data from URL. URL: https://huggingface.co/papers/2601.17111
[27.01.2026 04:42] Extract page data from URL. URL: https://huggingface.co/papers/2601.15849
[27.01.2026 04:42] Extract page data from URL. URL: https://huggingface.co/papers/2601.18130
[27.01.2026 04:42] Extract page data from URL. URL: https://huggingface.co/papers/2601.18081
[27.01.2026 04:42] Extract page data from URL. URL: https://huggingface.co/papers/2601.17323
[27.01.2026 04:42] Extract page data from URL. URL: https://huggingface.co/papers/2601.16207
[27.01.2026 04:42] Extract page data from URL. URL: https://huggingface.co/papers/2601.18759
[27.01.2026 04:42] Extract page data from URL. URL: https://huggingface.co/papers/2601.18202
[27.01.2026 04:42] Extract page data from URL. URL: https://huggingface.co/papers/2601.18157
[27.01.2026 04:42] Updating GitHub stars.
[27.01.2026 04:42] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[27.01.2026 04:42] No deleted papers detected.
[27.01.2026 04:42] Downloading and parsing papers (pdf, html). Total: 17.
[27.01.2026 04:42] Downloading and parsing paper https://huggingface.co/papers/2601.17737.
[27.01.2026 04:42] Downloading paper 2601.17737 from https://arxiv.org/pdf/2601.17737v1...
[27.01.2026 04:42] Extracting affiliations from text.
[27.01.2026 04:42] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 5 2 ] . [ 1 7 3 7 7 1 . 1 0 6 2 : r An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation Chenyu Mu ,1,2 , Xin He,1 , Qu Yang,1 , Wanshun Chen1 , Jiadi Yao1 , Huang Liu1 , Zihao Yi1 , Bo Zhao1 , Xingyu Chen1 , Ruotian Ma1 , Fanghua Ye1 , Erkun Yang2 , Cheng Deng2 , Zhaopeng Tu ,1 , Xiaolong Li1 , and Linus1 1Tencent Hunyuan Multimodal Department 2Xidian University https://github.com/Tencent/digitalhuman/tree/main/ScriptAgent Figure 1: Our proposed pipeline consists of three key components: (1) ScripterAgent, trained with GRPO to align its outputs with professional directorial standards; (2) DirectorAgent, which ensures seamless visual continuity across scenes, thereby overcoming the temporal incoherence caused by the fixed-duration constraints of SOTA video generation models; and (3) CriticAgent, which evaluates the generated film from both technical and cinematic perspectives. "
[27.01.2026 04:42] Failed to download and parse paper https://huggingface.co/papers/2601.17737: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 04:42] Downloading and parsing paper https://huggingface.co/papers/2601.17058.
[27.01.2026 04:42] Downloading paper 2601.17058 from https://arxiv.org/pdf/2601.17058v1...
[27.01.2026 04:42] Extracting affiliations from text.
[27.01.2026 04:42] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, JANUARY 2025 1 Can LLMs Clean Up Your Mess? Survey of Application-Ready Data Preparation with LLMs Wei Zhou, Jun Zhou, Haoyu Wang, Zhenghao Li, Qikang He, Shaokun Han, Guoliang Li Fellow, IEEE, Xuanhe Zhou, Yeye He, Chunwei Liu, Zirui Tang, Bin Wang, Shen Tang, Kai Zuo, Yuyu Luo, Zhenzhe Zheng, Conghui He, Jingren Zhou Fellow, IEEE, Fan Wu Awesome-Data-LLM: https:// github.com/ weAIDB/ awesome-data-llm 6 2 0 2 J 2 2 ] . [ 1 8 5 0 7 1 . 1 0 6 2 : r AbstractData preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming transformative and potentially dominant paradigm for data preparation. By investigating hundreds of recent literature works, this paper presents systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in ad"
[27.01.2026 04:42] Failed to download and parse paper https://huggingface.co/papers/2601.17058: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 04:42] Downloading and parsing paper https://huggingface.co/papers/2601.17367.
[27.01.2026 04:42] Downloading paper 2601.17367 from https://arxiv.org/pdf/2601.17367v1...
[27.01.2026 04:42] Extracting affiliations from text.
[27.01.2026 04:42] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers Zecheng Tang * 1 2 Quantong Qiu * 1 2 Yi Yang 1 2 Zhiyi Hong 1 2 Haiya Xiang 1 2 Kebin Liu 3 Qingqing Dang 3 Juntao Li 1 2 Min Zhang 1 6 2 0 2 4 2 ] . [ 1 7 6 3 7 1 . 1 0 6 2 : r Abstract The quadratic complexity of standard attention mechanisms poses significant scalability bottleneck for large language models (LLMs) in longcontext scenarios. While hybrid attention strategies that combine sparse and full attention within single model offer viable solution, they typically employ static computation ratios (i.e., fixed proportions of sparse versus full attention) and fail to adapt to the varying sparsity sensitivities of downstream tasks during inference. To address this issue, we propose Elastic Attention, which allows the model to dynamically adjust its overall sparsity based on the input. This is achieved by integrating lightweight Attention Router into the existing pretrained model, which dynamically assigns each attention head to different computation modes. Within only 12 hours of training on 8A800 GPUs, our method enables models to achieve both strong performance and efficient inference (see Figure 1). Experiments across three long-context benchmarks on widely-used LLMs demonstrate the superiority of our method. 1. Introduction Large language models (LLMs) have demonstrated remarkable capabilities in processing long-context sequences (Liu et al., 2025a;b; Mei et al., 2025). However, the quadratic computational and memory complexity of standard full attention (FA) mechanisms (Vaswani et al., 2017) poses significant scalability bottleneck as the context window continues to expand. Sparse attention (SA) mechanisms (Child, 2019; Zaheer et al., 2020) represent an effective strategy for mitigating this limitation by selectively attending to subset of critical tokens, thereby substantially reducing computational overhead and improving inference throughput. To balance the trade-off between"
[27.01.2026 04:42] Failed to download and parse paper https://huggingface.co/papers/2601.17367: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 04:42] Downloading and parsing paper https://huggingface.co/papers/2601.18577.
[27.01.2026 04:42] Downloading paper 2601.18577 from https://arxiv.org/pdf/2601.18577v1...
[27.01.2026 04:42] Extracting affiliations from text.
[27.01.2026 04:42] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Self-Refining Video Sampling Sangwon Jang * 1 Taekyung Ki * 1 Jaehyeong Jo * 1 Saining Xie 2 Jaehong Yoon 3 Sung Ju Hwang 1 4 1KAIST 2NYU 3NTU Singapore 4DeepAuto.ai Project Page: https://agwmon.github.io/self-refine-video/ 6 2 0 2 6 2 ] . [ 1 7 7 5 8 1 . 1 0 6 2 : r a "
[27.01.2026 04:42] Failed to download and parse paper https://huggingface.co/papers/2601.18577: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 04:42] Downloading and parsing paper https://huggingface.co/papers/2601.17027.
[27.01.2026 04:42] Downloading paper 2601.17027 from https://arxiv.org/pdf/2601.17027v1...
[27.01.2026 04:43] Extracting affiliations from text.
[27.01.2026 04:43] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 7 1 ] . [ 1 7 2 0 7 1 . 1 0 6 2 : r Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility Honglin Lin1,2, Chonghan Qin3,2, Zheng Liu4,2, Qizhi Pei2, Yu Li2, Zhanping Zhong1,2, Xin Gao1,2, Yanfeng Wang1, Conghui He2, Lijun Wu2 1Shanghai Jiao Tong University, 2OpenDataLab, Shanghai Artificial Intelligence Laboratory, 3The University of Hong Kong, 4Peking University While synthetic data has proven effective for improving scientific reasoning in the text domain, multimodal reasoning remains constrained by the difficulty of synthesizing scientifically rigorous images. Existing Text-to-Image (T2I) models often produce outputs that are visually plausible yet scientifically incorrect, resulting in persistent visuallogic divergence that limits their value for downstream reasoning. Motivated by recent advances in next-generation T2I models, we conduct systematic study of scientific image synthesis across generation paradigms, evaluation, and downstream use. We analyze both direct pixel-based generation and programmatic synthesis, and propose ImgCoder, logic-driven framework that follows an explicit understand plan code workflow to improve structural precision. To rigorously assess scientific correctness, we introduce SciGenBench, which evaluates generated images based on information utility and logical validity. Our evaluation reveals systematic failure modes in pixel-based models and highlights fundamental expressivenessprecision trade-off. Finally, we show that fine-tuning Large Multimodal Models (LMMs) on rigorously verified synthetic scientific images yields consistent reasoning gains, with potential scaling trends analogous to the text domain, validating high-fidelity scientific synthesis as viable path to unlocking massive multimodal reasoning capabilities. Date: January 27, 2026 Equal contribution: Honglin Lin, Chonghan Qin, Zheng Liu Correspondence: Lijun Wu, wulijun@pjlab.org.cn Project Page: https://SciGenbench.github.io With the ad"
[27.01.2026 04:43] Failed to download and parse paper https://huggingface.co/papers/2601.17027: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 04:43] Downloading and parsing paper https://huggingface.co/papers/2601.17124.
[27.01.2026 04:43] Downloading paper 2601.17124 from https://arxiv.org/pdf/2601.17124v1...
[27.01.2026 04:45] Extracting affiliations from text.
[27.01.2026 04:45] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 3 2 ] . [ 1 4 2 1 7 1 . 1 0 6 2 : r 2026-01-27 iFSQ: Improving FSQ for Image Generation with 1 Line of Code Bin Lin1,2, Zongjian Li1, Yuwei Niu1, Kaixiong Gong2, Yunyang Ge1,2, Yunlong Lin2, Mingzhe Zheng2, JianWei Zhang2, Miles Yang2, Zhao Zhong2, Liefeng Bo2, Li Yuan1, 1Peking University 2Tencent Hunyuan "
[27.01.2026 04:45] Failed to download and parse paper https://huggingface.co/papers/2601.17124: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 04:45] Downloading and parsing paper https://huggingface.co/papers/2601.18184.
[27.01.2026 04:45] Downloading paper 2601.18184 from https://arxiv.org/pdf/2601.18184v1...
[27.01.2026 04:45] Extracting affiliations from text.
[27.01.2026 04:45] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 6 2 ] . [ 1 4 8 1 8 1 . 1 0 6 2 : r VIBEVOICE-ASR Technical Report Zhiliang Peng, Jianwei Yu, Yaoyao Chang, Zilong Wang, Li Dong Yingbo Hao, Yujie Tu, Chenyu Yang, Wenhui Wang, Songchen Xu, Yutao Sun Hangbo Bao, Weijiang Xu, Yi Zhu, Zehua Wang, Ting Song, Yan Xia, Zewen Chi Shaohan Huang, Liang Wang, Chuang Ding, Shuai Wang, Xie Chen, Furu Wei Microsoft Research https://aka.ms/GeneralAI This report presents VIBEVOICE-ASR, general-purpose speech understanding framework built upon VIBEVOICE [PYW+25], designed to address the persistent challenges of context fragmentation and multi-speaker complexity in long-form audio (e.g., meetings, podcasts) that remain despite recent advancements in short-form speech recognition. Unlike traditional pipelined approaches that rely on audio chunking, VIBEVOICE-ASR supports single-pass processing for up to 60 minutes of audio. It unifies Automatic Speech Recognition, Speaker Diarization, and Timestamping into single end-to-end generation task. In addition, VIBEVOICE-ASR supports over 50 languages, requires no explicit language setting, and natively handles code-switching within and across utterances. Furthermore, we introduce prompt-based context injection mechanism that allows users to supply customized conetxt, significantly improving accuracy on domain-specific terminology and polyphonic character disambiguation. Code: github.com/microsoft/VibeVoice Hugging Face: microsoft/VibeVoice Demo: aka.ms/VibeVoice-ASR Figure 1: VIBEVOICE-ASR sets new state-of-the-art for long-form speech understanding, consistently outperforming strong closed-source multimodal models (Gemini-2.5/3-Pro) across five public benchmarks. The results demonstrate superior accuracy in both speaker attribution (DER) and time-aligned transcription (tcpWER), particularly in complex multi-speaker environments. Recent years have witnessed paradigm shift in speech processing, driven by the integration of Large Language Models (LLMs) with acoustic encoders [CXZ+23]"
[27.01.2026 04:45] Failed to download and parse paper https://huggingface.co/papers/2601.18184: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 04:45] Downloading and parsing paper https://huggingface.co/papers/2601.17761.
[27.01.2026 04:45] Downloading paper 2601.17761 from https://arxiv.org/pdf/2601.17761v1...
[27.01.2026 04:45] Extracting affiliations from text.
[27.01.2026 04:45] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 5 2 ] . [ 1 1 6 7 7 1 . 1 0 6 2 : r 2026-1-22 AR-Omni: Unified Autoregressive Model for Any-to-Any Generation Dongjie Cheng1* Ruifeng Yuan1* Yongqi Li1 Runyang You1 Wenjie Wang2 Liqiang Nie3 Lei Zhang1 Wenjie Li 1 The Hong Kong Polytechnic University 2 University of Science and Technology of China 3 Harbin Institute of Technology (Shenzhen) {dong-jie.cheng,ruifeng.yuan}@connect.polyu.hk, liyongqi0@gmail.com Abstract Real-world perception and interaction are inherently multimodal, encompassing not only language but also vision and speech, which motivates the development of Omni MLLMs that support both multimodal inputs and multimodal outputs. While sequence of omni MLLMs has emerged, most existing systems still rely on additional expert components to achieve multimodal generation, limiting the simplicity of unified training and inference. Autoregressive (AR) modeling, with single token stream, single next-token objective, and single decoder, is an elegant and scalable foundation in the text domain. Motivated by this, we present AR-Omni, unified any-to-any model in the autoregressive paradigm without any expert decoders. AR-Omni supports autoregressive text and image generation, as well as streaming speech generation, all under single Transformer decoder. We further address three practical issues in unified AR modeling: modality imbalance via task-aware loss reweighting, visual fidelity via lightweight token-level perceptual alignment loss for image tokens, and stabilitycreativity trade-offs via finite-state decoding mechanism. Empirically, AR-Omni achieves strong quality across three modalities while remaining real-time, achieving 0.88 real-time factor for speech generation. Project Page: https://modalitydance.github.io/AR-Omni called Omni [2]. Large Language Models (LLMs) have achieved strong performance in understanding and generating natural language [1]. However, their interface is largely limited to text. In contrast, real-world perception and interactio"
[27.01.2026 04:45] Failed to download and parse paper https://huggingface.co/papers/2601.17761: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 04:45] Downloading and parsing paper https://huggingface.co/papers/2601.17111.
[27.01.2026 04:45] Downloading paper 2601.17111 from https://arxiv.org/pdf/2601.17111v1...
[27.01.2026 04:45] Extracting affiliations from text.
[27.01.2026 04:45] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 3 2 ] . [ 1 1 1 1 7 1 . 1 0 6 2 : r Preprint - 2026 Salesforce AI Research LEAST-LOADED EXPERT PARALLELISM: LOAD BALANCING AN IMBALANCED MIXTURE-OF-EXPERTS Xuan-Phi Nguyen, Shrey Pandit, Austin Xu, Caiming Xiong, Shafiq Joty Salesforce AI Research xnguyen@salesforce.com "
[27.01.2026 04:45] Failed to download and parse paper https://huggingface.co/papers/2601.17111: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 04:45] Downloading and parsing paper https://huggingface.co/papers/2601.15849.
[27.01.2026 04:45] Downloading paper 2601.15849 from https://arxiv.org/pdf/2601.15849v1...
[27.01.2026 04:45] Extracting affiliations from text.
[27.01.2026 04:45] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 2 2 ] . [ 1 9 4 8 5 1 . 1 0 6 2 : r CGPT: Cluster-Guided Partial Tables with LLM-Generated Supervision for Table Retrieval Tsung-Hsiang Chou National Chung Hsing University Smart Sustainable New Agriculture Research Center (SMARTer) Taichung, Taiwan yumeow0122@smail.nchu.edu.tw Chen-Jui Yu National Chung Hsing University Smart Sustainable New Agriculture Research Center (SMARTer) Taichung, Taiwan rui0828@smail.nchu.edu.tw Shui-Hsiang Hsu National Chung Hsing University Smart Sustainable New Agriculture Research Center (SMARTer) Taichung, Taiwan g113056055@smail.nchu.edu.tw Yao-Chung Fan National Chung Hsing University Smart Sustainable New Agriculture Research Center (SMARTer) Taichung, Taiwan yfan@nchu.edu.tw "
[27.01.2026 04:45] Failed to download and parse paper https://huggingface.co/papers/2601.15849: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 04:45] Downloading and parsing paper https://huggingface.co/papers/2601.18130.
[27.01.2026 04:45] Downloading paper 2601.18130 from https://arxiv.org/pdf/2601.18130v1...
[27.01.2026 04:45] Extracting affiliations from text.
[27.01.2026 04:45] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"RouteMoA: Dynamic Routing without Pre-Inference Boosts Efficient Mixture-of-Agents Jize Wang1 Han Wu1 Zhiyuan You2 Yiming Song1 Yijun Wang3 Zifei Shan3 Yining Li4 Songyang Zhang4 Xinyi Le1 Cailian Chen1* Xinping Guan1 Dacheng Tao5 1 Shanghai Jiao Tong University 2 CUHK 3 Tencent 4Shanghai AI Laboratory 5Nanyang Technological University {jizewang2000,lexinyi,cailianchen}@sjtu.edu.cn 6 2 0 2 6 2 ] . [ 1 0 3 1 8 1 . 1 0 6 2 : r a "
[27.01.2026 04:45] Failed to download and parse paper https://huggingface.co/papers/2601.18130: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 04:45] Downloading and parsing paper https://huggingface.co/papers/2601.18081.
[27.01.2026 04:45] Downloading paper 2601.18081 from https://arxiv.org/pdf/2601.18081v1...
[27.01.2026 04:45] Extracting affiliations from text.
[27.01.2026 04:45] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 6 2 ] . [ 1 1 8 0 8 1 . 1 0 6 2 : r DRPG (Decompose, Retrieve, Plan, Generate): An Agentic Framework for Academic Rebuttal Peixuan Han, Yingjie Yu, Jingjun Xu, Jiaxuan You University of Illinois Urbana-Champaign {ph16,yyu69,jingjunx,jiaxuan}@illinois.edu "
[27.01.2026 04:45] Failed to download and parse paper https://huggingface.co/papers/2601.18081: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 04:45] Downloading and parsing paper https://huggingface.co/papers/2601.17323.
[27.01.2026 04:45] Downloading paper 2601.17323 from https://arxiv.org/pdf/2601.17323v1...
[27.01.2026 04:45] Extracting affiliations from text.
[27.01.2026 04:45] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 4 2 ] . [ 1 3 2 3 7 1 . 1 0 6 2 : r SkyReels-V3 Technique Report SkyReels Team "
[27.01.2026 04:45] Failed to download and parse paper https://huggingface.co/papers/2601.17323: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 04:45] Downloading and parsing paper https://huggingface.co/papers/2601.16207.
[27.01.2026 04:45] Downloading paper 2601.16207 from https://arxiv.org/pdf/2601.16207v1...
[27.01.2026 04:45] Extracting affiliations from text.
[27.01.2026 04:45] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"IVRA: Improving Visual-Token Relations for Robot Action Policy with Training-Free Hint-Based Guidance Jongwoo Park1, Kanchana Ranasinghe1, Jinhyeok Jang2, Cristina Mata1, Yoo Sung Jang1, Michael Ryoo1 1Stony Brook University 2ETRI jongwopark@cs.stonybrook.edu 6 2 0 J 2 2 ] . [ 1 7 0 2 6 1 . 1 0 6 2 : r Abstract Many Vision-Language-Action (VLA) models flatten image patches into 1D token sequence, weakening the 2D spatial cues needed for precise manipulation. We introduce IVRA, lightweight, training-free method that improves spatial understanding by exploiting affinity hints already available in the models built-in vision encoder, without requiring any external encoder or retraining. IVRA selectively injects layer in which these affinity signals into language-model instance-level features reside. This inference-time intervention realigns visual-token interactions and better preserves geometric structure while keeping all model parameters fixed. We demonstrate the generality of IVRA by applying it to diverse VLA architectures (LLaRA, OpenVLA, and FLOWER) across simulated benchmarks spanning both 2D and 3D manipulation (VIMA and LIBERO) and on various real-robot tasks. On 2D VIMA, IVRA improves average success by +4.2% over the baseline LLaRA in low-data regime. On 3D LIBERO, it yields consistent gains over the OpenVLA and FLOWER baselines, including improvements when baseline accuracy is near saturation (96.3% 97.1%). All code and models will be released publicly. Visualizations are available at: jongwoopark7978.github.io/IVRA I. INTRODUCTION Vision-Language-Action (VLA) models have rapidly emerged as promising approach for generating robot actions from images and natural-language instructions. Recent systems such as LLaRA [1], OpenVLA [2], FLOWER [3], and LLARVA [4] pair large-scale pretrained vision encoders (e.g., CLIP or DINO [5], [6]) with language models by flattening the 2D patch grid and appending the resulting visual tokens to the text sequence in single Tran"
[27.01.2026 04:45] Failed to download and parse paper https://huggingface.co/papers/2601.16207: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 04:45] Downloading and parsing paper https://huggingface.co/papers/2601.18759.
[27.01.2026 04:45] Downloading paper 2601.18759 from https://arxiv.org/pdf/2601.18759v1...
[27.01.2026 04:45] Extracting affiliations from text.
[27.01.2026 04:45] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 6 2 ] . [ 1 9 5 7 8 1 . 1 0 6 2 : r UI Remix: Supporting UI Design Through Interactive Example Retrieval and Remixing Junling Wang ETH Zurich Zurich, Switzerland junling.wang@ai.ethz.ch Hongyi Lan ETH Zurich Zurich, Switzerland honlan@student.ethz.ch Xiaotian Su ETH Zurich Zurich, Switzerland xiaotian.su@inf.ethz.ch Mustafa Doga Dogan Adobe Research Basel, Switzerland doga@adobe.com April Yi Wang ETH Zurich Zurich, Switzerland april.wang@inf.ethz.ch Figure 1: An overview of UI Remix, an example-driven assistant for mobile UI design featuring three main panels: (A) Conversation Panel: Users describe their design goals and interact with the system through three modes: Chat (generate and refine UI designs), Search (retrieve relevant UIs for inspiration), and Apply (adapt entire design through global remix or specific components through local remix from selected examples). (B) Example Gallery: Displays retrieved real-world UI examples along with source transparency cues (e.g., ratings, download counts, developer information) to help users assess credibility. (C) Editable Canvas: Presents live preview of the current design, supporting toggling between visual and code views. These authors contributed equally to this work. "
[27.01.2026 04:45] Failed to download and parse paper https://huggingface.co/papers/2601.18759: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 04:45] Downloading and parsing paper https://huggingface.co/papers/2601.18202.
[27.01.2026 04:45] Downloading paper 2601.18202 from https://arxiv.org/pdf/2601.18202v1...
[27.01.2026 04:45] Extracting affiliations from text.
[27.01.2026 04:45] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 6 2 ] . [ 1 2 0 2 8 1 . 1 0 6 2 : r SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback Fangyuan Xu*2, Rujun Han1, Yanfei Chen1, Zifeng Wang1, I-Hung Hsu1, Jun Yan1, Vishy Tirumalashetty1, Eunsol Choi2, Tomas Pfister1 and Chen-Yu Lee1 1Google Cloud AI Research, 2New York University Deep search agents, which aim to answer complex questions requiring reasoning across multiple documents, can significantly speed up the information-seeking process. Collecting human annotations for this application is prohibitively expensive due to long and complex exploration trajectories. We propose an agentic pipeline that automatically generates high-quality, difficulty-controlled deep search question-answer pairs for given corpus and target difficulty level. Our pipeline, SAGE, consists of data generator which proposes QA pairs and search agent which attempts to solve the generated question and provide execution feedback for the data generator. The two components interact over multiple rounds to iteratively refine the question-answer pairs until they satisfy the target difficulty level. Our intrinsic evaluation shows SAGE generates questions that require diverse reasoning strategies, while significantly increases the correctness and difficulty of the generated data. Our extrinsic evaluation demonstrates up to 23% relative performance gain on popular deep search benchmarks by training deep search agents with our synthetic data. Additional experiments show that agents trained on our data can adapt from fixed-corpus retrieval to Google Search at inference time, without further training. 1. Introduction Large language models (LLMs) are increasingly used as agents which interact with external environments and solve complicated tasks, such as coding (Dong et al., 2025; Jimenez et al., 2024), e-commerce and social forum discussion (Peeters et al., 2025; Zhou et al., 2024). Recently, there is growing interest in building search-augmented agents that retrie"
[27.01.2026 04:45] Failed to download and parse paper https://huggingface.co/papers/2601.18202: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 04:45] Downloading and parsing paper https://huggingface.co/papers/2601.18157.
[27.01.2026 04:45] Downloading paper 2601.18157 from https://arxiv.org/pdf/2601.18157v1...
[27.01.2026 04:45] Extracting affiliations from text.
[27.01.2026 04:45] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Agentic Very Long Video Understanding Aniket Rege1,2,, Arka Sadhu1, Yuliang Li1, Kejie Li1, Ramya Korlakai Vinayak2, Yuning Chai1, Yong Jae Lee2, Hyo Jin Kim1 1Reality Labs Research at Meta, 2University of Wisconsin-Madison Work done at Meta The advent of always-on personal AI assistants, enabled by all-day wearable devices such as smart glasses, demands new level of contextual understanding, one that goes beyond short, isolated events to encompass the continuous, longitudinal stream of egocentric video. Achieving this vision requires advances in long-horizon video understanding, where systems must interpret and recall visual and audio information spanning days or even weeks. Existing methods, including large language models and retrieval-augmented generation, are constrained by limited context windows and lack the ability to perform compositional, multi-hop reasoning over very long video streams. In this work, we address these challenges through EGAgent, an enhanced agentic framework centered on entity scene graphs, which represent people, places, objects, and their relationships over time. Our system equips planning agent with tools for structured search and reasoning over these graphs, as well as hybrid visual and audio search capabilities, enabling detailed, cross-modal, and temporally coherent reasoning. Experiments on the EgoLifeQA and Video-MME (Long) datasets show that our method achieves state-of-the-art performance on EgoLifeQA (57.5%) and competitive performance on Video-MME (Long) (74.1%) for complex longitudinal video understanding tasks. Correspondence: Aniket Rege at <aniketr@cs.wisc.edu>, Hyo Jin Kim at <kimhyojin@meta.com> 6 2 0 2 J 6 2 ] . [ 1 7 5 1 8 1 . 1 0 6 2 : r Figure 1 Given natural language query, our agentic framework EGAgent decomposes the query into subtasks and leverages visual search, audio transcript search, and entity scene graph search to identify relevant events spanning multiple days. This example highlights the frameworks ability"
[27.01.2026 04:45] Failed to download and parse paper https://huggingface.co/papers/2601.18157: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 04:45] Enriching papers with extra data.
[27.01.2026 04:45] ********************************************************************************
[27.01.2026 04:45] Abstract 0. A novel end-to-end agentic framework translates dialogue into cinematic videos through specialized agents that generate and orchestrate video content while maintaining narrative coherence.  					AI-generated summary 				 Recent advances in video generation have produced models capable of synthesizin...
[27.01.2026 04:45] ********************************************************************************
[27.01.2026 04:45] Abstract 1. LLM-enhanced data preparation methods are transforming data-centric workflows from rule-based pipelines to prompt-driven, context-aware approaches, organized into data cleaning, integration, and enrichment tasks.  					AI-generated summary 				 Data preparation aims to denoise raw datasets, uncover ...
[27.01.2026 04:45] ********************************************************************************
[27.01.2026 04:45] Abstract 2. Elastic Attention enables dynamic adjustment of attention sparsity during inference by integrating a lightweight Attention Router into pretrained models, achieving efficient long-context processing.  					AI-generated summary 				 The quadratic complexity of standard attention mechanisms poses a sig...
[27.01.2026 04:45] ********************************************************************************
[27.01.2026 04:45] Abstract 3. Self-refining video sampling improves motion coherence and physics alignment by using a pre-trained video generator as its own denoising autoencoder for iterative refinement with uncertainty-aware region selection.  					AI-generated summary 				 Modern video generators still struggle with complex p...
[27.01.2026 04:45] ********************************************************************************
[27.01.2026 04:45] Abstract 4. Scientific image synthesis using logic-driven frameworks like ImgCoder improves multimodal reasoning by addressing visual-logic divergence through structured generation and evaluation benchmarks.  					AI-generated summary 				 While synthetic data has proven effective for improving scientific reaso...
[27.01.2026 04:45] ********************************************************************************
[27.01.2026 04:45] Abstract 5. Finite Scalar Quantization with improved activation mapping enables unified modeling of discrete and continuous image generation approaches, revealing optimal representation balance and performance characteristics.  					AI-generated summary 				 The field of image generation is currently bifurcated...
[27.01.2026 04:45] ********************************************************************************
[27.01.2026 04:45] Abstract 6. VibeVoice-ASR is a unified end-to-end speech understanding framework that processes long-form audio in a single pass while supporting multilingual, code-switching, and domain-specific context injection.  					AI-generated summary 				 This report presents VibeVoice-ASR, a general-purpose speech unde...
[27.01.2026 04:45] ********************************************************************************
[27.01.2026 04:45] Abstract 7. AR-Omni is a unified autoregressive model that supports multimodal input and output generation through a single Transformer decoder, addressing modality balance, visual fidelity, and stability-creativity trade-offs.  					AI-generated summary 				 Real-world perception and interaction are inherently...
[27.01.2026 04:45] ********************************************************************************
[27.01.2026 04:45] Abstract 8. Imbalanced expert routing in Mixture-of-Experts models leads to computational inefficiencies in expert parallelism, which are addressed by a dynamic rerouting algorithm that balances workload and reduces memory usage.  					AI-generated summary 				 Mixture-of-Experts (MoE) models are typically pre-...
[27.01.2026 04:45] ********************************************************************************
[27.01.2026 04:45] Abstract 9. CGPT improves table retrieval by using LLM-generated synthetic queries for contrastive fine-tuning of embedding models through semantically diverse partial table construction.  					AI-generated summary 				 General-purpose embedding models have demonstrated strong performance in text retrieval but ...
[27.01.2026 04:45] ********************************************************************************
[27.01.2026 04:45] Abstract 10. RouteMoA reduces computational costs and latency in mixture-of-agents frameworks by using dynamic routing with lightweight scoring and judgment mechanisms.  					AI-generated summary 				 Mixture-of-Agents (MoA) improves LLM performance through layered collaboration, but its dense topology raises co...
[27.01.2026 04:45] ********************************************************************************
[27.01.2026 04:45] Abstract 11. An agentic framework for automatic academic rebuttal generation that decomposes reviews, retrieves evidence, plans rebuttal strategies, and generates persuasive responses with human-level performance using an 8B model.  					AI-generated summary 				 Despite the growing adoption of large language mo...
[27.01.2026 04:45] ********************************************************************************
[27.01.2026 04:45] Abstract 12. SkyReels-V3 is a unified multimodal video generation model that supports reference image-to-video, video-to-video extension, and audio-guided video generation through diffusion Transformers and in-context learning frameworks.  					AI-generated summary 				 Video generation serves as a cornerstone f...
[27.01.2026 04:45] ********************************************************************************
[27.01.2026 04:45] Abstract 13. IVRA enhances spatial understanding in vision-language-action models by injecting affinity signals into language-model layers without retraining or external encoders.  					AI-generated summary 				 Many Vision-Language-Action (VLA) models flatten image patches into a 1D token sequence, weakening th...
[27.01.2026 04:45] ********************************************************************************
[27.01.2026 04:45] Abstract 14. UI Remix is an interactive system that supports mobile UI design through example-driven workflows using a multimodal retrieval-augmented generation model, enabling iterative design adaptation with source transparency cues.  					AI-generated summary 				 Designing user interfaces (UIs) is a critical...
[27.01.2026 04:45] ********************************************************************************
[27.01.2026 04:45] Abstract 15. Deep search agents trained on synthetic question-answer pairs generated through an iterative agent-based pipeline demonstrate improved performance and adaptability across different search environments.  					AI-generated summary 				 Deep search agents, which aim to answer complex questions requirin...
[27.01.2026 04:45] ********************************************************************************
[27.01.2026 04:45] Abstract 16. An agentic framework using entity scene graphs enables long-horizon video understanding with structured search, temporal reasoning, and cross-modal capabilities for extended visual and audio interpretation.  					AI-generated summary 				 The advent of always-on personal AI assistants, enabled by al...
[27.01.2026 04:45] Read previous papers.
[27.01.2026 04:45] Generating reviews via LLM API.
[27.01.2026 04:45] Querying the API.
[27.01.2026 04:45] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A novel end-to-end agentic framework translates dialogue into cinematic videos through specialized agents that generate and orchestrate video content while maintaining narrative coherence.  					AI-generated summary 				 Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap'' between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking.
[27.01.2026 04:45] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 04:45] Querying the API.
[27.01.2026 04:45] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

LLM-enhanced data preparation methods are transforming data-centric workflows from rule-based pipelines to prompt-driven, context-aware approaches, organized into data cleaning, integration, and enrichment tasks.  					AI-generated summary 				 Data preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for a wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming a transformative and potentially dominant paradigm for data preparation.   By investigating hundreds of recent literature works, this paper presents a systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce a task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in advanced agents, the mismatch between advanced methods and weak evaluation). Moreover, we analyze commonly used datasets and evaluation metrics (the empirical part). Finally, we discuss open research challenges and outline a forward-looking roadmap that emphasizes scalable LLM-data systems, principled designs for reliable agentic workflows, and robust evaluation protocols.
[27.01.2026 04:45] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 04:45] Querying the API.
[27.01.2026 04:45] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Elastic Attention enables dynamic adjustment of attention sparsity during inference by integrating a lightweight Attention Router into pretrained models, achieving efficient long-context processing.  					AI-generated summary 				 The quadratic complexity of standard attention mechanisms poses a significant scalability bottleneck for large language models (LLMs) in long-context scenarios. While hybrid attention strategies that combine sparse and full attention within a single model offer a viable solution, they typically employ static computation ratios (i.e., fixed proportions of sparse versus full attention) and fail to adapt to the varying sparsity sensitivities of downstream tasks during inference. To address this issue, we propose Elastic Attention, which allows the model to dynamically adjust its overall sparsity based on the input. This is achieved by integrating a lightweight Attention Router into the existing pretrained model, which dynamically assigns each attention head to different computation modes. Within only 12 hours of training on 8xA800 GPUs, our method enables models to achieve both strong performance and efficient inference. Experiments across three long-context benchmarks on widely-used LLMs demonstrate the superiority of our method.
[27.01.2026 04:46] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 04:46] Querying the API.
[27.01.2026 04:46] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Self-refining video sampling improves motion coherence and physics alignment by using a pre-trained video generator as its own denoising autoencoder for iterative refinement with uncertainty-aware region selection.  					AI-generated summary 				 Modern video generators still struggle with complex physical dynamics, often falling short of physical realism. Existing approaches address this using external verifiers or additional training on augmented data, which is computationally expensive and still limited in capturing fine-grained motion. In this work, we present self-refining video sampling, a simple method that uses a pre-trained video generator trained on large-scale datasets as its own self-refiner. By interpreting the generator as a denoising autoencoder, we enable iterative inner-loop refinement at inference time without any external verifier or additional training. We further introduce an uncertainty-aware refinement strategy that selectively refines regions based on self-consistency, which prevents artifacts caused by over-refinement. Experiments on state-of-the-art video generators demonstrate significant improvements in motion coherence and physics alignment, achieving over 70\% human preference compared to the default sampler and guidance-based sampler.
[27.01.2026 04:46] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 04:46] Querying the API.
[27.01.2026 04:46] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Scientific image synthesis using logic-driven frameworks like ImgCoder improves multimodal reasoning by addressing visual-logic divergence through structured generation and evaluation benchmarks.  					AI-generated summary 				 While synthetic data has proven effective for improving scientific reasoning in the text domain, multimodal reasoning remains constrained by the difficulty of synthesizing scientifically rigorous images. Existing Text-to-Image (T2I) models often produce outputs that are visually plausible yet scientifically incorrect, resulting in a persistent visual-logic divergence that limits their value for downstream reasoning. Motivated by recent advances in next-generation T2I models, we conduct a systematic study of scientific image synthesis across generation paradigms, evaluation, and downstream use. We analyze both direct pixel-based generation and programmatic synthesis, and propose ImgCoder, a logic-driven framework that follows an explicit "understand - plan - code" workflow to improve structural precision. To rigorously assess scientific correctness, we introduce SciGenBench, which evaluates generated images based on information utility and logical validity. Our evaluation reveals systematic failure modes in pixel-based models and highlights a fundamental expressiveness-precision trade-off. Finally, we show that fine-tuning Large Multimodal Models (LMMs) on rigorously verified synthetic scientific images yields consistent reasoning gains, with potential scaling trends analogous to the text domain, validating high-fidelity scientific synthesis as a viable path to unlocking massive multimodal reasoning capabilities.
[27.01.2026 04:46] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 04:46] Querying the API.
[27.01.2026 04:46] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Finite Scalar Quantization with improved activation mapping enables unified modeling of discrete and continuous image generation approaches, revealing optimal representation balance and performance characteristics.  					AI-generated summary 				 The field of image generation is currently bifurcated into autoregressive (AR) models operating on discrete tokens and diffusion models utilizing continuous latents. This divide, rooted in the distinction between VQ-VAEs and VAEs, hinders unified modeling and fair benchmarking. Finite Scalar Quantization (FSQ) offers a theoretical bridge, yet vanilla FSQ suffers from a critical flaw: its equal-interval quantization can cause activation collapse. This mismatch forces a trade-off between reconstruction fidelity and information efficiency. In this work, we resolve this dilemma by simply replacing the activation function in original FSQ with a distribution-matching mapping to enforce a uniform prior. Termed iFSQ, this simple strategy requires just one line of code yet mathematically guarantees both optimal bin utilization and reconstruction precision. Leveraging iFSQ as a controlled benchmark, we uncover two key insights: (1) The optimal equilibrium between discrete and continuous representations lies at approximately 4 bits per dimension. (2) Under identical reconstruction constraints, AR models exhibit rapid initial convergence, whereas diffusion models achieve a superior performance ceiling, suggesting that strict sequential ordering may limit the upper bounds of generation quality. Finally, we extend our analysis by adapting Representation Alignment (REPA) to AR models, yielding LlamaGen-REPA. Codes is available at https://github.com/Tencent-Hunyuan/iFSQ
[27.01.2026 04:46] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 04:46] Querying the API.
[27.01.2026 04:46] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

VibeVoice-ASR is a unified end-to-end speech understanding framework that processes long-form audio in a single pass while supporting multilingual, code-switching, and domain-specific context injection.  					AI-generated summary 				 This report presents VibeVoice-ASR, a general-purpose speech understanding framework built upon VibeVoice, designed to address the persistent challenges of context fragmentation and multi-speaker complexity in long-form audio (e.g., meetings, podcasts) that remain despite recent advancements in short-form speech recognition. Unlike traditional pipelined approaches that rely on audio chunking, VibeVoice-ASRsupports single-pass processing for up to 60 minutes of audio. It unifies Automatic Speech Recognition, Speaker Diarization, and Timestamping into a single end-to-end generation task. In addition, VibeVoice-ASR supports over 50 languages, requires no explicit language setting, and natively handles code-switching within and across utterances. Furthermore, we introduce a prompt-based context injection mechanism that allows users to supply customized conetxt, significantly improving accuracy on domain-specific terminology and polyphonic character disambiguation.
[27.01.2026 04:46] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 04:46] Querying the API.
[27.01.2026 04:46] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

AR-Omni is a unified autoregressive model that supports multimodal input and output generation through a single Transformer decoder, addressing modality balance, visual fidelity, and stability-creativity trade-offs.  					AI-generated summary 				 Real-world perception and interaction are inherently multimodal, encompassing not only language but also vision and speech, which motivates the development of "Omni" MLLMs that support both multimodal inputs and multimodal outputs. While a sequence of omni MLLMs has emerged, most existing systems still rely on additional expert components to achieve multimodal generation, limiting the simplicity of unified training and inference. Autoregressive (AR) modeling, with a single token stream, a single next-token objective, and a single decoder, is an elegant and scalable foundation in the text domain. Motivated by this, we present AR-Omni, a unified any-to-any model in the autoregressive paradigm without any expert decoders. AR-Omni supports autoregressive text and image generation, as well as streaming speech generation, all under a single Transformer decoder. We further address three practical issues in unified AR modeling: modality imbalance via task-aware loss reweighting, visual fidelity via a lightweight token-level perceptual alignment loss for image tokens, and stability-creativity trade-offs via a finite-state decoding mechanism. Empirically, AR-Omni achieves strong quality across three modalities while remaining real-time, achieving a 0.88 real-time factor for speech generation.
[27.01.2026 04:46] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 04:46] Querying the API.
[27.01.2026 04:46] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Imbalanced expert routing in Mixture-of-Experts models leads to computational inefficiencies in expert parallelism, which are addressed by a dynamic rerouting algorithm that balances workload and reduces memory usage.  					AI-generated summary 				 Mixture-of-Experts (MoE) models are typically pre-trained with explicit load-balancing constraints to ensure statistically balanced expert routing. Despite this, we observe that even well-trained MoE models exhibit significantly imbalanced routing. This behavior is arguably natural-and even desirable - as imbalanced routing allows models to concentrate domain-specific knowledge within a subset of experts. Expert parallelism (EP) is designed to scale MoE models by distributing experts across multiple devices, but with a less-discussed assumption of balanced routing. Under extreme imbalance, EP can funnel a disproportionate number of tokens to a small number of experts, leading to compute- and memory-bound failures on overloaded devices during post-training or inference, where explicit load balancing is often inapplicable. We propose Least-Loaded Expert Parallelism (LLEP), a novel EP algorithm that dynamically reroutes excess tokens and associated expert parameters from overloaded devices to underutilized ones. This ensures that all devices complete their workloads within the minimum collective latency while respecting memory constraints. Across different model scales, LLEP achieves up to 5x speedup and 4x reduction in peak memory usage compared to standard EP. This enables faster and higher-throughput post-training and inference, with ~1.9x faster for gpt-oss-120b. We support our method with extensive theoretical analysis and comprehensive empirical evaluations, including ablation studies. These results illuminate key trade-offs and enable a principled framework for hardware-specific hyper-parameter tuning to achieve optimal performance.
[27.01.2026 04:46] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 04:46] Querying the API.
[27.01.2026 04:46] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

CGPT improves table retrieval by using LLM-generated synthetic queries for contrastive fine-tuning of embedding models through semantically diverse partial table construction.  					AI-generated summary 				 General-purpose embedding models have demonstrated strong performance in text retrieval but remain suboptimal for table retrieval, where highly structured content leads to semantic compression and query-table mismatch. Recent LLM-based retrieval augmentation methods mitigate this issue by generating synthetic queries, yet they often rely on heuristic partial-table selection and seldom leverage these synthetic queries as supervision to improve the embedding model. We introduce CGPT, a training framework that enhances table retrieval through LLM-generated supervision. CGPT constructs semantically diverse partial tables by clustering table instances using K-means and sampling across clusters to broaden semantic coverage. An LLM then generates synthetic queries for these partial tables, which are used in hard-negative contrastive fine-tuning to refine the embedding model. Experiments across four public benchmarks (MimoTable, OTTQA, FetaQA, and E2E-WTQ) show that CGPT consistently outperforms retrieval baselines, including QGpT, with an average R@1 improvement of 16.54 percent. In a unified multi-domain corpus setting, CGPT further demonstrates strong cross-domain generalization and remains effective even when using smaller LLMs for synthetic query generation. These results indicate that semantically guided partial-table construction, combined with contrastive training from LLM-generated supervision, provides an effective and scalable paradigm for large-scale table retrieval. Our code is available at https://github.com/yumeow0122/CGPT.
[27.01.2026 04:46] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 04:46] Querying the API.
[27.01.2026 04:46] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

RouteMoA reduces computational costs and latency in mixture-of-agents frameworks by using dynamic routing with lightweight scoring and judgment mechanisms.  					AI-generated summary 				 Mixture-of-Agents (MoA) improves LLM performance through layered collaboration, but its dense topology raises costs and latency. Existing methods employ LLM judges to filter responses, yet still require all models to perform inference before judging, failing to cut costs effectively. They also lack model selection criteria and struggle with large model pools, where full inference is costly and can exceed context limits. To address this, we propose RouteMoA, an efficient mixture-of-agents framework with dynamic routing. It employs a lightweight scorer to perform initial screening by predicting coarse-grained performance from the query, narrowing candidates to a high-potential subset without inference. A mixture of judges then refines these scores through lightweight self- and cross-assessment based on existing model outputs, providing posterior correction without additional inference. Finally, a model ranking mechanism selects models by balancing performance, cost, and latency. RouteMoA outperforms MoA across varying tasks and model pool sizes, reducing cost by 89.8% and latency by 63.6% in the large-scale model pool.
[27.01.2026 04:46] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 04:46] Querying the API.
[27.01.2026 04:46] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

An agentic framework for automatic academic rebuttal generation that decomposes reviews, retrieves evidence, plans rebuttal strategies, and generates persuasive responses with human-level performance using an 8B model.  					AI-generated summary 				 Despite the growing adoption of large language models (LLMs) in scientific research workflows, automated support for academic rebuttal, a crucial step in academic communication and peer review, remains largely underexplored. Existing approaches typically rely on off-the-shelf LLMs or simple pipelines, which struggle with long-context understanding and often fail to produce targeted and persuasive responses. In this paper, we propose DRPG, an agentic framework for automatic academic rebuttal generation that operates through four steps: Decompose reviews into atomic concerns, Retrieve relevant evidence from the paper, Plan rebuttal strategies, and Generate responses accordingly. Notably, the Planner in DRPG reaches over 98% accuracy in identifying the most feasible rebuttal direction. Experiments on data from top-tier conferences demonstrate that DRPG significantly outperforms existing rebuttal pipelines and achieves performance beyond the average human level using only an 8B model. Our analysis further demonstrates the effectiveness of the planner design and its value in providing multi-perspective and explainable suggestions. We also showed that DRPG works well in a more complex multi-round setting. These results highlight the effectiveness of DRPG and its potential to provide high-quality rebuttal content and support the scaling of academic discussions. Codes for this work are available at https://github.com/ulab-uiuc/DRPG-RebuttalAgent.
[27.01.2026 04:46] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 04:46] Querying the API.
[27.01.2026 04:46] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

SkyReels-V3 is a unified multimodal video generation model that supports reference image-to-video, video-to-video extension, and audio-guided video generation through diffusion Transformers and in-context learning frameworks.  					AI-generated summary 				 Video generation serves as a cornerstone for building world models, where multimodal contextual inference stands as the defining test of capability. In this end, we present SkyReels-V3, a conditional video generation model, built upon a unified multimodal in-context learning framework with diffusion Transformers. SkyReels-V3 model supports three core generative paradigms within a single architecture: reference images-to-video synthesis, video-to-video extension and audio-guided video generation. (i) reference images-to-video model is designed to produce high-fidelity videos with strong subject identity preservation, temporal coherence, and narrative consistency. To enhance reference adherence and compositional stability, we design a comprehensive data processing pipeline that leverages cross frame pairing, image editing, and semantic rewriting, effectively mitigating copy paste artifacts. During training, an image video hybrid strategy combined with multi-resolution joint optimization is employed to improve generalization and robustness across diverse scenarios. (ii) video extension model integrates spatio-temporal consistency modeling with large-scale video understanding, enabling both seamless single-shot continuation and intelligent multi-shot switching with professional cinematographic patterns. (iii) Talking avatar model supports minute-level audio-conditioned video generation by training first-and-last frame insertion patterns and reconstructing key-frame inference paradigms. On the basis of ensuring visual quality, synchronization of audio and videos has been optimized.   Extensive evaluations demonstrate that SkyReels-V3 achieves state-of-the-art or near state-of-the-art performance on key metrics including visual quality, instruction following, and specific aspect metrics, approaching leading closed-source systems. Github: https://github.com/SkyworkAI/SkyReels-V3.
[27.01.2026 04:46] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 04:46] Querying the API.
[27.01.2026 04:46] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

IVRA enhances spatial understanding in vision-language-action models by injecting affinity signals into language-model layers without retraining or external encoders.  					AI-generated summary 				 Many Vision-Language-Action (VLA) models flatten image patches into a 1D token sequence, weakening the 2D spatial cues needed for precise manipulation. We introduce IVRA, a lightweight, training-free method that improves spatial understanding by exploiting affinity hints already available in the model's built-in vision encoder, without requiring any external encoder or retraining. IVRA selectively injects these affinity signals into a language-model layer in which instance-level features reside. This inference-time intervention realigns visual-token interactions and better preserves geometric structure while keeping all model parameters fixed. We demonstrate the generality of IVRA by applying it to diverse VLA architectures (LLaRA, OpenVLA, and FLOWER) across simulated benchmarks spanning both 2D and 3D manipulation (VIMA and LIBERO) and on various real-robot tasks. On 2D VIMA, IVRA improves average success by +4.2% over the baseline LLaRA in a low-data regime. On 3D LIBERO, it yields consistent gains over the OpenVLA and FLOWER baselines, including improvements when baseline accuracy is near saturation (96.3% to 97.1%). All code and models will be released publicly. Visualizations are available at: jongwoopark7978.github.io/IVRA
[27.01.2026 04:46] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 04:46] Querying the API.
[27.01.2026 04:46] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

UI Remix is an interactive system that supports mobile UI design through example-driven workflows using a multimodal retrieval-augmented generation model, enabling iterative design adaptation with source transparency cues.  					AI-generated summary 				 Designing user interfaces (UIs) is a critical step when launching products, building portfolios, or personalizing projects, yet end users without design expertise often struggle to articulate their intent and to trust design choices. Existing example-based tools either promote broad exploration, which can cause overwhelm and design drift, or require adapting a single example, risking design fixation. We present UI Remix, an interactive system that supports mobile UI design through an example-driven design workflow. Powered by a multimodal retrieval-augmented generation (MMRAG) model, UI Remix enables iterative search, selection, and adaptation of examples at both the global (whole interface) and local (component) level. To foster trust, it presents source transparency cues such as ratings, download counts, and developer information. In an empirical study with 24 end users, UI Remix significantly improved participants' ability to achieve their design goals, facilitated effective iteration, and encouraged exploration of alternative designs. Participants also reported that source transparency cues enhanced their confidence in adapting examples. Our findings suggest new directions for AI-assisted, example-driven systems that empower end users to design with greater control, trust, and openness to exploration.
[27.01.2026 04:46] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 04:46] Querying the API.
[27.01.2026 04:46] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Deep search agents trained on synthetic question-answer pairs generated through an iterative agent-based pipeline demonstrate improved performance and adaptability across different search environments.  					AI-generated summary 				 Deep search agents, which aim to answer complex questions requiring reasoning across multiple documents, can significantly speed up the information-seeking process. Collecting human annotations for this application is prohibitively expensive due to long and complex exploration trajectories. We propose an agentic pipeline that automatically generates high quality, difficulty-controlled deep search question-answer pairs for a given corpus and a target difficulty level. Our pipeline, SAGE, consists of a data generator which proposes QA pairs and a search agent which attempts to solve the generated question and provide execution feedback for the data generator. The two components interact over multiple rounds to iteratively refine the question-answer pairs until they satisfy the target difficulty level. Our intrinsic evaluation shows SAGE generates questions that require diverse reasoning strategies, while significantly increases the correctness and difficulty of the generated data. Our extrinsic evaluation demonstrates up to 23% relative performance gain on popular deep search benchmarks by training deep search agents with our synthetic data. Additional experiments show that agents trained on our data can adapt from fixed-corpus retrieval to Google Search at inference time, without further training.
[27.01.2026 04:46] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 04:46] Querying the API.
[27.01.2026 04:46] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

An agentic framework using entity scene graphs enables long-horizon video understanding with structured search, temporal reasoning, and cross-modal capabilities for extended visual and audio interpretation.  					AI-generated summary 				 The advent of always-on personal AI assistants, enabled by all-day wearable devices such as smart glasses, demands a new level of contextual understanding, one that goes beyond short, isolated events to encompass the continuous, longitudinal stream of egocentric video. Achieving this vision requires advances in long-horizon video understanding, where systems must interpret and recall visual and audio information spanning days or even weeks. Existing methods, including large language models and retrieval-augmented generation, are constrained by limited context windows and lack the ability to perform compositional, multi-hop reasoning over very long video streams. In this work, we address these challenges through EGAgent, an enhanced agentic framework centered on entity scene graphs, which represent people, places, objects, and their relationships over time. Our system equips a planning agent with tools for structured search and reasoning over these graphs, as well as hybrid visual and audio search capabilities, enabling detailed, cross-modal, and temporally coherent reasoning. Experiments on the EgoLifeQA and Video-MME (Long) datasets show that our method achieves state-of-the-art performance on EgoLifeQA (57.5%) and competitive performance on Video-MME (Long) (74.1%) for complex longitudinal video understanding tasks.
[27.01.2026 04:46] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 04:46] Renaming data file.
[27.01.2026 04:46] Renaming previous data. hf_papers.json to ./d/2026-01-27.json
[27.01.2026 04:46] Saving new data file.
[27.01.2026 04:46] Generating page.
[27.01.2026 04:46] Renaming previous page.
[27.01.2026 04:46] Renaming previous data. index.html to ./d/2026-01-27.html
[27.01.2026 04:46] Writing result.
[27.01.2026 04:46] Renaming log file.
[27.01.2026 04:46] Renaming previous data. log.txt to ./logs/2026-01-27_last_log.txt
