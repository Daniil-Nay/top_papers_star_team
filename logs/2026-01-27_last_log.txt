[27.01.2026 01:19] Read previous papers.
[27.01.2026 01:19] Generating top page (month).
[27.01.2026 01:19] Writing top page (month).
[27.01.2026 03:42] Read previous papers.
[27.01.2026 03:42] Get feed.
[27.01.2026 03:42] Extract page data from URL. URL: https://huggingface.co/papers/2601.17737
[27.01.2026 03:43] Extract page data from URL. URL: https://huggingface.co/papers/2601.17367
[27.01.2026 03:43] Extract page data from URL. URL: https://huggingface.co/papers/2601.17058
[27.01.2026 03:43] Extract page data from URL. URL: https://huggingface.co/papers/2601.17761
[27.01.2026 03:43] Extract page data from URL. URL: https://huggingface.co/papers/2601.17111
[27.01.2026 03:43] Extract page data from URL. URL: https://huggingface.co/papers/2601.17027
[27.01.2026 03:43] Extract page data from URL. URL: https://huggingface.co/papers/2601.18081
[27.01.2026 03:43] Extract page data from URL. URL: https://huggingface.co/papers/2601.16207
[27.01.2026 03:43] Updating GitHub stars.
[27.01.2026 03:43] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[27.01.2026 03:43] Downloading and parsing papers (pdf, html). Total: 8.
[27.01.2026 03:43] Downloading and parsing paper https://huggingface.co/papers/2601.17737.
[27.01.2026 03:43] Downloading paper 2601.17737 from https://arxiv.org/pdf/2601.17737v1...
[27.01.2026 03:43] Extracting affiliations from text.
[27.01.2026 03:43] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 5 2 ] . [ 1 7 3 7 7 1 . 1 0 6 2 : r An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation Chenyu Mu ,1,2 , Xin He,1 , Qu Yang,1 , Wanshun Chen1 , Jiadi Yao1 , Huang Liu1 , Zihao Yi1 , Bo Zhao1 , Xingyu Chen1 , Ruotian Ma1 , Fanghua Ye1 , Erkun Yang2 , Cheng Deng2 , Zhaopeng Tu ,1 , Xiaolong Li1 , and Linus1 1Tencent Hunyuan Multimodal Department 2Xidian University https://github.com/Tencent/digitalhuman/tree/main/ScriptAgent Figure 1: Our proposed pipeline consists of three key components: (1) ScripterAgent, trained with GRPO to align its outputs with professional directorial standards; (2) DirectorAgent, which ensures seamless visual continuity across scenes, thereby overcoming the temporal incoherence caused by the fixed-duration constraints of SOTA video generation models; and (3) CriticAgent, which evaluates the generated film from both technical and cinematic perspectives. "
[27.01.2026 03:43] Failed to download and parse paper https://huggingface.co/papers/2601.17737: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 03:43] Downloading and parsing paper https://huggingface.co/papers/2601.17367.
[27.01.2026 03:43] Downloading paper 2601.17367 from https://arxiv.org/pdf/2601.17367v1...
[27.01.2026 03:43] Extracting affiliations from text.
[27.01.2026 03:43] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers Zecheng Tang * 1 2 Quantong Qiu * 1 2 Yi Yang 1 2 Zhiyi Hong 1 2 Haiya Xiang 1 2 Kebin Liu 3 Qingqing Dang 3 Juntao Li 1 2 Min Zhang 1 6 2 0 2 4 2 ] . [ 1 7 6 3 7 1 . 1 0 6 2 : r Abstract The quadratic complexity of standard attention mechanisms poses significant scalability bottleneck for large language models (LLMs) in longcontext scenarios. While hybrid attention strategies that combine sparse and full attention within single model offer viable solution, they typically employ static computation ratios (i.e., fixed proportions of sparse versus full attention) and fail to adapt to the varying sparsity sensitivities of downstream tasks during inference. To address this issue, we propose Elastic Attention, which allows the model to dynamically adjust its overall sparsity based on the input. This is achieved by integrating lightweight Attention Router into the existing pretrained model, which dynamically assigns each attention head to different computation modes. Within only 12 hours of training on 8A800 GPUs, our method enables models to achieve both strong performance and efficient inference (see Figure 1). Experiments across three long-context benchmarks on widely-used LLMs demonstrate the superiority of our method. 1. Introduction Large language models (LLMs) have demonstrated remarkable capabilities in processing long-context sequences (Liu et al., 2025a;b; Mei et al., 2025). However, the quadratic computational and memory complexity of standard full attention (FA) mechanisms (Vaswani et al., 2017) poses significant scalability bottleneck as the context window continues to expand. Sparse attention (SA) mechanisms (Child, 2019; Zaheer et al., 2020) represent an effective strategy for mitigating this limitation by selectively attending to subset of critical tokens, thereby substantially reducing computational overhead and improving inference throughput. To balance the trade-off between"
[27.01.2026 03:43] Failed to download and parse paper https://huggingface.co/papers/2601.17367: HTTPSConnectionPool(host='ngw.devices.sberbank.ru', port=9443): Max retries exceeded with url: /api/v2/oauth (Caused by ConnectTimeoutError(<HTTPSConnection(host='ngw.devices.sberbank.ru', port=9443) at 0x7f6ed9b26f50>, 'Connection to ngw.devices.sberbank.ru timed out. (connect timeout=30)'))
[27.01.2026 03:43] Downloading and parsing paper https://huggingface.co/papers/2601.17058.
[27.01.2026 03:43] Downloading paper 2601.17058 from https://arxiv.org/pdf/2601.17058v1...
[27.01.2026 03:43] Extracting affiliations from text.
[27.01.2026 03:43] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, JANUARY 2025 1 Can LLMs Clean Up Your Mess? Survey of Application-Ready Data Preparation with LLMs Wei Zhou, Jun Zhou, Haoyu Wang, Zhenghao Li, Qikang He, Shaokun Han, Guoliang Li Fellow, IEEE, Xuanhe Zhou, Yeye He, Chunwei Liu, Zirui Tang, Bin Wang, Shen Tang, Kai Zuo, Yuyu Luo, Zhenzhe Zheng, Conghui He, Jingren Zhou Fellow, IEEE, Fan Wu Awesome-Data-LLM: https:// github.com/ weAIDB/ awesome-data-llm 6 2 0 2 J 2 2 ] . [ 1 8 5 0 7 1 . 1 0 6 2 : r AbstractData preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming transformative and potentially dominant paradigm for data preparation. By investigating hundreds of recent literature works, this paper presents systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in ad"
[27.01.2026 03:44] Failed to download and parse paper https://huggingface.co/papers/2601.17058: HTTPSConnectionPool(host='ngw.devices.sberbank.ru', port=9443): Max retries exceeded with url: /api/v2/oauth (Caused by ConnectTimeoutError(<HTTPSConnection(host='ngw.devices.sberbank.ru', port=9443) at 0x7f6ed9673e50>, 'Connection to ngw.devices.sberbank.ru timed out. (connect timeout=30)'))
[27.01.2026 03:44] Downloading and parsing paper https://huggingface.co/papers/2601.17761.
[27.01.2026 03:44] Downloading paper 2601.17761 from https://arxiv.org/pdf/2601.17761v1...
[27.01.2026 03:44] Extracting affiliations from text.
[27.01.2026 03:44] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 5 2 ] . [ 1 1 6 7 7 1 . 1 0 6 2 : r 2026-1-22 AR-Omni: Unified Autoregressive Model for Any-to-Any Generation Dongjie Cheng1* Ruifeng Yuan1* Yongqi Li1 Runyang You1 Wenjie Wang2 Liqiang Nie3 Lei Zhang1 Wenjie Li 1 The Hong Kong Polytechnic University 2 University of Science and Technology of China 3 Harbin Institute of Technology (Shenzhen) {dong-jie.cheng,ruifeng.yuan}@connect.polyu.hk, liyongqi0@gmail.com Abstract Real-world perception and interaction are inherently multimodal, encompassing not only language but also vision and speech, which motivates the development of Omni MLLMs that support both multimodal inputs and multimodal outputs. While sequence of omni MLLMs has emerged, most existing systems still rely on additional expert components to achieve multimodal generation, limiting the simplicity of unified training and inference. Autoregressive (AR) modeling, with single token stream, single next-token objective, and single decoder, is an elegant and scalable foundation in the text domain. Motivated by this, we present AR-Omni, unified any-to-any model in the autoregressive paradigm without any expert decoders. AR-Omni supports autoregressive text and image generation, as well as streaming speech generation, all under single Transformer decoder. We further address three practical issues in unified AR modeling: modality imbalance via task-aware loss reweighting, visual fidelity via lightweight token-level perceptual alignment loss for image tokens, and stabilitycreativity trade-offs via finite-state decoding mechanism. Empirically, AR-Omni achieves strong quality across three modalities while remaining real-time, achieving 0.88 real-time factor for speech generation. Project Page: https://modalitydance.github.io/AR-Omni called Omni [2]. Large Language Models (LLMs) have achieved strong performance in understanding and generating natural language [1]. However, their interface is largely limited to text. In contrast, real-world perception and interactio"
[27.01.2026 03:45] Failed to download and parse paper https://huggingface.co/papers/2601.17761: HTTPSConnectionPool(host='ngw.devices.sberbank.ru', port=9443): Max retries exceeded with url: /api/v2/oauth (Caused by ConnectTimeoutError(<HTTPSConnection(host='ngw.devices.sberbank.ru', port=9443) at 0x7f6ed9b669d0>, 'Connection to ngw.devices.sberbank.ru timed out. (connect timeout=30)'))
[27.01.2026 03:45] Downloading and parsing paper https://huggingface.co/papers/2601.17111.
[27.01.2026 03:45] Downloading paper 2601.17111 from https://arxiv.org/pdf/2601.17111v1...
[27.01.2026 03:45] Extracting affiliations from text.
[27.01.2026 03:45] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 3 2 ] . [ 1 1 1 1 7 1 . 1 0 6 2 : r Preprint - 2026 Salesforce AI Research LEAST-LOADED EXPERT PARALLELISM: LOAD BALANCING AN IMBALANCED MIXTURE-OF-EXPERTS Xuan-Phi Nguyen, Shrey Pandit, Austin Xu, Caiming Xiong, Shafiq Joty Salesforce AI Research xnguyen@salesforce.com "
[27.01.2026 03:45] Failed to download and parse paper https://huggingface.co/papers/2601.17111: HTTPSConnectionPool(host='ngw.devices.sberbank.ru', port=9443): Max retries exceeded with url: /api/v2/oauth (Caused by ConnectTimeoutError(<HTTPSConnection(host='ngw.devices.sberbank.ru', port=9443) at 0x7f6ed9a459d0>, 'Connection to ngw.devices.sberbank.ru timed out. (connect timeout=30)'))
[27.01.2026 03:45] Downloading and parsing paper https://huggingface.co/papers/2601.17027.
[27.01.2026 03:45] Downloading paper 2601.17027 from https://arxiv.org/pdf/2601.17027v1...
[27.01.2026 03:45] Extracting affiliations from text.
[27.01.2026 03:45] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 7 1 ] . [ 1 7 2 0 7 1 . 1 0 6 2 : r Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility Honglin Lin1,2, Chonghan Qin3,2, Zheng Liu4,2, Qizhi Pei2, Yu Li2, Zhanping Zhong1,2, Xin Gao1,2, Yanfeng Wang1, Conghui He2, Lijun Wu2 1Shanghai Jiao Tong University, 2OpenDataLab, Shanghai Artificial Intelligence Laboratory, 3The University of Hong Kong, 4Peking University While synthetic data has proven effective for improving scientific reasoning in the text domain, multimodal reasoning remains constrained by the difficulty of synthesizing scientifically rigorous images. Existing Text-to-Image (T2I) models often produce outputs that are visually plausible yet scientifically incorrect, resulting in persistent visuallogic divergence that limits their value for downstream reasoning. Motivated by recent advances in next-generation T2I models, we conduct systematic study of scientific image synthesis across generation paradigms, evaluation, and downstream use. We analyze both direct pixel-based generation and programmatic synthesis, and propose ImgCoder, logic-driven framework that follows an explicit understand plan code workflow to improve structural precision. To rigorously assess scientific correctness, we introduce SciGenBench, which evaluates generated images based on information utility and logical validity. Our evaluation reveals systematic failure modes in pixel-based models and highlights fundamental expressivenessprecision trade-off. Finally, we show that fine-tuning Large Multimodal Models (LMMs) on rigorously verified synthetic scientific images yields consistent reasoning gains, with potential scaling trends analogous to the text domain, validating high-fidelity scientific synthesis as viable path to unlocking massive multimodal reasoning capabilities. Date: January 27, 2026 Equal contribution: Honglin Lin, Chonghan Qin, Zheng Liu Correspondence: Lijun Wu, wulijun@pjlab.org.cn Project Page: https://SciGenbench.github.io With the ad"
[27.01.2026 03:45] Failed to download and parse paper https://huggingface.co/papers/2601.17027: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 03:45] Downloading and parsing paper https://huggingface.co/papers/2601.18081.
[27.01.2026 03:45] Downloading paper 2601.18081 from https://arxiv.org/pdf/2601.18081v1...
[27.01.2026 03:46] Extracting affiliations from text.
[27.01.2026 03:46] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 6 2 ] . [ 1 1 8 0 8 1 . 1 0 6 2 : r DRPG (Decompose, Retrieve, Plan, Generate): An Agentic Framework for Academic Rebuttal Peixuan Han, Yingjie Yu, Jingjun Xu, Jiaxuan You University of Illinois Urbana-Champaign {ph16,yyu69,jingjunx,jiaxuan}@illinois.edu "
[27.01.2026 03:46] Failed to download and parse paper https://huggingface.co/papers/2601.18081: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 03:46] Downloading and parsing paper https://huggingface.co/papers/2601.16207.
[27.01.2026 03:46] Downloading paper 2601.16207 from https://arxiv.org/pdf/2601.16207v1...
[27.01.2026 03:46] Extracting affiliations from text.
[27.01.2026 03:46] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"IVRA: Improving Visual-Token Relations for Robot Action Policy with Training-Free Hint-Based Guidance Jongwoo Park1, Kanchana Ranasinghe1, Jinhyeok Jang2, Cristina Mata1, Yoo Sung Jang1, Michael Ryoo1 1Stony Brook University 2ETRI jongwopark@cs.stonybrook.edu 6 2 0 J 2 2 ] . [ 1 7 0 2 6 1 . 1 0 6 2 : r Abstract Many Vision-Language-Action (VLA) models flatten image patches into 1D token sequence, weakening the 2D spatial cues needed for precise manipulation. We introduce IVRA, lightweight, training-free method that improves spatial understanding by exploiting affinity hints already available in the models built-in vision encoder, without requiring any external encoder or retraining. IVRA selectively injects layer in which these affinity signals into language-model instance-level features reside. This inference-time intervention realigns visual-token interactions and better preserves geometric structure while keeping all model parameters fixed. We demonstrate the generality of IVRA by applying it to diverse VLA architectures (LLaRA, OpenVLA, and FLOWER) across simulated benchmarks spanning both 2D and 3D manipulation (VIMA and LIBERO) and on various real-robot tasks. On 2D VIMA, IVRA improves average success by +4.2% over the baseline LLaRA in low-data regime. On 3D LIBERO, it yields consistent gains over the OpenVLA and FLOWER baselines, including improvements when baseline accuracy is near saturation (96.3% 97.1%). All code and models will be released publicly. Visualizations are available at: jongwoopark7978.github.io/IVRA I. INTRODUCTION Vision-Language-Action (VLA) models have rapidly emerged as promising approach for generating robot actions from images and natural-language instructions. Recent systems such as LLaRA [1], OpenVLA [2], FLOWER [3], and LLARVA [4] pair large-scale pretrained vision encoders (e.g., CLIP or DINO [5], [6]) with language models by flattening the 2D patch grid and appending the resulting visual tokens to the text sequence in single Tran"
[27.01.2026 03:46] Failed to download and parse paper https://huggingface.co/papers/2601.16207: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 03:46] Enriching papers with extra data.
[27.01.2026 03:46] ********************************************************************************
[27.01.2026 03:46] Abstract 0. A novel end-to-end agentic framework translates dialogue into cinematic videos through specialized agents that generate and orchestrate video content while maintaining narrative coherence.  					AI-generated summary 				 Recent advances in video generation have produced models capable of synthesizin...
[27.01.2026 03:46] ********************************************************************************
[27.01.2026 03:46] Abstract 1. Elastic Attention enables dynamic adjustment of attention sparsity during inference by integrating a lightweight Attention Router into pretrained models, achieving efficient long-context processing.  					AI-generated summary 				 The quadratic complexity of standard attention mechanisms poses a sig...
[27.01.2026 03:46] ********************************************************************************
[27.01.2026 03:46] Abstract 2. LLM-enhanced data preparation methods are transforming data-centric workflows from rule-based pipelines to prompt-driven, context-aware approaches, organized into data cleaning, integration, and enrichment tasks.  					AI-generated summary 				 Data preparation aims to denoise raw datasets, uncover ...
[27.01.2026 03:46] ********************************************************************************
[27.01.2026 03:46] Abstract 3. AR-Omni is a unified autoregressive model that supports multimodal input and output generation through a single Transformer decoder, addressing modality balance, visual fidelity, and stability-creativity trade-offs.  					AI-generated summary 				 Real-world perception and interaction are inherently...
[27.01.2026 03:46] ********************************************************************************
[27.01.2026 03:46] Abstract 4. Imbalanced expert routing in Mixture-of-Experts models leads to computational inefficiencies in expert parallelism, which are addressed by a dynamic rerouting algorithm that balances workload and reduces memory usage.  					AI-generated summary 				 Mixture-of-Experts (MoE) models are typically pre-...
[27.01.2026 03:46] ********************************************************************************
[27.01.2026 03:46] Abstract 5. Scientific image synthesis using logic-driven frameworks like ImgCoder improves multimodal reasoning by addressing visual-logic divergence through structured generation and evaluation benchmarks.  					AI-generated summary 				 While synthetic data has proven effective for improving scientific reaso...
[27.01.2026 03:46] ********************************************************************************
[27.01.2026 03:46] Abstract 6. An agentic framework for automatic academic rebuttal generation that decomposes reviews, retrieves evidence, plans rebuttal strategies, and generates persuasive responses with human-level performance using an 8B model.  					AI-generated summary 				 Despite the growing adoption of large language mo...
[27.01.2026 03:46] ********************************************************************************
[27.01.2026 03:46] Abstract 7. IVRA enhances spatial understanding in vision-language-action models by injecting affinity signals into language-model layers without retraining or external encoders.  					AI-generated summary 				 Many Vision-Language-Action (VLA) models flatten image patches into a 1D token sequence, weakening th...
[27.01.2026 03:46] Read previous papers.
[27.01.2026 03:46] Generating reviews via LLM API.
[27.01.2026 03:46] Querying the API.
[27.01.2026 03:46] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A novel end-to-end agentic framework translates dialogue into cinematic videos through specialized agents that generate and orchestrate video content while maintaining narrative coherence.  					AI-generated summary 				 Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap'' between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking.
[27.01.2026 03:46] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 03:46] Querying the API.
[27.01.2026 03:46] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Elastic Attention enables dynamic adjustment of attention sparsity during inference by integrating a lightweight Attention Router into pretrained models, achieving efficient long-context processing.  					AI-generated summary 				 The quadratic complexity of standard attention mechanisms poses a significant scalability bottleneck for large language models (LLMs) in long-context scenarios. While hybrid attention strategies that combine sparse and full attention within a single model offer a viable solution, they typically employ static computation ratios (i.e., fixed proportions of sparse versus full attention) and fail to adapt to the varying sparsity sensitivities of downstream tasks during inference. To address this issue, we propose Elastic Attention, which allows the model to dynamically adjust its overall sparsity based on the input. This is achieved by integrating a lightweight Attention Router into the existing pretrained model, which dynamically assigns each attention head to different computation modes. Within only 12 hours of training on 8xA800 GPUs, our method enables models to achieve both strong performance and efficient inference. Experiments across three long-context benchmarks on widely-used LLMs demonstrate the superiority of our method.
[27.01.2026 03:46] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 03:46] Querying the API.
[27.01.2026 03:46] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

LLM-enhanced data preparation methods are transforming data-centric workflows from rule-based pipelines to prompt-driven, context-aware approaches, organized into data cleaning, integration, and enrichment tasks.  					AI-generated summary 				 Data preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for a wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming a transformative and potentially dominant paradigm for data preparation.   By investigating hundreds of recent literature works, this paper presents a systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce a task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in advanced agents, the mismatch between advanced methods and weak evaluation). Moreover, we analyze commonly used datasets and evaluation metrics (the empirical part). Finally, we discuss open research challenges and outline a forward-looking roadmap that emphasizes scalable LLM-data systems, principled designs for reliable agentic workflows, and robust evaluation protocols.
[27.01.2026 03:46] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 03:46] Querying the API.
[27.01.2026 03:46] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

AR-Omni is a unified autoregressive model that supports multimodal input and output generation through a single Transformer decoder, addressing modality balance, visual fidelity, and stability-creativity trade-offs.  					AI-generated summary 				 Real-world perception and interaction are inherently multimodal, encompassing not only language but also vision and speech, which motivates the development of "Omni" MLLMs that support both multimodal inputs and multimodal outputs. While a sequence of omni MLLMs has emerged, most existing systems still rely on additional expert components to achieve multimodal generation, limiting the simplicity of unified training and inference. Autoregressive (AR) modeling, with a single token stream, a single next-token objective, and a single decoder, is an elegant and scalable foundation in the text domain. Motivated by this, we present AR-Omni, a unified any-to-any model in the autoregressive paradigm without any expert decoders. AR-Omni supports autoregressive text and image generation, as well as streaming speech generation, all under a single Transformer decoder. We further address three practical issues in unified AR modeling: modality imbalance via task-aware loss reweighting, visual fidelity via a lightweight token-level perceptual alignment loss for image tokens, and stability-creativity trade-offs via a finite-state decoding mechanism. Empirically, AR-Omni achieves strong quality across three modalities while remaining real-time, achieving a 0.88 real-time factor for speech generation.
[27.01.2026 03:46] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 03:46] Querying the API.
[27.01.2026 03:46] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Imbalanced expert routing in Mixture-of-Experts models leads to computational inefficiencies in expert parallelism, which are addressed by a dynamic rerouting algorithm that balances workload and reduces memory usage.  					AI-generated summary 				 Mixture-of-Experts (MoE) models are typically pre-trained with explicit load-balancing constraints to ensure statistically balanced expert routing. Despite this, we observe that even well-trained MoE models exhibit significantly imbalanced routing. This behavior is arguably natural-and even desirable - as imbalanced routing allows models to concentrate domain-specific knowledge within a subset of experts. Expert parallelism (EP) is designed to scale MoE models by distributing experts across multiple devices, but with a less-discussed assumption of balanced routing. Under extreme imbalance, EP can funnel a disproportionate number of tokens to a small number of experts, leading to compute- and memory-bound failures on overloaded devices during post-training or inference, where explicit load balancing is often inapplicable. We propose Least-Loaded Expert Parallelism (LLEP), a novel EP algorithm that dynamically reroutes excess tokens and associated expert parameters from overloaded devices to underutilized ones. This ensures that all devices complete their workloads within the minimum collective latency while respecting memory constraints. Across different model scales, LLEP achieves up to 5x speedup and 4x reduction in peak memory usage compared to standard EP. This enables faster and higher-throughput post-training and inference, with ~1.9x faster for gpt-oss-120b. We support our method with extensive theoretical analysis and comprehensive empirical evaluations, including ablation studies. These results illuminate key trade-offs and enable a principled framework for hardware-specific hyper-parameter tuning to achieve optimal performance.
[27.01.2026 03:46] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 03:46] Querying the API.
[27.01.2026 03:46] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Scientific image synthesis using logic-driven frameworks like ImgCoder improves multimodal reasoning by addressing visual-logic divergence through structured generation and evaluation benchmarks.  					AI-generated summary 				 While synthetic data has proven effective for improving scientific reasoning in the text domain, multimodal reasoning remains constrained by the difficulty of synthesizing scientifically rigorous images. Existing Text-to-Image (T2I) models often produce outputs that are visually plausible yet scientifically incorrect, resulting in a persistent visual-logic divergence that limits their value for downstream reasoning. Motivated by recent advances in next-generation T2I models, we conduct a systematic study of scientific image synthesis across generation paradigms, evaluation, and downstream use. We analyze both direct pixel-based generation and programmatic synthesis, and propose ImgCoder, a logic-driven framework that follows an explicit "understand - plan - code" workflow to improve structural precision. To rigorously assess scientific correctness, we introduce SciGenBench, which evaluates generated images based on information utility and logical validity. Our evaluation reveals systematic failure modes in pixel-based models and highlights a fundamental expressiveness-precision trade-off. Finally, we show that fine-tuning Large Multimodal Models (LMMs) on rigorously verified synthetic scientific images yields consistent reasoning gains, with potential scaling trends analogous to the text domain, validating high-fidelity scientific synthesis as a viable path to unlocking massive multimodal reasoning capabilities.
[27.01.2026 03:46] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 03:46] Querying the API.
[27.01.2026 03:46] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

An agentic framework for automatic academic rebuttal generation that decomposes reviews, retrieves evidence, plans rebuttal strategies, and generates persuasive responses with human-level performance using an 8B model.  					AI-generated summary 				 Despite the growing adoption of large language models (LLMs) in scientific research workflows, automated support for academic rebuttal, a crucial step in academic communication and peer review, remains largely underexplored. Existing approaches typically rely on off-the-shelf LLMs or simple pipelines, which struggle with long-context understanding and often fail to produce targeted and persuasive responses. In this paper, we propose DRPG, an agentic framework for automatic academic rebuttal generation that operates through four steps: Decompose reviews into atomic concerns, Retrieve relevant evidence from the paper, Plan rebuttal strategies, and Generate responses accordingly. Notably, the Planner in DRPG reaches over 98% accuracy in identifying the most feasible rebuttal direction. Experiments on data from top-tier conferences demonstrate that DRPG significantly outperforms existing rebuttal pipelines and achieves performance beyond the average human level using only an 8B model. Our analysis further demonstrates the effectiveness of the planner design and its value in providing multi-perspective and explainable suggestions. We also showed that DRPG works well in a more complex multi-round setting. These results highlight the effectiveness of DRPG and its potential to provide high-quality rebuttal content and support the scaling of academic discussions. Codes for this work are available at https://github.com/ulab-uiuc/DRPG-RebuttalAgent.
[27.01.2026 03:46] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 03:46] Querying the API.
[27.01.2026 03:46] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

IVRA enhances spatial understanding in vision-language-action models by injecting affinity signals into language-model layers without retraining or external encoders.  					AI-generated summary 				 Many Vision-Language-Action (VLA) models flatten image patches into a 1D token sequence, weakening the 2D spatial cues needed for precise manipulation. We introduce IVRA, a lightweight, training-free method that improves spatial understanding by exploiting affinity hints already available in the model's built-in vision encoder, without requiring any external encoder or retraining. IVRA selectively injects these affinity signals into a language-model layer in which instance-level features reside. This inference-time intervention realigns visual-token interactions and better preserves geometric structure while keeping all model parameters fixed. We demonstrate the generality of IVRA by applying it to diverse VLA architectures (LLaRA, OpenVLA, and FLOWER) across simulated benchmarks spanning both 2D and 3D manipulation (VIMA and LIBERO) and on various real-robot tasks. On 2D VIMA, IVRA improves average success by +4.2% over the baseline LLaRA in a low-data regime. On 3D LIBERO, it yields consistent gains over the OpenVLA and FLOWER baselines, including improvements when baseline accuracy is near saturation (96.3% to 97.1%). All code and models will be released publicly. Visualizations are available at: jongwoopark7978.github.io/IVRA
[27.01.2026 03:46] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[27.01.2026 03:46] Renaming data file.
[27.01.2026 03:46] Renaming previous data. hf_papers.json to ./d/2026-01-27.json
[27.01.2026 03:46] Saving new data file.
[27.01.2026 03:46] Generating page.
[27.01.2026 03:46] Renaming previous page.
[27.01.2026 03:46] Renaming previous data. index.html to ./d/2026-01-27.html
[27.01.2026 03:46] Writing result.
[27.01.2026 03:46] Renaming log file.
[27.01.2026 03:46] Renaming previous data. log.txt to ./logs/2026-01-27_last_log.txt
