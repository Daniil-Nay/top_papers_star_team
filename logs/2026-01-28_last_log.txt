[28.01.2026 04:42] Read previous papers.
[28.01.2026 04:42] Generating top page (month).
[28.01.2026 04:42] Writing top page (month).
[28.01.2026 05:23] Read previous papers.
[28.01.2026 05:23] Get feed.
[28.01.2026 05:23] Extract page data from URL. URL: https://huggingface.co/papers/2601.18491
[28.01.2026 05:23] Extract page data from URL. URL: https://huggingface.co/papers/2601.18631
[28.01.2026 05:23] Extract page data from URL. URL: https://huggingface.co/papers/2601.17645
[28.01.2026 05:23] Extract page data from URL. URL: https://huggingface.co/papers/2601.19834
[28.01.2026 05:23] Extract page data from URL. URL: https://huggingface.co/papers/2601.19375
[28.01.2026 05:23] Extract page data from URL. URL: https://huggingface.co/papers/2601.19362
[28.01.2026 05:23] Extract page data from URL. URL: https://huggingface.co/papers/2601.19149
[28.01.2026 05:23] Updating GitHub stars.
[28.01.2026 05:23] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[28.01.2026 05:23] No deleted papers detected.
[28.01.2026 05:23] Downloading and parsing papers (pdf, html). Total: 7.
[28.01.2026 05:23] Downloading and parsing paper https://huggingface.co/papers/2601.18491.
[28.01.2026 05:23] Downloading paper 2601.18491 from https://arxiv.org/pdf/2601.18491v1...
[28.01.2026 05:23] Extracting affiliations from text.
[28.01.2026 05:23] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"AgentDoG: Diagnostic Guardrail Framework for AI Agent Safety and Security Shanghai Artificial Intelligence Laboratory https://github.com/AI45Lab/AgentDoG https://huggingface.co/collections/AI45Research/agentdog "
[28.01.2026 05:23] Failed to download and parse paper https://huggingface.co/papers/2601.18491: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[28.01.2026 05:23] Downloading and parsing paper https://huggingface.co/papers/2601.18631.
[28.01.2026 05:23] Downloading paper 2601.18631 from https://arxiv.org/pdf/2601.18631v1...
[28.01.2026 05:24] Extracting affiliations from text.
[28.01.2026 05:24] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 6 2 ] . [ 1 1 3 6 8 1 . 1 0 6 2 : r DAREASONER: DYNAMIC TOOL ORCHESTRA- Mingyang Song ,1, Haoyu Sun,2, Jiawei Gu,3, Linjie Li,4, Luxin Xu5, Ranjay Krishna4,, Yu Cheng5, 1Fudan University, 2Tongji University, 3National University of Singapore, 4University of Washington, 5University of Electronic Science and Technology of China, 6The Chinese University of Hong Kong Homepage: https://adareasoner.github.io Code: https://github.com/ssmisya/AdaReasoner Models and Data: https://huggingface.co/AdaReasoner "
[28.01.2026 05:24] Failed to download and parse paper https://huggingface.co/papers/2601.18631: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[28.01.2026 05:24] Downloading and parsing paper https://huggingface.co/papers/2601.17645.
[28.01.2026 05:24] Downloading paper 2601.17645 from https://arxiv.org/pdf/2601.17645v1...
[28.01.2026 05:24] Failed to download and parse paper https://huggingface.co/papers/2601.17645: 'LTChar' object is not iterable
[28.01.2026 05:24] Downloading and parsing paper https://huggingface.co/papers/2601.19834.
[28.01.2026 05:24] Downloading paper 2601.19834 from https://arxiv.org/pdf/2601.19834v1...
[28.01.2026 05:24] Extracting affiliations from text.
[28.01.2026 05:24] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 7 2 ] . [ 1 4 3 8 9 1 . 1 0 6 2 : r Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models Jialong Wu1,2,, Xiaoying Zhang2,, Hongyi Yuan2, Xiangcheng Zhang1,2,, Tianhao Huang1, Changjing He1, Chaoyi Deng1,2,, Renrui Zhang2, Youbin Wu2, Mingsheng Long1, 1Tsinghua University, 2ByteDance Seed Work done at ByteDance Seed, Corresponding authors "
[28.01.2026 05:24] Failed to download and parse paper https://huggingface.co/papers/2601.19834: HTTPSConnectionPool(host='ngw.devices.sberbank.ru', port=9443): Max retries exceeded with url: /api/v2/oauth (Caused by ConnectTimeoutError(<HTTPSConnection(host='ngw.devices.sberbank.ru', port=9443) at 0x7fb8996d1d10>, 'Connection to ngw.devices.sberbank.ru timed out. (connect timeout=30)'))
[28.01.2026 05:24] Downloading and parsing paper https://huggingface.co/papers/2601.19375.
[28.01.2026 05:24] Downloading paper 2601.19375 from https://arxiv.org/pdf/2601.19375v1...
[28.01.2026 05:24] Extracting affiliations from text.
[28.01.2026 05:24] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Selective Steering: Norm-Preserving Control Through Discriminative Layer Selection Quy-Anh Dang1,2, Chris Ngo2 1VNU University of Science, Vietnam 2Knovel Engineering Lab, Singapore {quyanh.dang, chris.ngo}@knoveleng.com Project: https://knoveleng.github.io/steering/ 6 2 0 2 7 ] . [ 1 5 7 3 9 1 . 1 0 6 2 : r a "
[28.01.2026 05:25] Failed to download and parse paper https://huggingface.co/papers/2601.19375: HTTPSConnectionPool(host='ngw.devices.sberbank.ru', port=9443): Max retries exceeded with url: /api/v2/oauth (Caused by ConnectTimeoutError(<HTTPSConnection(host='ngw.devices.sberbank.ru', port=9443) at 0x7fb8992ee9d0>, 'Connection to ngw.devices.sberbank.ru timed out. (connect timeout=30)'))
[28.01.2026 05:25] Downloading and parsing paper https://huggingface.co/papers/2601.19362.
[28.01.2026 05:25] Downloading paper 2601.19362 from https://arxiv.org/pdf/2601.19362v1...
[28.01.2026 05:25] Extracting affiliations from text.
[28.01.2026 05:25] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 7 2 ] . [ 1 2 6 3 9 1 . 1 0 6 2 : r Published as conference paper at ICLR 2026 REVISITING PARAMETER SERVER IN LLM POSTTRAINING Xinyi Wan1,2, Penghui Qi1,2, Guangxing Huang1, Chaoyi Ruan2, Min Lin1 & Jialin Li2 1Sea AI Lab 2National University of Singapore "
[28.01.2026 05:25] Failed to download and parse paper https://huggingface.co/papers/2601.19362: HTTPSConnectionPool(host='ngw.devices.sberbank.ru', port=9443): Max retries exceeded with url: /api/v2/oauth (Caused by ConnectTimeoutError(<HTTPSConnection(host='ngw.devices.sberbank.ru', port=9443) at 0x7fb8994a0fd0>, 'Connection to ngw.devices.sberbank.ru timed out. (connect timeout=30)'))
[28.01.2026 05:25] Downloading and parsing paper https://huggingface.co/papers/2601.19149.
[28.01.2026 05:25] Downloading paper 2601.19149 from https://arxiv.org/pdf/2601.19149v1...
[28.01.2026 05:25] Extracting affiliations from text.
[28.01.2026 05:25] Gigachat request. Model: GigaChat. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 7 2 ] . [ 1 9 4 1 9 1 . 1 0 6 2 : r GPCR-Filter: deep learning framework for efÔ¨Åcient and precise GPCR modulator discovery Jingjie Ning1 , Xiangzhen Shen2 , Li Hou3 , Shiyi Shen3 , Jiahao Yang4 , Junrui Li3 , Hong Shan3 , Sanan Wu5 , Sihan Gao6 , Huaqiang Eric Xu3 , Xinheng He3 1 School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA 2 College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan 410082, China 3 The State Key Laboratory of Drug Research, Shanghai Institute of Materia Medica, Chinese Academy of Sciences, Shanghai, China 4 Lingang Laboratory, Shanghai, China 5 Research Center for Medicinal Structural Biology, National Research Center for Translational Medicine at Shanghai, State Key Laboratory of Medical Genomics, Ruijin Hospital, Shanghai Jiao Tong University School of Medicine, Shanghai, China 6 School of Pharmacy, Fudan University, Shanghai 201203, China "
[28.01.2026 05:26] Failed to download and parse paper https://huggingface.co/papers/2601.19149: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[28.01.2026 05:26] Enriching papers with extra data.
[28.01.2026 05:26] ********************************************************************************
[28.01.2026 05:26] Abstract 0. AI agents face safety and security challenges from autonomous tool use and environmental interactions, requiring advanced guardrail frameworks for risk diagnosis and transparent monitoring.  					AI-generated summary 				 The rise of AI agents introduces complex safety and security challenges arisin...
[28.01.2026 05:26] ********************************************************************************
[28.01.2026 05:26] Abstract 1. AdaReasoner enables multimodal models to learn tool usage as a general reasoning skill through scalable data curation, reinforcement learning for tool selection, and adaptive learning mechanisms that improve performance on complex visual reasoning tasks.  					AI-generated summary 				 When humans f...
[28.01.2026 05:26] ********************************************************************************
[28.01.2026 05:26] Abstract 2. Current multimodal models demonstrate limited understanding of cultural and contextual audio-visual content, particularly excelling only in surface-level analysis rather than deeper semantic comprehension.  					AI-generated summary 				 Internet audio-visual clips convey meaning through time-varyin...
[28.01.2026 05:26] ********************************************************************************
[28.01.2026 05:26] Abstract 3. Visual generation enhances reasoning capabilities in multimodal models by providing more natural world models for physical and spatial tasks, while verbal reasoning remains sufficient for abstract domains.  					AI-generated summary 				 Humans construct internal world models and reason by manipulat...
[28.01.2026 05:26] ********************************************************************************
[28.01.2026 05:26] Abstract 4. Selective Steering enables continuous, norm-preserving control of language model behavior through targeted layer selection and mathematically rigorous rotation techniques.  					AI-generated summary 				 Despite significant progress in alignment, large language models (LLMs) remain vulnerable to adv...
[28.01.2026 05:26] ********************************************************************************
[28.01.2026 05:26] Abstract 5. On-Demand Communication (ODC) adapts parameter server principles to Fully Sharded Data Parallel training by replacing collective communication with point-to-point communication, improving device utilization and throughput in imbalanced large language model training scenarios.  					AI-generated summ...
[28.01.2026 05:26] ********************************************************************************
[28.01.2026 05:26] Abstract 6. GPCR-Filter is a deep learning framework that combines protein language models and graph neural networks to identify GPCR modulators with high accuracy and generalization across unseen receptors and ligands.  					AI-generated summary 				 G protein-coupled receptors (GPCRs) govern diverse physiolog...
[28.01.2026 05:26] Read previous papers.
[28.01.2026 05:26] Generating reviews via LLM API.
[28.01.2026 05:26] Querying the API.
[28.01.2026 05:26] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

AI agents face safety and security challenges from autonomous tool use and environmental interactions, requiring advanced guardrail frameworks for risk diagnosis and transparent monitoring.  					AI-generated summary 				 The rise of AI agents introduces complex safety and security challenges arising from autonomous tool use and environmental interactions. Current guardrail models lack agentic risk awareness and transparency in risk diagnosis. To introduce an agentic guardrail that covers complex and numerous risky behaviors, we first propose a unified three-dimensional taxonomy that orthogonally categorizes agentic risks by their source (where), failure mode (how), and consequence (what). Guided by this structured and hierarchical taxonomy, we introduce a new fine-grained agentic safety benchmark (ATBench) and a Diagnostic Guardrail framework for agent safety and security (AgentDoG). AgentDoG provides fine-grained and contextual monitoring across agent trajectories. More Crucially, AgentDoG can diagnose the root causes of unsafe actions and seemingly safe but unreasonable actions, offering provenance and transparency beyond binary labels to facilitate effective agent alignment. AgentDoG variants are available in three sizes (4B, 7B, and 8B parameters) across Qwen and Llama model families. Extensive experimental results demonstrate that AgentDoG achieves state-of-the-art performance in agentic safety moderation in diverse and complex interactive scenarios. All models and datasets are openly released.
[28.01.2026 05:26] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[28.01.2026 05:26] Querying the API.
[28.01.2026 05:26] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

AdaReasoner enables multimodal models to learn tool usage as a general reasoning skill through scalable data curation, reinforcement learning for tool selection, and adaptive learning mechanisms that improve performance on complex visual reasoning tasks.  					AI-generated summary 				 When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce AdaReasoner, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw.
[28.01.2026 05:26] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[28.01.2026 05:26] Querying the API.
[28.01.2026 05:26] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Current multimodal models demonstrate limited understanding of cultural and contextual audio-visual content, particularly excelling only in surface-level analysis rather than deeper semantic comprehension.  					AI-generated summary 				 Internet audio-visual clips convey meaning through time-varying sound and motion, which extend beyond what text alone can represent. To examine whether AI models can understand such signals in human cultural contexts, we introduce AVMeme Exam, a human-curated benchmark of over one thousand iconic Internet sounds and videos spanning speech, songs, music, and sound effects. Each meme is paired with a unique Q&A assessing levels of understanding from surface content to context and emotion to usage and world knowledge, along with metadata such as original year, transcript, summary, and sensitivity. We systematically evaluate state-of-the-art multimodal large language models (MLLMs) alongside human participants using this benchmark. Our results reveal a consistent limitation: current models perform poorly on textless music and sound effects, and struggle to think in context and in culture compared to surface content. These findings highlight a key gap in human-aligned multimodal intelligence and call for models that can perceive contextually and culturally beyond the surface of what they hear and see. Project page: avmemeexam.github.io/public
[28.01.2026 05:26] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[28.01.2026 05:26] Querying the API.
[28.01.2026 05:26] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Visual generation enhances reasoning capabilities in multimodal models by providing more natural world models for physical and spatial tasks, while verbal reasoning remains sufficient for abstract domains.  					AI-generated summary 				 Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI.
[28.01.2026 05:26] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[28.01.2026 05:26] Querying the API.
[28.01.2026 05:26] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Selective Steering enables continuous, norm-preserving control of language model behavior through targeted layer selection and mathematically rigorous rotation techniques.  					AI-generated summary 				 Despite significant progress in alignment, large language models (LLMs) remain vulnerable to adversarial attacks that elicit harmful behaviors. Activation steering techniques offer a promising inference-time intervention approach, but existing methods suffer from critical limitations: activation addition requires careful coefficient tuning and is sensitive to layer-specific norm variations, while directional ablation provides only binary control. Recent work on Angular Steering introduces continuous control via rotation in a 2D subspace, but its practical implementation violates norm preservation, causing distribution shift and generation collapse, particularly in models below 7B parameters. We propose Selective Steering, which addresses these limitations through two key innovations: (1) a mathematically rigorous norm-preserving rotation formulation that maintains activation distribution integrity, and (2) discriminative layer selection that applies steering only where feature representations exhibit opposite-signed class alignment. Experiments across nine models demonstrate that Selective Steering achieves 5.5x higher attack success rates than prior methods while maintaining zero perplexity violations and approximately 100\% capability retention on standard benchmarks. Our approach provides a principled, efficient framework for controllable and stable LLM behavior modification. Code: https://github.com/knoveleng/steering
[28.01.2026 05:26] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[28.01.2026 05:26] Querying the API.
[28.01.2026 05:26] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

On-Demand Communication (ODC) adapts parameter server principles to Fully Sharded Data Parallel training by replacing collective communication with point-to-point communication, improving device utilization and throughput in imbalanced large language model training scenarios.  					AI-generated summary 				 Modern data parallel (DP) training favors collective communication over parameter servers (PS) for its simplicity and efficiency under balanced workloads. However, the balanced workload assumption no longer holds in large language model (LLM) post-training due to the high variance in sequence lengths. Under imbalanced workloads, collective communication creates synchronization barriers, leading to under-utilization of devices with smaller workloads. This change in training dynamics calls for a revisit of the PS paradigm for its robustness to such imbalance. We propose On-Demand Communication (ODC), which adapts PS into Fully Sharded Data Parallel (FSDP) by replacing collective all-gather and reduce-scatter with direct point-to-point communication. Compared to FSDP, ODC reduces the synchronization barrier from once per layer to once per minibatch and decouples the workload on each device so that faster workers are not stalled. It also enables simpler and more effective load balancing at the minibatch level. Across diverse LLM post-training tasks, ODC consistently improves device utilization and training throughput, achieving up to a 36\% speedup over standard FSDP. These results demonstrate that ODC is a superior fit for the prevalent imbalanced workloads in LLM post-training. Our implementation of ODC and integration with FSDP is open-sourced at https://github.com/sail-sg/odc.
[28.01.2026 05:26] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[28.01.2026 05:26] Querying the API.
[28.01.2026 05:26] Gigachat request. Model: GigaChat. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

GPCR-Filter is a deep learning framework that combines protein language models and graph neural networks to identify GPCR modulators with high accuracy and generalization across unseen receptors and ligands.  					AI-generated summary 				 G protein-coupled receptors (GPCRs) govern diverse physiological processes and are central to modern pharmacology. Yet discovering GPCR modulators remains challenging because receptor activation often arises from complex allosteric effects rather than direct binding affinity, and conventional assays are slow, costly, and not optimized for capturing these dynamics. Here we present GPCR-Filter, a deep learning framework specifically developed for GPCR modulator discovery. We assembled a high-quality dataset of over 90,000 experimentally validated GPCR-ligand pairs, providing a robust foundation for training and evaluation. GPCR-Filter integrates the ESM-3 protein language model for high-fidelity GPCR sequence representations with graph neural networks that encode ligand structures, coupled through an attention-based fusion mechanism that learns receptor-ligand functional relationships. Across multiple evaluation settings, GPCR-Filter consistently outperforms state-of-the-art compound-protein interaction models and exhibits strong generalization to unseen receptors and ligands. Notably, the model successfully identified micromolar-level agonists of the 5-HT1A receptor with distinct chemical frameworks. These results establish GPCR-Filter as a scalable and effective computational approach for GPCR modulator discovery, advancing AI-assisted drug development for complex signaling systems.
[28.01.2026 05:26] Error getting data: 401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth
[28.01.2026 05:26] Renaming data file.
[28.01.2026 05:26] Renaming previous data. hf_papers.json to ./d/2026-01-28.json
[28.01.2026 05:26] Saving new data file.
[28.01.2026 05:26] Generating page.
[28.01.2026 05:26] Renaming previous page.
[28.01.2026 05:26] Renaming previous data. index.html to ./d/2026-01-28.html
[28.01.2026 05:26] Writing result.
[28.01.2026 05:26] Renaming log file.
[28.01.2026 05:26] Renaming previous data. log.txt to ./logs/2026-01-28_last_log.txt
