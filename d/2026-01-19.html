
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 22 papers. January 19.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
            align-items: center;
        }
        .github-link {
            display: inline-flex;
            align-items: center;
            gap: 6px;
            padding: 4px 8px;
            border-radius: 12px;
            background: #f5f5f5;
            font-size: 0.95em;
        }
        .dark-theme .github-link {
            background: #555;
        }
        .github-stars {
            font-weight: 600;
            color: var(--text-color);
        }
        .dark-theme .github-stars {
            color: #fff;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["–º–∏–Ω—É—Ç—É", "–º–∏–Ω—É—Ç—ã", "–º–∏–Ω—É—Ç"],
                hour: ["—á–∞—Å", "—á–∞—Å–∞", "—á–∞—Å–æ–≤"],
                day: ["–¥–µ–Ω—å", "–¥–Ω—è", "–¥–Ω–µ–π"],
                justNow: "—Ç–æ–ª—å–∫–æ —á—Ç–æ",
                ago: "–Ω–∞–∑–∞–¥"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["ÂàÜÈíü", "ÂàÜÈíü", "ÂàÜÈíü"],
                hour: ["Â∞èÊó∂", "Â∞èÊó∂", "Â∞èÊó∂"],
                day: ["Â§©", "Â§©", "Â§©"],
                justNow: "ÂàöÂàö",
                ago: "Ââç"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            return `${number} ${unitWord} ${timeUnits[lang].ago}`;
        }

        if (!['ru', 'en'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "—Å—Ç–∞—Ç–µ–π";
            } else if (lastDigit === 1) {
                word = "—Å—Ç–∞—Ç—å—è";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "—Å—Ç–∞—Ç—å–∏";
            } else {
                word = "—Å—Ç–∞—Ç–µ–π";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ÁØáËÆ∫Êñá"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">üî∫</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">19 —è–Ω–≤–∞—Ä—è</span> | <span id="title-articles-count">22 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2026-01-16.html">‚¨ÖÔ∏è <span id="prev-date">16.01</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2026-01-20.html">‚û°Ô∏è <span id="next-date">20.01</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2026-01.html">üìà <span id='top-month-label'>–ú–µ—Å—è—Ü</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">üîÄ <span id="sort-label-text">–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">—Ä–µ–π—Ç–∏–Ω–≥—É</option>
                    <option value="github">GitHub stars</option>
                    <option value="pub_date">–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏</option>
                    <option value="issue_id">–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">üè∑Ô∏è –§–∏–ª—å—Ç—Ä</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A‚à™B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A‚à©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">üßπ</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ‚úñÔ∏è <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '19 —è–Ω–≤–∞—Ä—è', 'en': 'January 19', 'zh': '1Êúà19Êó•'};
        let feedDateNext = {'ru': '20.01', 'en': '01/20', 'zh': '1Êúà20Êó•'};
        let feedDatePrev = {'ru': '16.01', 'en': '01/16', 'zh': '1Êúà16Êó•'};
        let filterLabel = {'ru': '–§–∏–ª—å—Ç—Ä', 'en': 'Topics', 'zh': '‰∏ªÈ¢òÁ≠õÈÄâ'}
        let publishedLabel = {'ru': '—Å—Ç–∞—Ç—å—è –æ—Ç ', 'en': 'published on ', 'zh': 'ÂèëË°®‰∫é'}
        let sortLabel = {'ru': '–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ', 'en': 'Sort by', 'zh': 'ÊéíÂ∫èÊñπÂºè'}
        let paperLabel = {'ru': '–°—Ç–∞—Ç—å—è', 'en': 'Paper', 'zh': 'ËÆ∫Êñá'}
        let topMonthLabel = {'ru': '–ú–µ—Å—è—Ü', 'en': 'Month', 'zh': 'ÊúàÂ∫¶ËÆ∫Êñá'}
        let topDayLabel = {'ru': '–î–µ–Ω—å', 'en': 'Day', 'zh': 'Êó•Â∫¶ËÆ∫Êñá'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2601.08521', 'title': 'Your Group-Relative Advantage Is Biased', 'url': 'https://huggingface.co/papers/2601.08521', 'abstract': 'Group-based reinforcement learning from verifier rewards suffers from biased advantage estimation that underestimates hard prompts and overestimates easy prompts, which is addressed through a history-aware adaptive difficulty weighting method.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning from Verifier Rewards (RLVR) has emerged as a widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood.   In this work, we uncover a fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide the first theoretical analysis showing that it systematically underestimates advantages for hard prompts and overestimates them for easy prompts, leading to imbalanced exploration and exploitation. To address this issue, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on an evolving difficulty anchor and training dynamics. Both theoretical analysis and experiments on five mathematical reasoning benchmarks demonstrate that HA-DW consistently improves performance when integrated into GRPO and its variants. Our results suggest that correcting biased advantage estimation is critical for robust and efficient RLVR training.', 'score': 94, 'issue_id': 552, 'pub_date': '2026-01-13', 'pub_date_card': {'ru': '13 —è–Ω–≤–∞—Ä—è', 'en': 'January 13', 'zh': '1Êúà13Êó•'}, 'hash': 'caf215355050af49', 'github_url': '', 'github_stars': 0, 'authors': ['Fengkai Yang', 'Zherui Chen', 'Xiaohan Wang', 'Xiaodong Lu', 'Jiajun Chai', 'Guojun Yin', 'Wei Lin', 'Shuai Ma', 'Fuzhen Zhuang', 'Deqing Wang', 'Yaodong Yang', 'Jianxin Li', 'Yikun Ban'], 'affiliations': ['Beihang University', 'Meituan', 'Peking University', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2601.08521.jpg', 'data': {'categories': ['#rl', '#optimization', '#reasoning', '#reinforcement_learning'], 'emoji': '‚öôÔ∏è', 'ru': {'title': '–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–º–µ—â–µ–Ω–∏–π –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤ –≤ –æ–±—É—á–µ–Ω–∏–∏ –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏—é —Å –ø–æ–º–æ—â—å—é –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏', 'desc': '–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ —Å–º–µ—â–µ–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –≤ –º–µ—Ç–æ–¥–∞—Ö –≥—Ä—É–ø–ø–æ–≤–æ–≥–æ –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π. –ü–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ —Ç–∞–∫–∞—è –æ—Ü–µ–Ω–∫–∞ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–µ–¥–æ–æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ –∏ –ø–µ—Ä–µ–æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø—Ä–æ—Å—Ç—ã–µ, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –ø—Ä–æ—Ü–µ—Å—Å—É –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏ —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏. –î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è (HA-DW), –∫–æ—Ç–æ—Ä—ã–π –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç —Å–º–µ—â–µ–Ω–∏—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤ –∏ —É–ª—É—á—à–∞–µ—Ç –æ–±—â—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏—é.'}, 'en': {'title': 'History-Aware Adaptive Difficulty Weighting for Group-Based Reinforcement Learning', 'desc': 'This paper addresses the bias in advantage estimation in group-based reinforcement learning from verifier rewards by introducing History-Aware Adaptive Difficulty Weighting (HA-DW). The method adjusts advantage estimates dynamically using an evolving difficulty anchor and training dynamics, improving performance across various mathematical reasoning benchmarks.'}}}, {'id': 'https://huggingface.co/papers/2601.11496', 'title': 'The Poisoned Apple Effect: Strategic Manipulation of Mediated Markets via Technology Expansion of AI Agents', 'url': 'https://huggingface.co/papers/2601.11496', 'abstract': 'The integration of AI agents into economic markets fundamentally alters the landscape of strategic interaction. We investigate the economic implications of expanding the set of available technologies in three canonical game-theoretic settings: bargaining (resource division), negotiation (asymmetric information trade), and persuasion (strategic information transmission). We find that simply increasing the choice of AI delegates can drastically shift equilibrium payoffs and regulatory outcomes, often creating incentives for regulators to proactively develop and release technologies. Conversely, we identify a strategic phenomenon termed the "Poisoned Apple" effect: an agent may release a new technology, which neither they nor their opponent ultimately uses, solely to manipulate the regulator\'s choice of market design in their favor. This strategic release improves the releaser\'s welfare at the expense of their opponent and the regulator\'s fairness objectives. Our findings demonstrate that static regulatory frameworks are vulnerable to manipulation via technology expansion, necessitating dynamic market designs that adapt to the evolving landscape of AI capabilities.', 'score': 38, 'issue_id': 552, 'pub_date': '2026-01-16', 'pub_date_card': {'ru': '16 —è–Ω–≤–∞—Ä—è', 'en': 'January 16', 'zh': '1Êúà16Êó•'}, 'hash': '111af399807fc2ee', 'github_url': '', 'github_stars': 0, 'authors': ['Eilam Shapira', 'Roi Reichart', 'Moshe Tennenholtz'], 'affiliations': ['Technion Israel Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2601.11496.jpg', 'data': {'categories': ['#ethics', '#games'], 'emoji': 'ü§ñ', 'ru': {'title': '–í–ª–∏—è–Ω–∏–µ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ —ç–∫–æ–Ω–æ–º–∏–∫—É –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ', 'desc': '–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–ª–∏—è–Ω–∏–µ –≤–Ω–µ–¥—Ä–µ–Ω–∏—è –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –≤ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–µ —Ä—ã–Ω–∫–∏ –Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤. –†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è —Ç—Ä–∏ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –∏–≥—Ä–æ–≤—ã–µ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ —Å–∏—Ç—É–∞—Ü–∏–∏: –≤–µ–¥–µ–Ω–∏–µ –ø–µ—Ä–µ–≥–æ–≤–æ—Ä–æ–≤ –ø–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—é —Ä–µ—Å—É—Ä—Å–æ–≤, –ø–µ—Ä–µ–≥–æ–≤–æ—Ä—ã –ø—Ä–∏ –∞—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –ø–µ—Ä–µ–¥–∞—á–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ, —á—Ç–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –Ω–∞–±–æ—Ä–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –º–µ–Ω—è–µ—Ç —Ä–∞–≤–Ω–æ–≤–µ—Å–Ω—ã–µ –≤—ã–ø–ª–∞—Ç—ã –∏ —Ä–µ–≥—É–ª—è—Ç–∏–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —Å–æ–∑–¥–∞–≤–∞—è —Å—Ç–∏–º—É–ª—ã –¥–ª—è —Ä–µ–≥—É–ª—è—Ç–æ—Ä–æ–≤ —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏ –≤—ã–ø—É—Å–∫–∞—Ç—å –Ω–æ–≤—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏. –¢–∞–∫–∂–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω —Ñ–µ–Ω–æ–º–µ–Ω ¬´—è–¥–æ–≤–∏—Ç–æ–≥–æ —è–±–ª–æ–∫–∞¬ª, –∫–æ–≥–¥–∞ –∞–≥–µ–Ω—Ç –≤—ã–ø—É—Å–∫–∞–µ—Ç –Ω–æ–≤—É—é —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—é –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ —Å —Ü–µ–ª—å—é –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ –≤—ã–±–æ—Ä–æ–º —Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ä—ã–Ω–∫–∞ –≤ —Å–≤–æ—é –ø–æ–ª—å–∑—É, —É—Ö—É–¥—à–∞—è –ø–æ–ª–æ–∂–µ–Ω–∏–µ —Å–≤–æ–µ–≥–æ –æ–ø–ø–æ–Ω–µ–Ω—Ç–∞ –∏ —Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω—ã—Ö —Ü–µ–ª–µ–π —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç–∏. –ê–≤—Ç–æ—Ä—ã –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Ä—ã–Ω–æ—á–Ω—ã—Ö –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–π, —Å–ø–æ—Å–æ–±–Ω—ã—Ö –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ —Ä–∞—Å—Ç—É—â–∏–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞.'}, 'en': {'title': 'Article', 'desc': ''}}}, {'id': 'https://huggingface.co/papers/2601.10355', 'title': 'Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text', 'url': 'https://huggingface.co/papers/2601.10355', 'abstract': 'A text-based data synthesis approach generates multi-turn tool-use trajectories for large language models, achieving improved performance and reduced computational costs through a specialized trajectory synthesizer.  \t\t\t\t\tAI-generated summary \t\t\t\t Enabling Large Language Models (LLMs) to effectively utilize tools in multi-turn interactions is essential for building capable autonomous agents. However, acquiring diverse and realistic multi-turn tool-use data remains a significant challenge. In this work, we propose a novel text-based paradigm. We observe that textual corpora naturally contain rich, multi-step problem-solving experiences, which can serve as an untapped, scalable, and authentic data source for multi-turn tool-use tasks. Based on this insight, we introduce GEM, a data synthesis pipeline that enables the generation and extraction of multi-turn tool-use trajectories from text corpora through a four-stage process: relevance filtering, workflow & tool extraction, trajectory grounding, and complexity refinement. To reduce the computational cost, we further train a specialized Trajectory Synthesizer via supervised fine-tuning. This model distills the complex generation pipeline into an efficient, end-to-end trajectory generator. Experiments demonstrate that our GEM-32B achieve a 16.5% improvement on the BFCL V3 Multi-turn benchmark. Our models partially surpass the performance of models trained on œÑ - bench (Airline and Retail) in-domain data, highlighting the superior generalization capability derived from our text-based synthesis paradigm. Notably, our Trajectory Synthesizer matches the quality of the full pipeline while significantly reducing inference latency and costs.', 'score': 30, 'issue_id': 552, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 —è–Ω–≤–∞—Ä—è', 'en': 'January 15', 'zh': '1Êúà15Êó•'}, 'hash': '561cb5995d29e4d4', 'github_url': '', 'github_stars': 0, 'authors': ['Zhihao Xu', 'Rumei Li', 'Jiahuan Li', 'Rongxiang Weng', 'Jingang Wang', 'Xunliang Cai', 'Xiting Wang'], 'affiliations': ['Meituan, China', 'Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2601.10355.jpg', 'data': {'categories': ['#agent_memory', '#memory', '#context_window', '#tool_use', '#multiturn', '#tools', '#dataset', '#training'], 'emoji': 'üìö', 'ru': {'title': '–°–∏–Ω—Ç–µ–∑ –º–Ω–æ–≥–æ—Çurn–æ–≤—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö', 'desc': '–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –ø–æ–¥—Ö–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –º–Ω–æ–≥–æ—Çurn–æ–≤—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–≤—ã—Å–∏—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ —Å–Ω–∏–∑–∏—Ç—å –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –∑–∞ —Å—á–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞—Ç–æ—Ä–∞ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π. –ü–æ—Å—Ä–µ–¥—Å—Ç–≤–æ–º —á–µ—Ç—ã—Ä–µ—Ö—ç—Ç–∞–ø–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏, –≤—ã–¥–µ–ª–µ–Ω–∏—è —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, –ø—Ä–∏–≤—è–∑–∫–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –∫ —Ä–µ–∞–ª—å–Ω—ã–º –¥–∞–Ω–Ω—ã–º –∏ —É—Ç–æ—á–Ω–µ–Ω–∏—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏, –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è —É–ª—É—á—à–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –º–æ–¥–µ–ª—å GEM-32B –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ 16.5%, –∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞—Ç–æ—Ä–∞ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º—É—é –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –ø—Ä–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–º —Å–æ–∫—Ä–∞—â–µ–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –∑–∞—Ç—Ä–∞—Ç.'}, 'en': {'title': 'Text-Based Data Synthesis for Efficient Tool-Use Trajectories in LLM Multi-Turn Interactions', 'desc': 'This paper introduces GEM, a text-based data synthesis pipeline designed to generate multi-turn tool-use trajectories for large language models. It leverages textual corpora to extract and refine multi-step problem-solving experiences, enabling efficient and scalable training. A specialized Trajectory Synthesizer reduces computational costs by distilling the complex generation pipeline into an end-to-end model, resulting in improved performance and lower inference latency compared to traditional methods.'}}}, {'id': 'https://huggingface.co/papers/2601.08430', 'title': 'RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation', 'url': 'https://huggingface.co/papers/2601.08430', 'abstract': 'RLVR has advanced reasoning capabilities but struggles with open-ended generation due to lack of ground truth; this work proposes an automated rubric generation framework and dataset to improve performance in health reasoning benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has driven substantial progress in reasoning-intensive domains like mathematics. However, optimizing open-ended generation remains challenging due to the lack of ground truth. While rubric-based evaluation offers a structured proxy for verification, existing methods suffer from scalability bottlenecks and coarse criteria, resulting in a supervision ceiling effect. To address this, we propose an automated Coarse-to-Fine Rubric Generation framework. By synergizing principle-guided synthesis, multi-model aggregation, and difficulty evolution, our approach produces comprehensive and highly discriminative criteria capable of capturing the subtle nuances. Based on this framework, we introduce RubricHub, a large-scale (sim110k) and multi-domain dataset. We validate its utility through a two-stage post-training pipeline comprising Rubric-based Rejection Sampling Fine-Tuning (RuFT) and Reinforcement Learning (RuRL). Experimental results demonstrate that RubricHub unlocks significant performance gains: our post-trained Qwen3-14B achieves state-of-the-art (SOTA) results on HealthBench (69.3), surpassing proprietary frontier models such as GPT-5. The code and data will be released soon.', 'score': 25, 'issue_id': 552, 'pub_date': '2026-01-13', 'pub_date_card': {'ru': '13 —è–Ω–≤–∞—Ä—è', 'en': 'January 13', 'zh': '1Êúà13Êó•'}, 'hash': '9d3d0f8e4f3e3b69', 'github_url': 'https://github.com/teqkilla/RubricHub', 'github_stars': 24, 'authors': ['Sunzhu Li', 'Jiale Zhao', 'Miteto Wei', 'Huimin Ren', 'Yang Zhou', 'Jingwen Yang', 'Shunyu Liu', 'Kaike Zhang', 'Wei Chen'], 'affiliations': ['Li Auto Inc., China', 'Nanyang Technological University', 'The Chinese University of Hong Kong, Shenzhen, China', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2601.08430.jpg', 'data': {'categories': ['#healthcare', '#benchmark', '#transfer_learning', '#retrieval', '#reasoning', '#dataset'], 'emoji': 'üìö', 'ru': {'title': '–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∏—è –æ—Ü–µ–Ω–æ—á–Ω—ã—Ö —à–∫–∞–ª –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö', 'desc': '–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ –∑–∞–¥–∞—á–∞—Ö —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∫–æ–Ω—Ü–æ–º —É –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ RLVR. –ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ä—É–±—Ä–∏–∫ –∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö RubricHub –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö. –ò—Å–ø–æ–ª—å–∑—É—è –ø–æ–¥—Ö–æ–¥ –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å–∏–Ω—Ç–µ–∑–∞ –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤, –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –∏ —ç–≤–æ–ª—é—Ü–∏–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –º–µ—Ç–æ–¥–∏–∫–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞—é—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ—à–µ–Ω–∏–π.'}, 'en': {'title': 'Automated Rubric Generation Framework for Improved Open-Ended Reasoning', 'desc': 'This paper addresses the limitations of Reinforcement Learning with Verifiable Rewards (RLVR) in open-ended generation by proposing an automated rubric generation framework and dataset. It introduces a Coarse-to-Fine Rubric Generation method combining principle-guided synthesis, multi-model aggregation, and difficulty evolution. This framework is applied to create RubricHub, a large-scale dataset used for fine-tuning and reinforcement learning. The experimental results show significant improvements in performance on HealthBench, achieving state-of-the-art results.'}}}, {'id': 'https://huggingface.co/papers/2601.11000', 'title': 'When Personalization Misleads: Understanding and Mitigating Hallucinations in Personalized LLMs', 'url': 'https://huggingface.co/papers/2601.11000', 'abstract': "Personalized large language models can generate false information aligned with user history instead of factual truth, but a new method called FPPS helps maintain both factual accuracy and personalized responses while preserving existing personalization effects.  \t\t\t\t\tAI-generated summary \t\t\t\t Personalized large language models (LLMs) adapt model behavior to individual users to enhance user satisfaction, yet personalization can inadvertently distort factual reasoning. We show that when personalized LLMs face factual queries, there exists a phenomenon where the model generates answers aligned with a user's prior history rather than the objective truth, resulting in personalization-induced hallucinations that degrade factual reliability and may propagate incorrect beliefs, due to representational entanglement between personalization and factual representations. To address this issue, we propose Factuality-Preserving Personalized Steering (FPPS), a lightweight inference-time approach that mitigates personalization-induced factual distortions while preserving personalized behavior. We further introduce PFQABench, the first benchmark designed to jointly evaluate factual and personalized question answering under personalization. Experiments across multiple LLM backbones and personalization methods show that FPPS substantially improves factual accuracy while maintaining personalized performance.", 'score': 21, 'issue_id': 552, 'pub_date': '2026-01-16', 'pub_date_card': {'ru': '16 —è–Ω–≤–∞—Ä—è', 'en': 'January 16', 'zh': '1Êúà16Êó•'}, 'hash': '891d8c6f80792efc', 'github_url': '', 'github_stars': 0, 'authors': ['Zhongxiang Sun', 'Yi Zhan', 'Chenglei Shen', 'Weijie Yu', 'Xiao Zhang', 'Ming He', 'Jun Xu'], 'affiliations': ['AI Lab at Lenovo Research', 'Gaolol School of Artificial Intelligence, Renmin University of China', 'School of Artificial Intelligence and Data Science, University of International Business and Economics'], 'pdf_title_img': 'assets/pdf/title_img/2601.11000.jpg', 'data': {'categories': ['#hallucinations', '#benchmark', '#personalization', '#inference', '#data'], 'emoji': 'üìö', 'ru': {'title': 'FPPS: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –§–∞–∫—Ç–∏—á–µ—Å–∫–æ–π –¢–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –ë–æ–ª—å—à–∏–º–∏ –Ø–∑—ã–∫–æ–≤—ã–º–∏ –ú–æ–¥–µ–ª—è–º–∏', 'desc': '–î–∞–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∏—Å–∫–∞–∂–µ–Ω–∏—è —Ñ–∞–∫—Ç–æ–≤ –≤ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM), –∫–æ—Ç–æ—Ä—ã–µ –∞–¥–∞–ø—Ç–∏—Ä—É—é—Ç—Å—è –ø–æ–¥ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, —á—Ç–æ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–æ–¥–∏—Ç—å –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ª–æ–∂–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π –∏—Å—Ç–æ—Ä–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –∞ –Ω–µ —Ñ–∞–∫—Ç–∞–º. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ FPPS –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –∫–∞–∫ —Ç–æ—á–Ω–æ—Å—Ç—å —Ñ–∞–∫—Ç–æ–≤, —Ç–∞–∫ –∏ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏ –±–µ–∑ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–µ—Ä—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏. –î–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –±—ã–ª —Å–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö PFQABench, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –æ—Ü–µ–Ω–∫—É —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ñ–∞–∫—Ç–æ–≤ –∏ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –æ—Ç–≤–µ—Ç–æ–≤.'}, 'en': {'title': 'Factuality-Preserving Personalized Steering for Large Language Models', 'desc': 'This paper addresses the challenge of maintaining factual accuracy in personalized large language models (LLMs). It introduces FPPS, a technique for factuality-preserving personalized steering that ensures factual correctness without compromising personalization effects. The study also presents PFQABench, a benchmark specifically designed to assess factual and personalized question answering. Experimental results demonstrate significant improvements in factual accuracy while retaining personalized performance.'}}}, {'id': 'https://huggingface.co/papers/2601.11404', 'title': 'ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models', 'url': 'https://huggingface.co/papers/2601.11404', 'abstract': 'Vision-Language-Action models are enhanced by incorporating action-space reasoning through a structured sequence of coarse action intents, improving manipulation task performance in both simulation and real-world environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models have emerged as essential generalist robot policies for diverse manipulation tasks, conventionally relying on directly translating multimodal inputs into actions via Vision-Language Model (VLM) embeddings. Recent advancements have introduced explicit intermediary reasoning, such as sub-task prediction (language) or goal image synthesis (vision), to guide action generation. However, these intermediate reasoning are often indirect and inherently limited in their capacity to convey the full, granular information required for precise action execution. Instead, we posit that the most effective form of reasoning is one that deliberates directly in the action space. We introduce Action Chain-of-Thought (ACoT), a paradigm where the reasoning process itself is formulated as a structured sequence of coarse action intents that guide the final policy. In this paper, we propose ACoT-VLA, a novel architecture that materializes the ACoT paradigm. Specifically, we introduce two complementary components: an Explicit Action Reasoner (EAR) and Implicit Action Reasoner (IAR). The former proposes coarse reference trajectories as explicit action-level reasoning steps, while the latter extracts latent action priors from internal representations of multimodal input, co-forming an ACoT that conditions the downstream action head to enable grounded policy learning. Extensive experiments in real-world and simulation environments demonstrate the superiority of our proposed method, which achieves 98.5%, 84.1%, and 47.4% on LIBERO, LIBERO-Plus and VLABench, respectively.', 'score': 18, 'issue_id': 552, 'pub_date': '2026-01-16', 'pub_date_card': {'ru': '16 —è–Ω–≤–∞—Ä—è', 'en': 'January 16', 'zh': '1Êúà16Êó•'}, 'hash': 'feebd83e55594c09', 'github_url': '', 'github_stars': 0, 'authors': ['Linqing Zhong', 'Yi Liu', 'Yifei Wei', 'Ziyu Xiong', 'Maoqing Yao', 'Si Liu', 'Guanghui Ren'], 'affiliations': ['AgiBot', 'Beihang University'], 'pdf_title_img': 'assets/pdf/title_img/2601.11404.jpg', 'data': {'categories': ['#action', '#multimodal', '#reasoning', '#language', '#action_space_reasoning', '#vision'], 'emoji': 'ü§ñ', 'ru': {'title': '–ü–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π VLA –∑–∞ —Å—á–µ—Ç —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –¥–µ–π—Å—Ç–≤–∏–π', 'desc': '–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —É–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å Vision-Language-Action (VLA), –∫–æ—Ç–æ—Ä–∞—è –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –¥–µ–π—Å—Ç–≤–∏–π —á–µ—Ä–µ–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≥—Ä—É–±—ã—Ö –Ω–∞–º–µ—Ä–µ–Ω–∏–π –¥–µ–π—Å—Ç–≤–∏–π. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–≤—ã—Å–∏—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ –∫–∞–∫ –≤ —Å–∏–º—É–ª—è—Ü–∏–æ–Ω–Ω—ã—Ö, —Ç–∞–∫ –∏ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å—Ä–µ–¥–∞—Ö. –í–≤–µ–¥–µ–Ω—ã –¥–≤–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞: –Ø–≤–Ω—ã–π –†–∞–∑—É–º –î–µ–π—Å—Ç–≤–∏—è (Explicit Action Reasoner ‚Äî EAR) –¥–ª—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏—è —è–≤–Ω—ã—Ö —à–∞–≥–æ–≤ –¥–µ–π—Å—Ç–≤–∏–π –∏ –ù–µ—è–≤–Ω—ã–π –†–∞–∑—É–º –î–µ–π—Å—Ç–≤–∏—è (Implicit Action Reasoner ‚Äî IAR) –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å–∫—Ä—ã—Ç—ã—Ö –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π –¥–µ–π—Å—Ç–≤–∏—è –∏–∑ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –≤—Ö–æ–¥–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.'}, 'en': {'title': '', 'desc': ''}}}, {'id': 'https://huggingface.co/papers/2601.11037', 'title': 'BAPO: Boundary-Aware Policy Optimization for Reliable Agentic Search', 'url': 'https://huggingface.co/papers/2601.11037', 'abstract': "Reinforcement learning framework for agentic search that improves reliability by teaching agents to recognize reasoning limits and respond appropriately when evidence is insufficient.  \t\t\t\t\tAI-generated summary \t\t\t\t RL-based agentic search enables LLMs to solve complex questions via dynamic planning and external search. While this approach significantly enhances accuracy with agent policies optimized via large-scale reinforcement learning, we identify a critical gap in reliability: these agents fail to recognize their reasoning boundaries and rarely admit ``I DON'T KNOW'' (IDK) even when evidence is insufficient or reasoning reaches its limit. The lack of reliability often leads to plausible but unreliable answers, introducing significant risks in many real-world scenarios. To this end, we propose Boundary-Aware Policy Optimization (BAPO), a novel RL framework designed to cultivate reliable boundary awareness without compromising accuracy. BAPO introduces two key components: (i) a group-based boundary-aware reward that encourages an IDK response only when the reasoning reaches its limit, and (ii) an adaptive reward modulator that strategically suspends this reward during early exploration, preventing the model from exploiting IDK as a shortcut. Extensive experiments on four benchmarks demonstrate that BAPO substantially enhances the overall reliability of agentic search.", 'score': 12, 'issue_id': 552, 'pub_date': '2026-01-16', 'pub_date_card': {'ru': '16 —è–Ω–≤–∞—Ä—è', 'en': 'January 16', 'zh': '1Êúà16Êó•'}, 'hash': '872fa7ac4160c51e', 'github_url': 'https://github.com/Liushiyu-0709/BAPO-Reliable-Search', 'github_stars': 16, 'authors': ['Shiyu Liu', 'Yongjing Yin', 'Jianhao Yan', 'Yunbo Tang', 'Qinggang Zhang', 'Bei Li', 'Xin Chen', 'Jingang Wang', 'Xunliang Cai', 'Jinsong Su'], 'affiliations': ['Meituan Inc.', 'The Hong Kong Polytechnic University', 'Westlake University', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2601.11037.jpg', 'data': {'categories': ['#reliability', '#rl', '#reasoning', '#boundary_aware_policy_optimization', '#agi'], 'emoji': 'ü§î', 'ru': {'title': '–ü–æ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç—Å–∫–∏—Ö —Å–∏—Å—Ç–µ–º —á–µ—Ä–µ–∑ –æ—Å–æ–∑–Ω–∞–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π', 'desc': '–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –ø–æ–¥—Ö–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏—è –¥–ª—è –∞–≥–µ–Ω—Ç—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –∑–∞ —Å—á–µ—Ç –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—é –≥—Ä–∞–Ω–∏—Ü —Å–≤–æ–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –∞–¥–µ–∫–≤–∞—Ç–Ω–æ–º—É —Ä–µ–∞–≥–∏—Ä–æ–≤–∞–Ω–∏—é –ø—Ä–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –Ω–∞–∑–≤–∞–Ω–Ω–∞—è Boundary-Aware Policy Optimization (BAPO), –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –º–µ—Ö–∞–Ω–∏–∑–º –ø–æ–æ—â—Ä–µ–Ω–∏–π, —Å—Ç–∏–º—É–ª–∏—Ä—É—é—â–∏–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ ¬´–Ø –ù–ï –ó–ù–ê–Æ¬ª —Ç–æ–ª—å–∫–æ —Ç–æ–≥–¥–∞, –∫–æ–≥–¥–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –¥–æ—Å—Ç–∏–≥–∞—é—Ç –ø—Ä–µ–¥–µ–ª–∞ —Å–≤–æ–∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π, –∞ —Ç–∞–∫–∂–µ –º–æ–¥—É–ª—è—Ç–æ—Ä –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞—é—â–∏–π –∑–ª–æ—É–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ —ç—Ç–∏–º –æ—Ç–≤–µ—Ç–æ–º –Ω–∞ —Ä–∞–Ω–Ω–∏—Ö —ç—Ç–∞–ø–∞—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è.'}, 'en': {'title': 'Improving Reliability in Agentic Search Through Boundary-Aware Policy Optimization', 'desc': 'A Reinforcement Learning framework called Boundary-Aware Policy Optimization (BAPO) is introduced to enhance the reliability of AI agents performing agentic search tasks. It teaches agents to recognize their reasoning limits and respond appropriately when evidence is insufficient, reducing the risk of providing unreliable answers. This framework includes a group-based boundary-aware reward mechanism and an adaptive reward modulator to prevent exploitation of the IDK response as a shortcut.'}}}, {'id': 'https://huggingface.co/papers/2601.10909', 'title': 'FrankenMotion: Part-level Human Motion Generation and Composition', 'url': 'https://huggingface.co/papers/2601.10909', 'abstract': 'A diffusion-based framework generates human motion from text prompts with fine-grained part-level control using a newly constructed dataset with atomic, temporally-aware annotations.  \t\t\t\t\tAI-generated summary \t\t\t\t Human motion generation from text prompts has made remarkable progress in recent years. However, existing methods primarily rely on either sequence-level or action-level descriptions due to the absence of fine-grained, part-level motion annotations. This limits their controllability over individual body parts. In this work, we construct a high-quality motion dataset with atomic, temporally-aware part-level text annotations, leveraging the reasoning capabilities of large language models (LLMs). Unlike prior datasets that either provide synchronized part captions with fixed time segments or rely solely on global sequence labels, our dataset captures asynchronous and semantically distinct part movements at fine temporal resolution. Based on this dataset, we introduce a diffusion-based part-aware motion generation framework, namely FrankenMotion, where each body part is guided by its own temporally-structured textual prompt. This is, to our knowledge, the first work to provide atomic, temporally-aware part-level motion annotations and have a model that allows motion generation with both spatial (body part) and temporal (atomic action) control. Experiments demonstrate that FrankenMotion outperforms all previous baseline models adapted and retrained for our setting, and our model can compose motions unseen during training. Our code and dataset will be publicly available upon publication.', 'score': 10, 'issue_id': 552, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 —è–Ω–≤–∞—Ä—è', 'en': 'January 15', 'zh': '1Êúà15Êó•'}, 'hash': 'a6f74fdfe5df1e23', 'github_url': 'https://github.com/Coral79/FrankenMotion-Code', 'github_stars': 29, 'authors': ['Chuqiao Li', 'Xianghui Xie', 'Yong Cao', 'Andreas Geiger', 'Gerard Pons-Moll'], 'affiliations': ['Max Planck Institute for Informatics, Saarland Informatics Campus, Germany', 'Tubingen AI Center, University of Tubingen, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2601.10909.jpg', 'data': {'categories': ['#data', '#dataset'], 'emoji': 'üë©\u200dü¶∂', 'ru': {'title': '–§—Ä–∞–Ω–∫–µ–Ω–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç –¥–≤–∏–∂–µ–Ω–∏–π: –∫–æ–Ω—Ç—Ä–æ–ª—å —Ç–µ–ª–∞ –∏ –≤—Ä–µ–º–µ–Ω–∏', 'desc': '–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫ —Å –≤—ã—Å–æ–∫–æ–π —Å—Ç–µ–ø–µ–Ω—å—é –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —á–∞—Å—Ç–µ–π —Ç–µ–ª–∞. –î–ª—è —ç—Ç–æ–≥–æ –±—ã–ª —Å–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å –∞—Ç–æ–º–Ω—ã–º–∏ –∏ –≤—Ä–µ–º–µ–Ω–Ω–æ-–∑–∞–≤–∏—Å–∏–º—ã–º–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏ —á–∞—Å—Ç–µ–π —Ç–µ–ª–∞. –ù–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –º–æ–¥–µ–ª—å –§—Ä–∞–Ω–∫–µ–Ω–Ω–æ–ºotion, –ø–æ–∑–≤–æ–ª—è—é—â–∞—è —É–ø—Ä–∞–≤–ª—è—Ç—å –¥–≤–∏–∂–µ–Ω–∏—è–º–∏ –∫–∞–∫ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ, —Ç–∞–∫ –∏ –≤–æ –≤—Ä–µ–º–µ–Ω–∏, —á—Ç–æ —Ä–∞–Ω–µ–µ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤—ã–≤–∞–ª–æ—Å—å –≤ –ø–æ–ª–Ω–æ–π –º–µ—Ä–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞–¥ –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏.'}, 'en': {'title': '', 'desc': ''}}}, {'id': 'https://huggingface.co/papers/2601.09001', 'title': 'Entropy Sentinel: Continuous LLM Accuracy Monitoring from Decoding Entropy Traces in STEM', 'url': 'https://huggingface.co/papers/2601.09001', 'abstract': 'Output-entropy profiles computed from final-layer next-token probabilities serve as a scalable signal for monitoring LLM performance and prioritizing data acquisition under domain shifts.  \t\t\t\t\tAI-generated summary \t\t\t\t Deploying LLMs raises two coupled challenges: (1) monitoring - estimating where a model underperforms as traffic and domains drift - and (2) improvement - prioritizing data acquisition to close the largest performance gaps. We test whether an inference-time signal can estimate slice-level accuracy under domain shift. For each response, we compute an output-entropy profile from final-layer next-token probabilities (from top-k logprobs) and summarize it with eleven statistics. A lightweight classifier predicts instance correctness, and averaging predicted probabilities yields a domain-level accuracy estimate. We evaluate on ten STEM reasoning benchmarks with exhaustive train/test compositions (k in {1,2,3,4}; all "10 choose k" combinations), across nine LLMs from six families (3B-20B). Estimates often track held-out benchmark accuracy, and several models show near-monotonic ordering of domains. Output-entropy profiles are thus an accessible signal for scalable monitoring and for targeting data acquisition.', 'score': 10, 'issue_id': 552, 'pub_date': '2026-01-13', 'pub_date_card': {'ru': '13 —è–Ω–≤–∞—Ä—è', 'en': 'January 13', 'zh': '1Êúà13Êó•'}, 'hash': 'fa2ecacb83da9b5f', 'github_url': '', 'github_stars': 0, 'authors': ['Pedro Memoli Buffa', 'Luciano Del Corro'], 'affiliations': ['Departamento de Matematica, FCEyN Universidad de Buenos Aires', 'ELIAS Lab, Departamento de Ingenier√° Universidad de San Andres'], 'pdf_title_img': 'assets/pdf/title_img/2601.09001.jpg', 'data': {'categories': ['#training', '#interpretability', '#optimization', '#inference'], 'emoji': 'üìä', 'ru': {'title': '–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ —É–ª—É—á—à–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –ø—Ä–æ—Ñ–∏–ª–µ–π —ç–Ω—Ç—Ä–æ–ø–∏–∏ –≤—ã–≤–æ–¥–∞', 'desc': '–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—Ñ–∏–ª–µ–π —ç–Ω—Ç—Ä–æ–ø–∏–∏ –≤—ã–≤–æ–¥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –¥–æ–º–µ–Ω–æ–≤. –ü–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ —ç—Ç–∏ –ø—Ä–æ—Ñ–∏–ª–∏ –º–æ–≥—É—Ç —Å–ª—É–∂–∏—Ç—å –Ω–∞–¥–µ–∂–Ω—ã–º —Å–∏–≥–Ω–∞–ª–æ–º –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö –∏ –ø–æ–º–æ–≥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –Ω–∞–ø—Ä–∞–≤–ª—è—Ç—å –ø—Ä–æ—Ü–µ—Å—Å —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': '', 'desc': ''}}}, {'id': 'https://huggingface.co/papers/2601.09195', 'title': 'ProFit: Leveraging High-Value Signals in SFT via Probability-Guided Token Selection', 'url': 'https://huggingface.co/papers/2601.09195', 'abstract': 'Supervised fine-tuning with multiple references addresses overfitting to non-core expressions by masking low-probability tokens based on their semantic importance.  \t\t\t\t\tAI-generated summary \t\t\t\t Supervised fine-tuning (SFT) is a fundamental post-training strategy to align Large Language Models (LLMs) with human intent. However, traditional SFT often ignores the one-to-many nature of language by forcing alignment with a single reference answer, leading to the model overfitting to non-core expressions. Although our empirical analysis suggests that introducing multiple reference answers can mitigate this issue, the prohibitive data and computational costs necessitate a strategic shift: prioritizing the mitigation of single-reference overfitting over the costly pursuit of answer diversity. To achieve this, we reveal the intrinsic connection between token probability and semantic importance: high-probability tokens carry the core logical framework, while low-probability tokens are mostly replaceable expressions. Based on this insight, we propose ProFit, which selectively masks low-probability tokens to prevent surface-level overfitting. Extensive experiments confirm that ProFit consistently outperforms traditional SFT baselines on general reasoning and mathematical benchmarks.', 'score': 9, 'issue_id': 552, 'pub_date': '2026-01-14', 'pub_date_card': {'ru': '14 —è–Ω–≤–∞—Ä—è', 'en': 'January 14', 'zh': '1Êúà14Êó•'}, 'hash': '687e04f44bf56850', 'github_url': 'https://github.com/Utaotao/ProFit', 'github_stars': 18, 'authors': ['Tao Liu', 'Taiqiang Wu', 'Runming Yang', 'Shaoning Sun', 'Junjie Wang', 'Yujiu Yang'], 'affiliations': ['The University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2601.09195.jpg', 'data': {'categories': ['#training', '#optimization', '#reasoning'], 'emoji': 'üìö', 'ru': {'title': '–ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å—Å—ã–ª–∫–∞–º–∏ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω–æ–π –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–Ω–æ—Å—Ç–∏', 'desc': '–î–∞–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∏–∑–±—ã—Ç–æ—á–Ω–æ–π –ø–æ–¥–≥–æ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∫ –≤—Ç–æ—Ä–æ—Å—Ç–µ–ø–µ–Ω–Ω—ã–º –≤—ã—Ä–∞–∂–µ–Ω–∏—è–º –ø—Ä–∏ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–º –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–º –¥–æ–æ–±—É—á–µ–Ω–∏–∏ (SFT). –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ ProFit —Ä–µ—à–∞–µ—Ç —ç—Ç—É –∑–∞–¥–∞—á—É –ø—É—Ç–µ–º –º–∞—Å–∫–∏—Ä–æ–≤–∫–∏ –Ω–∏–∑–∫–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Ö —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –ª—É—á—à–µ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –∫–ª—é—á–µ–≤—ã–µ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ —É–ª—É—á—à–∞—Ç—å –µ–µ –æ–±—â—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å.'}, 'en': {'title': 'Article', 'desc': ''}}}, {'id': 'https://huggingface.co/papers/2601.11514', 'title': 'ShapeR: Robust Conditional 3D Shape Generation from Casual Captures', 'url': 'https://huggingface.co/papers/2601.11514', 'abstract': 'ShapeR generates high-fidelity 3D shapes from casual image sequences using visual-inertial SLAM, 3D detection, and vision-language models with rectified flow transformer conditioning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in 3D shape generation have achieved impressive results, but most existing methods rely on clean, unoccluded, and well-segmented inputs. Such conditions are rarely met in real-world scenarios. We present ShapeR, a novel approach for conditional 3D object shape generation from casually captured sequences. Given an image sequence, we leverage off-the-shelf visual-inertial SLAM, 3D detection algorithms, and vision-language models to extract, for each object, a set of sparse SLAM points, posed multi-view images, and machine-generated captions. A rectified flow transformer trained to effectively condition on these modalities then generates high-fidelity metric 3D shapes. To ensure robustness to the challenges of casually captured data, we employ a range of techniques including on-the-fly compositional augmentations, a curriculum training scheme spanning object- and scene-level datasets, and strategies to handle background clutter. Additionally, we introduce a new evaluation benchmark comprising 178 in-the-wild objects across 7 real-world scenes with geometry annotations. Experiments show that ShapeR significantly outperforms existing approaches in this challenging setting, achieving an improvement of 2.7x in Chamfer distance compared to state of the art.', 'score': 6, 'issue_id': 552, 'pub_date': '2026-01-16', 'pub_date_card': {'ru': '16 —è–Ω–≤–∞—Ä—è', 'en': 'January 16', 'zh': '1Êúà16Êó•'}, 'hash': 'e76e2e46a2ba726d', 'github_url': 'https://github.com/facebookresearch/ShapeR', 'github_stars': 115, 'authors': ['Yawar Siddiqui', 'Duncan Frost', 'Samir Aroudj', 'Armen Avetisyan', 'Henry Howard-Jenkins', 'Daniel DeTone', 'Pierre Moulon', 'Qirui Wu', 'Zhengqin Li', 'Julian Straub', 'Richard Newcombe', 'Jakob Engel'], 'affiliations': ['Meta Reality Labs Research', 'Simon Fraser University'], 'pdf_title_img': 'assets/pdf/title_img/2601.11514.jpg', 'data': {'categories': ['#3d'], 'emoji': 'üì¶', 'ru': {'title': 'ShapeR: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω—ã—Ö 3D —Ñ–æ—Ä–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–µ—Ñ–æ—Ä–º–∞–ª—å–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': 'ShapeR ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–º–æ–¥–µ–ª–µ–π –æ–±—ä–µ–∫—Ç–æ–≤ –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏–∑ –æ–±—ã—á–Ω—ã—Ö –≤–∏–¥–µ–æ–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–∏–∑—É–∞–ª—å–Ω–æ-–∏–Ω–µ—Ä—Ü–∏–∞–ª—å–Ω–æ–≥–æ SLAM, –¥–µ—Ç–µ–∫—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤ –∏ –º–æ–¥–µ–ª–µ–π –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞ —Å —É—Å–ª–æ–≤–∏–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –ø–æ—Ç–æ–∫–∞, –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç —Ç–µ—Ö–Ω–∏–∫–∏ —É—Å–∏–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏, –ø–æ—ç—Ç–∞–ø–Ω—É—é —Å—Ö–µ–º—É –æ–±—É—á–µ–Ω–∏—è –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –±–æ—Ä—å–±—ã —Å —Ñ–æ–Ω–æ–º, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏.'}, 'en': {'title': 'High-Fidelity 3D Shape Generation from Casual Image Sequences Using Multimodal Conditioning', 'desc': 'ShapeR uses visual-inertial SLAM, 3D detection, and vision-language models with a rectified flow transformer to generate high-fidelity 3D shapes from casual image sequences. It employs on-the-fly augmentations, curriculum training, and handling of background clutter to improve performance. The method achieves significant improvements over previous approaches in real-world settings.'}}}, {'id': 'https://huggingface.co/papers/2601.10825', 'title': 'Reasoning Models Generate Societies of Thought', 'url': 'https://huggingface.co/papers/2601.10825', 'abstract': 'Reasoning models demonstrate enhanced performance through multi-agent-like interactions that create diverse cognitive perspectives and improve problem-solving through structured social organization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have achieved remarkable capabilities across domains, yet mechanisms underlying sophisticated reasoning remain elusive. Recent reasoning models outperform comparable instruction-tuned models on complex cognitive tasks, attributed to extended computation through longer chains of thought. Here we show that enhanced reasoning emerges not from extended computation alone, but from simulating multi-agent-like interactions -- a society of thought -- which enables diversification and debate among internal cognitive perspectives characterized by distinct personality traits and domain expertise. Through quantitative analysis and mechanistic interpretability methods applied to reasoning traces, we find that reasoning models like DeepSeek-R1 and QwQ-32B exhibit much greater perspective diversity than instruction-tuned models, activating broader conflict between heterogeneous personality- and expertise-related features during reasoning. This multi-agent structure manifests in conversational behaviors, including question-answering, perspective shifts, and the reconciliation of conflicting views, and in socio-emotional roles that characterize sharp back-and-forth conversations, together accounting for the accuracy advantage in reasoning tasks. Controlled reinforcement learning experiments reveal that base models increase conversational behaviors when rewarded solely for reasoning accuracy, and fine-tuning models with conversational scaffolding accelerates reasoning improvement over base models. These findings indicate that the social organization of thought enables effective exploration of solution spaces. We suggest that reasoning models establish a computational parallel to collective intelligence in human groups, where diversity enables superior problem-solving when systematically structured, which suggests new opportunities for agent organization to harness the wisdom of crowds.', 'score': 5, 'issue_id': 552, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 —è–Ω–≤–∞—Ä—è', 'en': 'January 15', 'zh': '1Êúà15Êó•'}, 'hash': '7dcd7d60c59e0050', 'github_url': '', 'github_stars': 0, 'authors': ['Junsol Kim', 'Shiyang Lai', 'Nino Scherrer', 'Blaise Ag√ºera y Arcas', 'James Evans'], 'affiliations': ['Google', 'Paradigms of Intelligence Team', 'Santa Fe Institute', 'University of Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2601.10825.jpg', 'data': {'categories': ['#reasoning', '#agents', '#social_interaction', '#interpretability', '#cognitive_modeling'], 'emoji': 'ü§ñ', 'ru': {'title': '–°–æ—Ü–∏–∞–ª—å–Ω–∞—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è –º—ã—à–ª–µ–Ω–∏—è –∫–∞–∫ –∫–ª—é—á –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏', 'desc': '–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ –º–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–ª–∞–≥–æ–¥–∞—Ä—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—é –≤ —Å—Ç–∏–ª–µ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º, —Å–æ–∑–¥–∞—é—â–∏—Ö —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã –∏ —É–ª—É—á—à–∞—é—â–∏—Ö —Ä–µ—à–µ–Ω–∏–µ –∑–∞–¥–∞—á –∑–∞ —Å—á–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å–æ—Ü–∏–∞–ª—å–Ω–æ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–∏–ª–æ, —á—Ç–æ –≤—ã—Å–æ–∫–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –æ–±—É—Å–ª–æ–≤–ª–µ–Ω–∞ –Ω–µ —Ç–æ–ª—å–∫–æ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º –¥–ª–∏–Ω—ã —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –Ω–æ –∏ –∏–º–∏—Ç–∞—Ü–∏–µ–π –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –ª–∏—á–Ω–æ—Å—Ç–Ω—ã–º–∏ —á–µ—Ä—Ç–∞–º–∏ –∏ –æ–±–ª–∞—Å—Ç—è–º–∏ –∑–Ω–∞–Ω–∏–π. –≠—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–æ–º—É –æ–±—Å—É–∂–¥–µ–Ω–∏—é –∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—é –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ –º–Ω–µ–Ω–∏–π –≤–Ω—É—Ç—Ä–∏ –º–æ–¥–µ–ª–∏, —á—Ç–æ —Å–ø–æ—Å–æ–±—Å—Ç–≤—É–µ—Ç –ø–æ–≤—ã—à–µ–Ω–∏—é —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á.'}, 'en': {'title': 'Computational Collective Intelligence in Reasoning Models', 'desc': 'This paper explores how reasoning models enhance their performance through multi-agent-like interactions, creating diverse cognitive perspectives and improving problem-solving via structured social organization. It highlights that these models achieve better reasoning outcomes not just due to extended computation, but also because they simulate a society of thought, enabling diverse perspectives and debates. Quantitative analysis reveals that reasoning models like DeepSeek-R1 and QwQ-32B exhibit higher perspective diversity compared to instruction-tuned models, leading to improved reasoning accuracy. The study suggests that such social organization mimics collective intelligence in humans, offering new insights into leveraging the wisdom of crowds.'}}}, {'id': 'https://huggingface.co/papers/2601.11087', 'title': 'PhysRVG: Physics-Aware Unified Reinforcement Learning for Video Generative Models', 'url': 'https://huggingface.co/papers/2601.11087', 'abstract': "A physics-aware reinforcement learning paradigm is introduced for video generation that enforces physical collision rules directly in high-dimensional spaces, ensuring strict application of physics knowledge rather than treating it as conditional constraints.  \t\t\t\t\tAI-generated summary \t\t\t\t Physical principles are fundamental to realistic visual simulation, but remain a significant oversight in transformer-based video generation. This gap highlights a critical limitation in rendering rigid body motion, a core tenet of classical mechanics. While computer graphics and physics-based simulators can easily model such collisions using Newton formulas, modern pretrain-finetune paradigms discard the concept of object rigidity during pixel-level global denoising. Even perfectly correct mathematical constraints are treated as suboptimal solutions (i.e., conditions) during model optimization in post-training, fundamentally limiting the physical realism of generated videos. Motivated by these considerations, we introduce, for the first time, a physics-aware reinforcement learning paradigm for video generation models that enforces physical collision rules directly in high-dimensional spaces, ensuring the physics knowledge is strictly applied rather than treated as conditions. Subsequently, we extend this paradigm to a unified framework, termed Mimicry-Discovery Cycle (MDcycle), which allows substantial fine-tuning while fully preserving the model's ability to leverage physics-grounded feedback. To validate our approach, we construct new benchmark PhysRVGBench and perform extensive qualitative and quantitative experiments to thoroughly assess its effectiveness.", 'score': 4, 'issue_id': 552, 'pub_date': '2026-01-16', 'pub_date_card': {'ru': '16 —è–Ω–≤–∞—Ä—è', 'en': 'January 16', 'zh': '1Êúà16Êó•'}, 'hash': '7de1634e59778ee2', 'github_url': '', 'github_stars': 0, 'authors': ['Qiyuan Zhang', 'Biao Gong', 'Shuai Tan', 'Zheng Zhang', 'Yujun Shen', 'Xing Zhu', 'Yuyuan Li', 'Kelu Yao', 'Chunhua Shen', 'Changqing Zou'], 'affiliations': ['Ant Group', 'Zhejiang Lab', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2601.11087.jpg', 'data': {'categories': ['#rl', '#video_generation', '#physics'], 'emoji': 'üîÑ', 'ru': {'title': '–§–∏–∑–∏—á–µ—Å–∫–∏ –æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ —Ñ–∏–∑–∏–∫—É, –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä–∞—è –Ω–∞–ø—Ä—è–º—É—é –ø—Ä–∏–º–µ–Ω—è–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö —Å—Ç–æ–ª–∫–Ω–æ–≤–µ–Ω–∏–π –≤ –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å—Ç—Ä–æ–≥–æ —Å–æ–±–ª—é–¥–∞—Ç—å —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞–Ω–∏—è –≤–º–µ—Å—Ç–æ —Ç–æ–≥–æ, —á—Ç–æ–±—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å –∏—Ö –∫–∞–∫ —É—Å–ª–æ–≤–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è —Ü–∏–∫–ª –ø–æ–¥—Ä–∞–∂–∞–Ω–∏—è-–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è (MDcycle), –∫–æ—Ç–æ—Ä—ã–π –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–π –æ—Å–Ω–æ–≤–µ. –î–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –±—ã–ª —Å–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö PhysRVGBench –∏ –ø—Ä–æ–≤–µ–¥–µ–Ω—ã –ø–æ–¥—Ä–æ–±–Ω—ã–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã.'}, 'en': {'title': '', 'desc': ''}}}, {'id': 'https://huggingface.co/papers/2601.09636', 'title': 'PersonalAlign: Hierarchical Implicit Intent Alignment for Personalized GUI Agent with Long-Term User-Centric Records', 'url': 'https://huggingface.co/papers/2601.09636', 'abstract': "PersonalAlign framework addresses GUI agent alignment with implicit user intents through hierarchical memory organization and long-term record reasoning, improving both execution and proactive performance.  \t\t\t\t\tAI-generated summary \t\t\t\t While GUI agents have shown strong performance under explicit and completion instructions, real-world deployment requires aligning with users' more complex implicit intents. In this work, we highlight Hierarchical Implicit Intent Alignment for Personalized GUI Agent (PersonalAlign), a new agent task that requires agents to leverage long-term user records as persistent context to resolve omitted preferences in vague instructions and anticipate latent routines by user state for proactive assistance. To facilitate this study, we introduce AndroidIntent, a benchmark designed to evaluate agents' ability in resolving vague instructions and providing proactive suggestions through reasoning over long-term user records. We annotated 775 user-specific preferences and 215 routines from 20k long-term records across different users for evaluation. Furthermore, we introduce Hierarchical Intent Memory Agent (HIM-Agent), which maintains a continuously updating personal memory and hierarchically organizes user preferences and routines for personalization. Finally, we evaluate a range of GUI agents on AndroidIntent, including GPT-5, Qwen3-VL, and UI-TARS, further results show that HIM-Agent significantly improves both execution and proactive performance by 15.7% and 7.3%.", 'score': 4, 'issue_id': 552, 'pub_date': '2026-01-14', 'pub_date_card': {'ru': '14 —è–Ω–≤–∞—Ä—è', 'en': 'January 14', 'zh': '1Êúà14Êó•'}, 'hash': 'c7c5d5c2262a2509', 'github_url': '', 'github_stars': 0, 'authors': ['Yibo Lyu', 'Gongwei Chen', 'Rui Shao', 'Weili Guan', 'Liqiang Nie'], 'affiliations': ['Harbin Institute of Technology, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2601.09636.jpg', 'data': {'categories': ['#agent_memory', '#hallucinations', '#retrieval', '#context_window', '#reasoning', '#session_memory', '#agi', '#agents'], 'emoji': 'üì±', 'ru': {'title': '–õ–∏—á–Ω—ã–π –±–∞–ª–∞–Ω—Å: –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –Ω–∞–º–µ—Ä–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —á–µ—Ä–µ–∑ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—É—é –ø–∞–º—è—Ç—å –ò–ò-–∞–≥–µ–Ω—Ç–∞', 'desc': 'Framework PersonalAlign —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á—É –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –Ω–µ—è–≤–Ω—ã–º–∏ –Ω–∞–º–µ—Ä–µ–Ω–∏—è–º–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —á–µ—Ä–µ–∑ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é –ø–∞–º—è—Ç–∏ –∏ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–ø–∏—Å–µ–π, —á—Ç–æ —É–ª—É—á—à–∞–µ—Ç –∫–∞–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ, —Ç–∞–∫ –∏ –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤. –í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∑–∞–¥–∞—á–µ PersonalAlign, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –ø–µ—Ä—Å–æ–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é –ø–∞–º—è—Ç—å –¥–ª—è —É—á–µ—Ç–∞ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –∏ —Ä—É—Ç–∏–Ω–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ —Ç–µ—á–µ–Ω–∏–µ –¥–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏. –î–ª—è –æ—Ü–µ–Ω–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –±—ã–ª —Å–æ–∑–¥–∞–Ω –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö AndroidIntent, –≤–∫–ª—é—á–∞—é—â–∏–π –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –∏ —Ä—É—Ç–∏–Ω—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏ –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥—Ä—É–≥–∏–º–∏ –∞–≥–µ–Ω—Ç–∞–º–∏, —Ç–∞–∫–∏–º–∏ –∫–∞–∫ GPT-5 –∏ Qwen3-VL.'}, 'en': {'title': 'Hierarchical Implicit Intent Alignment for Personalized GUI Agents', 'desc': "The PersonalAlign framework focuses on aligning GUI agents with user's implicit intents using hierarchical memory organization and long-term record reasoning. It enhances both execution and proactive performance by leveraging persistent user contexts. The AndroidIntent benchmark evaluates agents' abilities to resolve vague instructions and provide proactive suggestions based on long-term user records. The Hierarchical Intent Memory Agent (HIM-Agent) maintains personalized memories and organizes user preferences and routines hierarchically for better personalization. Experimental results demonstrate significant improvements in execution and proactive performance compared to other agents."}}}, {'id': 'https://huggingface.co/papers/2601.11516', 'title': 'Building Production-Ready Probes For Gemini', 'url': 'https://huggingface.co/papers/2601.11516', 'abstract': "Activation probes for language model misuse mitigation face challenges with long-context generalization, requiring new architectures and diverse training for robust performance across production shifts.  \t\t\t\t\tAI-generated summary \t\t\t\t Frontier language model capabilities are improving rapidly. We thus need stronger mitigations against bad actors misusing increasingly powerful systems. Prior work has shown that activation probes may be a promising misuse mitigation technique, but we identify a key remaining challenge: probes fail to generalize under important production distribution shifts. In particular, we find that the shift from short-context to long-context inputs is difficult for existing probe architectures. We propose several new probe architecture that handle this long-context distribution shift.   We evaluate these probes in the cyber-offensive domain, testing their robustness against various production-relevant shifts, including multi-turn conversations, static jailbreaks, and adaptive red teaming. Our results demonstrate that while multimax addresses context length, a combination of architecture choice and training on diverse distributions is required for broad generalization. Additionally, we show that pairing probes with prompted classifiers achieves optimal accuracy at a low cost due to the computational efficiency of probes.   These findings have informed the successful deployment of misuse mitigation probes in user-facing instances of Gemini, Google's frontier language model. Finally, we find early positive results using AlphaEvolve to automate improvements in both probe architecture search and adaptive red teaming, showing that automating some AI safety research is already possible.", 'score': 3, 'issue_id': 552, 'pub_date': '2026-01-16', 'pub_date_card': {'ru': '16 —è–Ω–≤–∞—Ä—è', 'en': 'January 16', 'zh': '1Êúà16Êó•'}, 'hash': '87ddd4cbadd8d1f3', 'github_url': '', 'github_stars': 0, 'authors': ['J√°nos Kram√°r', 'Joshua Engels', 'Zheng Wang', 'Bilal Chughtai', 'Rohin Shah', 'Neel Nanda', 'Arthur Conmy'], 'affiliations': ['Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2601.11516.jpg', 'data': {'categories': ['#agent_memory', '#context_window', '#security', '#long_context', '#prompting', '#data'], 'emoji': 'üîÆ', 'ru': {'title': '–£–ª—É—á—à–µ–Ω–∏–µ –∑–∞—â–∏—Ç—ã —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –∞–∫—Ç–∏–≤–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø—Ä–æ–±—ã', 'desc': '–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –ø—Ä–æ–±–ª–µ–º—ã –æ–±–æ–±—â–µ–Ω–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–æ–Ω–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø—Ä–∏ –ø–µ—Ä–µ—Ö–æ–¥–µ –æ—Ç –∫–æ—Ä–æ—Ç–∫–∏—Ö –∫ –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º, —á—Ç–æ —Ç—Ä–µ–±—É–µ—Ç —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ–π —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–µ–π –≤ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –ø—Ä–æ–±–µ–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º–∏, –∞ —Ç–∞–∫–∂–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –∏—Ö —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ —á–µ—Ä–µ–∑ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –æ–±–ª–∞—Å—Ç–∏ –∫–∏–±–µ—Ä–∞—Ç–∞–∫. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å —Å–æ—á–µ—Ç–∞–Ω–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏ —à–∏—Ä–æ–∫–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –æ–±—â–µ–π —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç–∏, –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—è –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ò–ò —á–µ—Ä–µ–∑ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –≤—Ä–æ–¥–µ AlphaEvolve.'}, 'en': {'title': 'Mitigating Language Model Misuse via Robust Activation Probes', 'desc': 'This paper explores the challenges of long-context generalization in activation probes for language models used in misuse mitigation. It proposes new architectures and diverse training methods to enhance robustness across different production environments. The study evaluates these probes in cyber-offensive scenarios and finds that combining specific architectures with prompt-based classifiers yields optimal performance. This work informs the deployment of misuse mitigation techniques in advanced language models like Gemini.'}}}, {'id': 'https://huggingface.co/papers/2601.10781', 'title': 'Future Optical Flow Prediction Improves Robot Control & Video Generation', 'url': 'https://huggingface.co/papers/2601.10781', 'abstract': 'A novel language-conditioned optical flow forecasting model combines Vision-Language Model and Diffusion architecture to predict future motion from noisy web-scale video data, demonstrating versatility in robotic manipulation and video generation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Future motion representations, such as optical flow, offer immense value for control and generative tasks. However, forecasting generalizable spatially dense motion representations remains a key challenge, and learning such forecasting from noisy, real-world data remains relatively unexplored. We introduce FOFPred, a novel language-conditioned optical flow forecasting model featuring a unified Vision-Language Model (VLM) and Diffusion architecture. This unique combination enables strong multimodal reasoning with pixel-level generative fidelity for future motion prediction. Our model is trained on web-scale human activity data-a highly scalable but unstructured source. To extract meaningful signals from this noisy video-caption data, we employ crucial data preprocessing techniques and our unified architecture with strong image pretraining. The resulting trained model is then extended to tackle two distinct downstream tasks in control and generation. Evaluations across robotic manipulation and video generation under language-driven settings establish the cross-domain versatility of FOFPred, confirming the value of a unified VLM-Diffusion architecture and scalable learning from diverse web data for future optical flow prediction.', 'score': 3, 'issue_id': 559, 'pub_date': '2026-01-15', 'pub_date_card': {'ru': '15 —è–Ω–≤–∞—Ä—è', 'en': 'January 15', 'zh': '1Êúà15Êó•'}, 'hash': '100f67342322af94', 'github_url': 'https://github.com/SalesforceAIResearch/FOFPred', 'github_stars': 2, 'authors': ['Kanchana Ranasinghe', 'Honglu Zhou', 'Yu Fang', 'Luyu Yang', 'Le Xue', 'Ran Xu', 'Caiming Xiong', 'Silvio Savarese', 'Michael S Ryoo', 'Juan Carlos Niebles'], 'affiliations': ['Salesforce AI Research', 'Stony Brook University'], 'pdf_title_img': 'assets/pdf/title_img/2601.10781.jpg', 'data': {'categories': ['#optical_flow_forecasting', '#vision_language_model', '#diffusion', '#robotics', '#multimodal'], 'emoji': 'üìà', 'ru': {'title': '–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ—Ç–æ–∫–∞ —Å –ø–æ–º–æ—â—å—é –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–π Vision-Language Model –∏ Diffusion', 'desc': '–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å FOFPred –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –æ–ø—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ—Ç–æ–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è Vision-Language Model –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –¥–∏—Ñ—Ñ—É–∑–∏–∏. –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–∞—Ö —Ä–∞–∑–Ω–æ—Ä–æ–¥–Ω—ã—Ö –≤–∏–¥–µ–æ–¥–∞–Ω–Ω—ã—Ö –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞ —Å —Ü–µ–ª—å—é –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±—É–¥—É—â–∏—Ö –¥–≤–∏–∂–µ–Ω–∏–π –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ–±–∏—Ç—å—Å—è –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–≤–∏–∂–µ–Ω–∏—è –Ω–∞ –ø–∏–∫—Å–µ–ª—å–Ω–æ–º —É—Ä–æ–≤–Ω–µ. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –≤ –∑–∞–¥–∞—á–∞—Ö —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏ –∏ —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏–¥–µ–æ.'}, 'en': {'title': 'Unified Vision-Language Model and Diffusion Architecture for Optical Flow Forecasting', 'desc': 'FOFPred is a novel language-conditioned optical flow forecasting model combining a Vision-Language Model and a Diffusion architecture. It predicts future motion from noisy web-scale video data, showcasing its utility in robotic manipulation and video generation tasks. By leveraging a unified Vision-Language Model and Diffusion architecture, it achieves strong multimodal reasoning and high pixel-level generative fidelity. Trained on large-scale human activity data, it demonstrates versatile applications across control and generation tasks.'}}}, {'id': 'https://huggingface.co/papers/2601.11044', 'title': 'AgencyBench: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts', 'url': 'https://huggingface.co/papers/2601.11044', 'abstract': 'AgencyBench presents a comprehensive benchmark for evaluating autonomous agents across real-world scenarios, enabling automated evaluation through user simulation and sandbox environments while revealing performance gaps between closed-source and open-source models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) based autonomous agents demonstrate multifaceted capabilities to contribute substantially to economic production. However, existing benchmarks remain focused on single agentic capability, failing to capture long-horizon real-world scenarios. Moreover, the reliance on human-in-the-loop feedback for realistic tasks creates a scalability bottleneck, hindering automated rollout collection and evaluation. To bridge this gap, we introduce AgencyBench, a comprehensive benchmark derived from daily AI usage, evaluating 6 core agentic capabilities across 32 real-world scenarios, comprising 138 tasks with specific queries, deliverables, and rubrics. These scenarios require an average of 90 tool calls, 1 million tokens, and hours of execution time to resolve. To enable automated evaluation, we employ a user simulation agent to provide iterative feedback, and a Docker sandbox to conduct visual and functional rubric-based assessment. Experiments reveal that closed-source models significantly outperform open-source models (48.4% vs 32.1%). Further analysis reveals significant disparities across models in resource efficiency, feedback-driven self-correction, and specific tool-use preferences. Finally, we investigate the impact of agentic scaffolds, observing that proprietary models demonstrate superior performance within their native ecosystems (e.g., Claude-4.5-Opus via Claude-Agent-SDK), while open-source models exhibit distinct performance peaks, suggesting potential optimization for specific execution frameworks. AgencyBench serves as a critical testbed for next-generation agents, highlighting the necessity of co-optimizing model architecture with agentic frameworks. We believe this work sheds light on the future direction of autonomous agents, and we release the full benchmark and evaluation toolkit at https://github.com/GAIR-NLP/AgencyBench.', 'score': 2, 'issue_id': 559, 'pub_date': '2026-01-16', 'pub_date_card': {'ru': '16 —è–Ω–≤–∞—Ä—è', 'en': 'January 16', 'zh': '1Êúà16Êó•'}, 'hash': '7fa13a458769b5b4', 'github_url': 'https://github.com/GAIR-NLP/AgencyBench', 'github_stars': 18, 'authors': ['Keyu Li', 'Junhao Shi', 'Yang Xiao', 'Mohan Jiang', 'Jie Sun', 'Yunze Wu', 'Shijie Xia', 'Xiaojie Cai', 'Tianze Xu', 'Weiye Si', 'Wenjie Li', 'Dequan Wang', 'Pengfei Liu'], 'affiliations': ['GAIR', 'PolyU', 'SII', 'SJTU'], 'pdf_title_img': 'assets/pdf/title_img/2601.11044.jpg', 'data': {'categories': ['#agents', '#agent_memory', '#agi', '#tool_use', '#benchmark'], 'emoji': 'üöÄ', 'ru': {'title': 'AgencyBench: –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤', 'desc': 'AgencyBench –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é –ø–ª–∞—Ç—Ñ–æ—Ä–º—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö, –ø–æ–∑–≤–æ–ª—è—é—â—É—é –ø—Ä–æ–≤–æ–¥–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –æ—Ü–µ–Ω–∫—É —á–µ—Ä–µ–∑ —Å–∏–º—É–ª—è—Ü–∏—é –∏ –ø–µ—Å–æ—á–Ω–∏—Ü—ã, –≤—ã—è–≤–ª—è—è –ø—Ä–æ–±–µ–ª—ã –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–º–∏ –∏ –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è 32 —Ä–µ–∞–ª—å–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ —Å 138 –∑–∞–¥–∞—á–∞–º–∏, —Ç—Ä–µ–±—É—é—â–∏–º–∏ –¥–æ 90 –≤—ã–∑–æ–≤–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, –º–∏–ª–ª–∏–æ–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤ –∏ —á–∞—Å–æ–≤ –Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∞–≥–µ–Ω—Ç–∞-—Å–∏–º—É–ª—è—Ü–∏–∏ –∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ Docker –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—ã—è–≤–∏—Ç—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–∞–∑–ª–∏—á–∏—è –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π, –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ–π –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –æ—Ç–∫—Ä—ã—Ç—ã–µ –ø–æ —Ä—è–¥—É –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.'}, 'en': {'title': 'Evaluating Autonomous Agents Through Real-World Scenarios Using AgencyBench', 'desc': 'AgencyBench introduces a comprehensive benchmark for autonomous agents, evaluating them across diverse real-world scenarios using user simulations and sandbox environments. It highlights performance differences between closed-source and open-source models, emphasizing the need for optimized agentic frameworks. The benchmark includes 32 scenarios, 138 tasks, and requires extensive computational resources. Results show that proprietary models excel within their ecosystems, while open-source models have distinct strengths.'}}}, {'id': 'https://huggingface.co/papers/2601.07812', 'title': 'More Images, More Problems? A Controlled Analysis of VLM Failure Modes', 'url': 'https://huggingface.co/papers/2601.07812', 'abstract': 'Large Vision Language Models exhibit significant limitations in multi-image understanding and reasoning, which are revealed through a new benchmark and addressed via procedural data generation and attention masking techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Vision Language Models (LVLMs) have demonstrated remarkable capabilities, yet their proficiency in understanding and reasoning over multiple images remains largely unexplored. While existing benchmarks have initiated the evaluation of multi-image models, a comprehensive analysis of their core weaknesses and their causes is still lacking. In this work, we introduce MIMIC (Multi-Image Model Insights and Challenges), a new benchmark designed to rigorously evaluate the multi-image capabilities of LVLMs. Using MIMIC, we conduct a series of diagnostic experiments that reveal pervasive issues: LVLMs often fail to aggregate information across images and struggle to track or attend to multiple concepts simultaneously. To address these failures, we propose two novel complementary remedies. On the data side, we present a procedural data-generation strategy that composes single-image annotations into rich, targeted multi-image training examples. On the optimization side, we analyze layer-wise attention patterns and derive an attention-masking scheme tailored for multi-image inputs. Experiments substantially improved cross-image aggregation, while also enhancing performance on existing multi-image benchmarks, outperforming prior state of the art across tasks. Data and code will be made available at https://github.com/anurag-198/MIMIC.', 'score': 2, 'issue_id': 552, 'pub_date': '2026-01-12', 'pub_date_card': {'ru': '12 —è–Ω–≤–∞—Ä—è', 'en': 'January 12', 'zh': '1Êúà12Êó•'}, 'hash': '1087868e6e3d0c16', 'github_url': 'https://github.com/anurag-198/MIMIC', 'github_stars': 0, 'authors': ['Anurag Das', 'Adrian Bulat', 'Alberto Baldrati', 'Ioannis Maniadis Metaxas', 'Bernt Schiele', 'Georgios Tzimiropoulos', 'Brais Martinez'], 'affiliations': ['MPI for Informatics, Saarland Informatics Campus', 'Queen Mary University of London, UK', 'Samsung AI, Cambridge', 'Technical University of Ias, i, Romania'], 'pdf_title_img': 'assets/pdf/title_img/2601.07812.jpg', 'data': {'categories': ['#reasoning', '#interpretability', '#data', '#benchmark'], 'emoji': 'üìä', 'ru': {'title': '–í—ã—è–≤–ª–µ–Ω–∏–µ –∏ –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –º—É–ª—å—Ç–∏–∏–∑–æ–±—Ä–∞–∑–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞', 'desc': '–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É—é—Ç—Å—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞ (LVLM) –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏–∏ –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –î–ª—è —ç—Ç–æ–≥–æ –±—ã–ª —Å–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º MIMIC, –∫–æ—Ç–æ—Ä—ã–π –≤—ã—è–≤–ª—è–µ—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã —Ç–∞–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –¥–≤–∞ —Ä–µ—à–µ–Ω–∏—è: –ø—Ä–æ—Ü–µ–¥—É—Ä–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –º—É–ª—å—Ç–∏-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –º–∞—Å–∫–∏—Ä–æ–≤–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–ª–æ–µ–≤ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –≤ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –ø–æ–≤—ã—à–µ–Ω–∏–∏ –æ–±—â–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'Improving Multi-Image Understanding and Reasoning with Procedural Data Generation and Attention Masking', 'desc': 'This paper highlights the limitations of Large Vision Language Models (LVLMs) in multi-image understanding and reasoning. It introduces a new benchmark called MIMIC to assess these limitations and reveals that LVLMs struggle to aggregate information across images and track multiple concepts simultaneously. The authors propose two solutions: procedural data generation and attention masking techniques to enhance cross-image aggregation and improve overall performance.'}}}, {'id': 'https://huggingface.co/papers/2601.11354', 'title': 'AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems', 'url': 'https://huggingface.co/papers/2601.11354', 'abstract': 'Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research.', 'score': 1, 'issue_id': 552, 'pub_date': '2026-01-16', 'pub_date_card': {'ru': '16 —è–Ω–≤–∞—Ä—è', 'en': 'January 16', 'zh': '1Êúà16Êó•'}, 'hash': '91ba6b67dda55368', 'github_url': 'https://github.com/Mtrya/astro-reason', 'github_stars': 4, 'authors': ['Weiyi Wang', 'Xinchi Chen', 'Jingjing Gong', 'Xuanjing Huang', 'Xipeng Qiu'], 'affiliations': ['Fudan University', 'OpenMOSS Team', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2601.11354.jpg', 'data': {'categories': ['#agent_memory', '#agents', '#reasoning', '#benchmark'], 'emoji': 'üöÄ', 'ru': {'title': 'AstroReason-Bench: –∏—Å–ø—ã—Ç–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–Ω—ã—Ö LLM –≤ –∑–∞–¥–∞—á–∞—Ö –∫–æ—Å–º–∏—á–µ—Å–∫–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ —Å–æ–∑–¥–∞–Ω–∏—é –Ω–æ–≤–æ–≥–æ –∫–æ–º–ø–ª–µ–∫—Å–∞ –∏—Å–ø—ã—Ç–∞–Ω–∏–π ‚Äî AstroReason-Bench, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω–æ–≥–æ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –∞–≥–µ–Ω—Ç–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (–∞–≥–µ–Ω—Ç–Ω—ã—Ö LLM) –≤ —Ä–µ—à–µ–Ω–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –∫–æ—Å–º–æ—Å–æ–º –∏ —Å—Ç—Ä–æ–≥–∏–º–∏ —Ñ–∏–∑–∏—á–µ—Å–∫–∏–º–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏. –≠—Ç–æ—Ç –∫–æ–º–ø–ª–µ–∫—Å –≤–∫–ª—é—á–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —Ä–µ–∂–∏–º—ã –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è, —Ç–∞–∫–∏–µ –∫–∞–∫ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–µ–π –Ω–∞–∑–µ–º–Ω—ã—Ö —Å—Ç–∞–Ω—Ü–∏–π –∏ –≥–∏–±–∫–æ–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –ó–µ–º–ª–∏ –∏–∑ –∫–æ—Å–º–æ—Å–∞, –∞ —Ç–∞–∫–∂–µ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ—Ç–æ–∫–æ–ª –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –∞–≥–µ–Ω—Ç–æ–≤. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç—É–ø–∞—é—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Å–∏—Å—Ç–µ–º–∞–º –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö, –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è –∏ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–Ω—ã—Ö LLM.'}, 'en': {'title': 'AstroReason-Bench: A Benchmark for Agentic Planning in Space Planning Problems', 'desc': 'This paper introduces AstroReason-Bench, a new benchmark designed to evaluate the capabilities of agentic Large Language Models (LLMs) in handling complex, physically constrained real-world problems such as space planning. It focuses on Space Planning Problems (SPP), which involve heterogeneous objectives, strict physical constraints, and long-term decision-making. The benchmark includes various scheduling scenarios like ground station communication and agile Earth observation, providing a unified interaction protocol for agents. The evaluation reveals significant limitations of current generalist LLMs compared to specialized solvers, underscoring the need for further advancements in agentic research.'}}}, {'id': 'https://huggingface.co/papers/2601.11227', 'title': 'Language of Thought Shapes Output Diversity in Large Language Models', 'url': 'https://huggingface.co/papers/2601.11227', 'abstract': "Controlling the language of thought in large language models increases output diversity by leveraging distinct thinking spaces across different languages, with mixed-language sampling providing superior results.  \t\t\t\t\tAI-generated summary \t\t\t\t Output diversity is crucial for Large Language Models as it underpins pluralism and creativity. In this work, we reveal that controlling the language used during model thinking-the language of thought-provides a novel and structural source of output diversity. Our preliminary study shows that different thinking languages occupy distinct regions in a model's thinking space. Based on this observation, we study two repeated sampling strategies under multilingual thinking-Single-Language Sampling and Mixed-Language Sampling-and conduct diversity evaluation on outputs that are controlled to be in English, regardless of the thinking language used. Across extensive experiments, we demonstrate that switching the thinking language from English to non-English languages consistently increases output diversity, with a clear and consistent positive correlation such that languages farther from English in the thinking space yield larger gains. We further show that aggregating samples across multiple thinking languages yields additional improvements through compositional effects, and that scaling sampling with linguistic heterogeneity expands the model's diversity ceiling. Finally, we show that these findings translate into practical benefits in pluralistic alignment scenarios, leading to broader coverage of cultural knowledge and value orientations in LLM outputs. Our code is publicly available at https://github.com/iNLP-Lab/Multilingual-LoT-Diversity.", 'score': 1, 'issue_id': 552, 'pub_date': '2026-01-16', 'pub_date_card': {'ru': '16 —è–Ω–≤–∞—Ä—è', 'en': 'January 16', 'zh': '1Êúà16Êó•'}, 'hash': 'a590fc798fadb73e', 'github_url': 'https://github.com/iNLP-Lab/Multilingual-LoT-Diversity', 'github_stars': 2, 'authors': ['Shaoyang Xu', 'Wenxuan Zhang'], 'affiliations': ['Singapore University of Technology and Design'], 'pdf_title_img': 'assets/pdf/title_img/2601.11227.jpg', 'data': {'categories': ['#language_of_thought', '#language_modeling', '#output_diversity', '#interpretability', '#multilingual'], 'emoji': 'üåç', 'ru': {'title': '–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–º –º—ã—à–ª–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è', 'desc': '–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–ª–∏—è–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —è–∑—ã–∫–æ–≤ –º—ã—à–ª–µ–Ω–∏—è –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ë—ã–ª–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–æ–≤ –≤–æ –≤—Ä–µ–º—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –º—ã—à–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–∞–Ω–∏–º–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏ –≤ –µ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–π, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —É–≤–µ–ª–∏—á–µ–Ω–∏—é —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –≤—ã—Ö–æ–¥–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ —Å –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –Ω–∞ –¥—Ä—É–≥–∏–µ —è–∑—ã–∫–∏ –¥–∞–µ—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç, –ø—Ä–∏—á–µ–º –Ω–∞–∏–±–æ–ª—å—à–∏–π –ø—Ä–∏—Ä–æ—Å—Ç –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —è–∑—ã–∫–æ–≤, –Ω–∞–∏–±–æ–ª–µ–µ —É–¥–∞–ª–µ–Ω–Ω—ã—Ö –æ—Ç –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –º—ã—Å–ª–µ–π –º–æ–¥–µ–ª–∏. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è –∑–∞ —Å—á–µ—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –æ–±—Ä–∞–∑—Ü–æ–≤ –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —è–∑—ã–∫–æ–≤, —á—Ç–æ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'Article', 'desc': ''}}}, {'id': 'https://huggingface.co/papers/2601.10922', 'title': 'What Matters in Data Curation for Multimodal Reasoning? Insights from the DCVLR Challenge', 'url': 'https://huggingface.co/papers/2601.10922', 'abstract': 'Data curation for multimodal reasoning shows that difficulty-based example selection on aligned datasets drives performance gains, while increasing dataset size mainly reduces variance and synthetic augmentation heuristics often degrade performance.  \t\t\t\t\tAI-generated summary \t\t\t\t We study data curation for multimodal reasoning through the NeurIPS 2025 Data Curation for Vision-Language Reasoning (DCVLR) challenge, which isolates dataset selection by fixing the model and training protocol. Using a compact curated dataset derived primarily from Walton Multimodal Cold Start, our submission placed first in the challenge. Through post-competition ablations, we show that difficulty-based example selection on an aligned base dataset is the dominant driver of performance gains. Increasing dataset size does not reliably improve mean accuracy under the fixed training recipe, but mainly reduces run-to-run variance, while commonly used diversity and synthetic augmentation heuristics provide no additional benefit and often degrade performance. These results characterize DCVLR as a saturation-regime evaluation and highlight the central role of alignment and difficulty in data-efficient multimodal reasoning.', 'score': 1, 'issue_id': 560, 'pub_date': '2026-01-16', 'pub_date_card': {'ru': '16 —è–Ω–≤–∞—Ä—è', 'en': 'January 16', 'zh': '1Êúà16Êó•'}, 'hash': '89cd9e27c2aea55e', 'github_url': '', 'github_stars': 0, 'authors': ['Yosub Shin', 'Michael Buriek', 'Boris Sobolev', 'Pavel Bushuyeu', 'Vikas Kumar', 'Haoyang Xu', 'Samuel Watson', 'Igor Molybog'], 'affiliations': ['Cisco, USA', 'PwC, USA', 'University of Hawaii at Manoa'], 'pdf_title_img': 'assets/pdf/title_img/2601.10922.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#data', '#interpretability'], 'emoji': 'üìö', 'ru': {'title': '–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –º–Ω–æ–≥–æ–≤–µ–∫—Ç–æ—Ä–Ω–æ–π —Ä–∞—Å—Å—É–¥–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏', 'desc': '–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ—Ü–µ—Å—Å –∫—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω–æ–π —Ä–∞—Å—Å—É–¥–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏. –ë—ã–ª–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ, —á—Ç–æ –≤—ã–±–æ—Ä –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –≤ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö —è–≤–ª—è–µ—Ç—Å—è –æ—Å–Ω–æ–≤–Ω—ã–º —Ñ–∞–∫—Ç–æ—Ä–æ–º —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –£–≤–µ–ª–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –≥–ª–∞–≤–Ω—ã–º –æ–±—Ä–∞–∑–æ–º —Å–Ω–∏–∂–∞–µ—Ç –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å, –∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —á–∞—Å—Ç–æ –ø—Ä–∏–≤–æ–¥—è—Ç –∫ —É—Ö—É–¥—à–µ–Ω–∏—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.'}, 'en': {'title': 'Data Curation for Multimodal Reasoning Insights', 'desc': 'This paper explores how difficulty-based example selection on aligned datasets significantly improves performance in multimodal reasoning tasks. It highlights that increasing dataset size mostly reduces variance rather than improving mean accuracy. Synthetic augmentations often harm performance instead of enhancing it.'}}}, {'id': 'https://huggingface.co/papers/2601.09255', 'title': 'PhyRPR: Training-Free Physics-Constrained Video Generation', 'url': 'https://huggingface.co/papers/2601.09255', 'abstract': 'A three-stage pipeline decouples physical reasoning from visual synthesis in video generation, improving physical plausibility and motion controllability through distinct phases of reasoning, planning, and refinement.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent diffusion-based video generation models can synthesize visually plausible videos, yet they often struggle to satisfy physical constraints. A key reason is that most existing approaches remain single-stage: they entangle high-level physical understanding with low-level visual synthesis, making it hard to generate content that require explicit physical reasoning. To address this limitation, we propose a training-free three-stage pipeline,PhyRPR:Phy\\uline{Reason}--Phy\\uline{Plan}--Phy\\uline{Refine}, which decouples physical understanding from visual synthesis. Specifically, PhyReason uses a large multimodal model for physical state reasoning and an image generator for keyframe synthesis; PhyPlan deterministically synthesizes a controllable coarse motion scaffold; and PhyRefine injects this scaffold into diffusion sampling via a latent fusion strategy to refine appearance while preserving the planned dynamics. This staged design enables explicit physical control during generation. Extensive experiments under physics constraints show that our method consistently improves physical plausibility and motion controllability.', 'score': 1, 'issue_id': 552, 'pub_date': '2026-01-14', 'pub_date_card': {'ru': '14 —è–Ω–≤–∞—Ä—è', 'en': 'January 14', 'zh': '1Êúà14Êó•'}, 'hash': '4bc3324d676d118d', 'github_url': '', 'github_stars': 0, 'authors': ['Yibo Zhao', 'Hengjia Li', 'Xiaofei He', 'Boxi Wu'], 'affiliations': ['State Key Lab of CAD&CG, Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2601.09255.jpg', 'data': {'categories': ['#diffusion', '#physical_reasoning', '#multimodal', '#3d'], 'emoji': 'üîÑ', 'ru': {'title': '–¢—Ä–µ—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥ PhyRPR –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ñ–∏–∑–∏—á–µ—Å–∫–æ–π –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω–æ—Å—Ç–∏ –∏ –∫–æ–Ω—Ç—Ä–æ–ª—è –¥–≤–∏–∂–µ–Ω–∏–π –≤ –≤–∏–¥–µ–æ', 'desc': '–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Ç—Ä–µ—Ö—ç—Ç–∞–ø–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ PhyRPR –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–¥–µ–ª—è–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å—ã —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞. –ù–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ PhyReason –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Ñ–∏–∑–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏, –∞ –Ω–∞ –≤—Ç–æ—Ä–æ–º —ç—Ç–∞–ø–µ PhyPlan —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç—Å—è –≥—Ä—É–±—ã–π –ø–ª–∞–Ω –¥–≤–∏–∂–µ–Ω–∏—è. –ó–∞–≤–µ—Ä—à–∞—é—â–∏–π —ç—Ç–∞–ø PhyRefine —É–ª—É—á—à–∞–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞ –¥–≤–∏–∂–µ–Ω–∏–π. –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—Å–∏—Ç—å —Ñ–∏–∑–∏—á–µ—Å–∫—É—é –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω–æ—Å—Ç—å –∏ —É–ø—Ä–∞–≤–ª—è–µ–º–æ—Å—Ç—å –¥–≤–∏–∂–µ–Ω–∏–π –≤ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –≤–∏–¥–µ–æ.'}, 'en': {'title': 'Decoupling Physical Reasoning from Visual Synthesis in Video Generation', 'desc': 'This paper introduces a three-stage pipeline called PhyRPR:Phy Reason-Phy Plan-Phy Refine, which decouples physical reasoning and visual synthesis in video generation. It enhances physical plausibility and motion controllability by separating reasoning, planning, and refinement stages. The PhyReason stage uses a multimodal model for physical state reasoning and an image generator for keyframe synthesis. The PhyPlan stage generates a controllable coarse motion scaffold, and the PhyRefine stage integrates this scaffold into diffusion sampling to refine appearance while maintaining planned dynamics.'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agent_memory (5)', '#agents (4)', '#agi (3)', '#alignment', '#architecture', '#audio', '#benchmark (5)', '#compression', '#context_window (3)', '#continual_learning', '#data (5)', '#dataset (3)', '#diffusion (2)', '#ethics (1)', '#games (1)', '#graphs', '#hallucinations (2)', '#healthcare (1)', '#inference (2)', '#interpretability (5)', '#knowledge_graphs', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math', '#memorization', '#memory (1)', '#multilingual (1)', '#multimodal (4)', '#open_source', '#optimization (3)', '#plp', '#prompt_caching', '#prompting (1)', '#rag', '#reasoning (10)', '#retrieval (2)', '#retrieval_eval', '#rl (3)', '#rlhf', '#robotics (1)', '#science', '#security (1)', '#session_memory (1)', '#small_models', '#story_generation', '#survey', '#synthetic', '#tool_use (2)', '#training (3)', '#transfer_learning (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                const githubStars = Number(item["github_stars"]);
                let githubBlock = "";
                if (item["github_url"]) {
                    const starsText = Number.isFinite(githubStars) ? githubStars : 0;
                    githubBlock = `<span class="github-link"><span class="github-stars">‚≠ê ${starsText}</span><a href="${item['github_url']}" target="_blank">GitHub</a></span>`;
                }
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            üî∫ ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                                ${githubBlock}
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else if (sortBy === 'github') {
                sortedArticles.sort((a, b) => (Number(b.github_stars) || 0) - (Number(a.github_stars) || 0));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'üîÑ ' + getTimeDiff('2026-01-19 21:14',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "—Ä–µ–π—Ç–∏–Ω–≥—É",
                    pub_date: "–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏",
                    issue_id: "–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF",
                    github: "–∑–≤—ë–∑–¥–∞–º GitHub"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date",
                    github: "GitHub stars"
                },
                zh: {
                    default: "ËØÑÂàÜ",
                    pub_date: "ÂèëÂ∏ÉÊó•Êúü",
                    issue_id: "HF Êî∂ÂΩïÊó•Êúü",
                    github: "GitHub ÊòüÊ†á"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }

        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2026-01-19 21:14')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2026-01-19 21:14')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    