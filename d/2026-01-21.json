{
    "date": {
        "ru": "21 января",
        "en": "January 21",
        "zh": "1月21日"
    },
    "time_utc": "2026-01-21 06:26",
    "weekday": 2,
    "issue_id": 603,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2601.11522",
            "title": "UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation",
            "url": "https://huggingface.co/papers/2601.11522",
            "abstract": "UniX presents a unified medical foundation model that decouples visual understanding and generation tasks using distinct autoregressive and diffusion branches with cross-modal attention for enhanced performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite recent progress, medical foundation models still struggle to unify visual understanding and generation, as these tasks have inherently conflicting goals: semantic abstraction versus pixel-level reconstruction. Existing approaches, typically based on parameter-shared autoregressive architectures, frequently lead to compromised performance in one or both tasks. To address this, we present UniX, a next-generation unified medical foundation model for chest X-ray understanding and generation. UniX decouples the two tasks into an autoregressive branch for understanding and a diffusion branch for high-fidelity generation. Crucially, a cross-modal self-attention mechanism is introduced to dynamically guide the generation process with understanding features. Coupled with a rigorous data cleaning pipeline and a multi-stage training strategy, this architecture enables synergistic collaboration between tasks while leveraging the strengths of diffusion models for superior generation. On two representative benchmarks, UniX achieves a 46.1% improvement in understanding performance (Micro-F1) and a 24.2% gain in generation quality (FD-RadDino), using only a quarter of the parameters of LLM-CXR. By achieving performance on par with task-specific models, our work establishes a scalable paradigm for synergistic medical image understanding and generation. Codes and models are available at https://github.com/ZrH42/UniX.",
            "score": 14,
            "issue_id": 603,
            "pub_date": "2026-01-16",
            "pub_date_card": {
                "ru": "16 января",
                "en": "January 16",
                "zh": "1月16日"
            },
            "hash": "2e8309152ea38796",
            "github_url": "https://github.com/ZrH42/UniX",
            "github_stars": 17,
            "pdf_title_img": "assets/pdf/title_img/2601.11522.jpg",
            "data": {
                "error": "401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth"
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.11655",
            "title": "Advances and Frontiers of LLM-based Issue Resolution in Software Engineering: A Comprehensive Survey",
            "url": "https://huggingface.co/papers/2601.11655",
            "abstract": "Large language models face significant challenges in software issue resolution, prompting the development of autonomous coding agents through various training-free and training-based methodologies.  \t\t\t\t\tAI-generated summary \t\t\t\t Issue resolution, a complex Software Engineering (SWE) task integral to real-world development, has emerged as a compelling challenge for artificial intelligence. The establishment of benchmarks like SWE-bench revealed this task as profoundly difficult for large language models, thereby significantly accelerating the evolution of autonomous coding agents. This paper presents a systematic survey of this emerging domain. We begin by examining data construction pipelines, covering automated collection and synthesis approaches. We then provide a comprehensive analysis of methodologies, spanning training-free frameworks with their modular components to training-based techniques, including supervised fine-tuning and reinforcement learning. Subsequently, we discuss critical analyses of data quality and agent behavior, alongside practical applications. Finally, we identify key challenges and outline promising directions for future research. An open-source repository is maintained at https://github.com/DeepSoftwareAnalytics/Awesome-Issue-Resolution to serve as a dynamic resource in this field.",
            "score": 12,
            "issue_id": 603,
            "pub_date": "2026-01-15",
            "pub_date_card": {
                "ru": "15 января",
                "en": "January 15",
                "zh": "1月15日"
            },
            "hash": "3bba28a3c4ff036b",
            "github_url": "https://github.com/DeepSoftwareAnalytics/Awesome-Issue-Resolution",
            "github_stars": 19,
            "pdf_title_img": "assets/pdf/title_img/2601.11655.jpg",
            "data": {
                "error": "401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth"
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.12294",
            "title": "ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents",
            "url": "https://huggingface.co/papers/2601.12294",
            "abstract": "ToolPRMBench is introduced as a large-scale benchmark for evaluating process reward models in tool-using agents, featuring step-level test cases and multi-LLM verification to ensure data quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Reward-guided search methods have demonstrated strong potential in enhancing tool-using agents by effectively guiding sampling and exploration over complex action spaces. As a core design, those search methods utilize process reward models (PRMs) to provide step-level rewards, enabling more fine-grained monitoring. However, there is a lack of systematic and reliable evaluation benchmarks for PRMs in tool-using settings. In this paper, we introduce ToolPRMBench, a large-scale benchmark specifically designed to evaluate PRMs for tool-using agents. ToolPRMBench is built on top of several representative tool-using benchmarks and converts agent trajectories into step-level test cases. Each case contains the interaction history, a correct action, a plausible but incorrect alternative, and relevant tool metadata. We respectively utilize offline sampling to isolate local single-step errors and online sampling to capture realistic multi-step failures from full agent rollouts. A multi-LLM verification pipeline is proposed to reduce label noise and ensure data quality. We conduct extensive experiments across large language models, general PRMs, and tool-specialized PRMs on ToolPRMBench. The results reveal clear differences in PRM effectiveness and highlight the potential of specialized PRMs for tool-using. Code and data will be released at https://github.com/David-Li0406/ToolPRMBench.",
            "score": 11,
            "issue_id": 603,
            "pub_date": "2026-01-18",
            "pub_date_card": {
                "ru": "18 января",
                "en": "January 18",
                "zh": "1月18日"
            },
            "hash": "4ace6c0ff6a7867b",
            "github_url": "",
            "github_stars": 0,
            "pdf_title_img": "assets/pdf/title_img/2601.12294.jpg",
            "data": {
                "error": "401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth"
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.12993",
            "title": "Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization",
            "url": "https://huggingface.co/papers/2601.12993",
            "abstract": "Being-H0.5 is a Vision-Language-Action model that enables robust cross-embodiment generalization through human-centric learning and a Mixture-of-Transformers architecture with specialized embodiment handling.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Being-H0.5, a foundational Vision-Language-Action (VLA) model designed for robust cross-embodiment generalization across diverse robotic platforms. While existing VLAs often struggle with morphological heterogeneity and data scarcity, we propose a human-centric learning paradigm that treats human interaction traces as a universal \"mother tongue\" for physical interaction. To support this, we present UniHand-2.0, the largest embodied pre-training recipe to date, comprising over 35,000 hours of multimodal data across 30 distinct robotic embodiments. Our approach introduces a Unified Action Space that maps heterogeneous robot controls into semantically aligned slots, enabling low-resource robots to bootstrap skills from human data and high-resource platforms. Built upon this human-centric foundation, we design a unified sequential modeling and multi-task pre-training paradigm to bridge human demonstrations and robotic execution. Architecturally, Being-H0.5 utilizes a Mixture-of-Transformers design featuring a novel Mixture-of-Flow (MoF) framework to decouple shared motor primitives from specialized embodiment-specific experts. Finally, to make cross-embodiment policies stable in the real world, we introduce Manifold-Preserving Gating for robustness under sensory shift and Universal Async Chunking to universalize chunked control across embodiments with different latency and control profiles. We empirically demonstrate that Being-H0.5 achieves state-of-the-art results on simulated benchmarks, such as LIBERO (98.9%) and RoboCasa (53.9%), while also exhibiting strong cross-embodiment capabilities on five robotic platforms.",
            "score": 10,
            "issue_id": 603,
            "pub_date": "2026-01-19",
            "pub_date_card": {
                "ru": "19 января",
                "en": "January 19",
                "zh": "1月19日"
            },
            "hash": "ccaa8b7bf86cfa7f",
            "github_url": "https://github.com/BeingBeyond/Being-H",
            "github_stars": 250,
            "pdf_title_img": "assets/pdf/title_img/2601.12993.jpg",
            "data": {
                "error": "401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth"
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.11969",
            "title": "MemoryRewardBench: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models",
            "url": "https://huggingface.co/papers/2601.11969",
            "abstract": "A benchmark called MemoryRewardBench is introduced to systematically evaluate reward models' ability to assess long-term memory management in large language models across various context lengths and memory patterns.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing works increasingly adopt memory-centric mechanisms to process long contexts in a segment manner, and effective memory management is one of the key capabilities that enables large language models to effectively propagate information across the entire sequence. Therefore, leveraging reward models (RMs) to automatically and reliably evaluate memory quality is critical. In this work, we introduce MemoryRewardBench, the first benchmark to systematically study the ability of RMs to evaluate long-term memory management processes. MemoryRewardBench covers both long-context comprehension and long-form generation tasks, featuring 10 distinct settings with different memory management patterns, with context length ranging from 8K to 128K tokens. Evaluations on 13 cutting-edge RMs indicate a diminishing performance gap between open-source and proprietary models, with newer-generation models consistently outperforming their predecessors regardless of parameter count. We further expose the capabilities and fundamental limitations of current RMs in evaluating LLM memory management across diverse settings.",
            "score": 10,
            "issue_id": 603,
            "pub_date": "2026-01-17",
            "pub_date_card": {
                "ru": "17 января",
                "en": "January 17",
                "zh": "1月17日"
            },
            "hash": "3d51cd5cc576b6cc",
            "github_url": "https://github.com/LCM-Lab/MemRewardBench",
            "github_stars": 1,
            "pdf_title_img": "assets/pdf/title_img/2601.11969.jpg",
            "data": {
                "error": "401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth"
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.13247",
            "title": "Aligning Agentic World Models via Knowledgeable Experience Learning",
            "url": "https://huggingface.co/papers/2601.13247",
            "abstract": "WorldMind addresses the modal disconnect in LLMs by autonomously building a symbolic world knowledge repository that enhances physical feasibility and task optimality through experience-based learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Current Large Language Models (LLMs) exhibit a critical modal disconnect: they possess vast semantic knowledge but lack the procedural grounding to respect the immutable laws of the physical world. Consequently, while these agents implicitly function as world models, their simulations often suffer from physical hallucinations-generating plans that are logically sound but physically unexecutable. Existing alignment strategies predominantly rely on resource-intensive training or fine-tuning, which attempt to compress dynamic environmental rules into static model parameters. However, such parametric encapsulation is inherently rigid, struggling to adapt to the open-ended variability of physical dynamics without continuous, costly retraining. To bridge this gap, we introduce WorldMind, a framework that autonomously constructs a symbolic World Knowledge Repository by synthesizing environmental feedback. Specifically, it unifies Process Experience to enforce physical feasibility via prediction errors and Goal Experience to guide task optimality through successful trajectories. Experiments on EB-ALFRED and EB-Habitat demonstrate that WorldMind achieves superior performance compared to baselines with remarkable cross-model and cross-environment transferability.",
            "score": 9,
            "issue_id": 603,
            "pub_date": "2026-01-19",
            "pub_date_card": {
                "ru": "19 января",
                "en": "January 19",
                "zh": "1月19日"
            },
            "hash": "d94ae6ed0aceb888",
            "github_url": "https://github.com/zjunlp/WorldMind",
            "github_stars": 13,
            "pdf_title_img": "assets/pdf/title_img/2601.13247.jpg",
            "data": {
                "error": "401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth"
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.14250",
            "title": "OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer",
            "url": "https://huggingface.co/papers/2601.14250",
            "abstract": "OmniTransfer presents a unified framework for spatio-temporal video transfer that enhances appearance consistency and temporal control through multi-view information and multimodal semantic guidance.  \t\t\t\t\tAI-generated summary \t\t\t\t Videos convey richer information than images or text, capturing both spatial and temporal dynamics. However, most existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit the rich spatio-temporal information inherent in videos, thereby limiting flexibility and generalization in video generation. To address these limitations, we propose OmniTransfer, a unified framework for spatio-temporal video transfer. It leverages multi-view information across frames to enhance appearance consistency and exploits temporal cues to enable fine-grained temporal control. To unify various video transfer tasks, OmniTransfer incorporates three key designs: Task-aware Positional Bias that adaptively leverages reference video information to improve temporal alignment or appearance consistency; Reference-decoupled Causal Learning separating reference and target branches to enable precise reference transfer while improving efficiency; and Task-adaptive Multimodal Alignment using multimodal semantic guidance to dynamically distinguish and tackle different tasks. Extensive experiments show that OmniTransfer outperforms existing methods in appearance (ID and style) and temporal transfer (camera movement and video effects), while matching pose-guided methods in motion transfer without using pose, establishing a new paradigm for flexible, high-fidelity video generation.",
            "score": 7,
            "issue_id": 603,
            "pub_date": "2026-01-20",
            "pub_date_card": {
                "ru": "20 января",
                "en": "January 20",
                "zh": "1月20日"
            },
            "hash": "ea0f06a5786530a6",
            "github_url": "https://github.com/PangzeCheung/OmniTransfer",
            "github_stars": 23,
            "pdf_title_img": "assets/pdf/title_img/2601.14250.jpg",
            "data": {
                "error": "401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth"
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.14192",
            "title": "Toward Efficient Agents: Memory, Tool learning, and Planning",
            "url": "https://huggingface.co/papers/2601.14192",
            "abstract": "Efficiency in agentic systems is examined across memory, tool learning, and planning components, analyzing trade-offs between effectiveness and computational costs through various optimization strategies and benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.",
            "score": 6,
            "issue_id": 603,
            "pub_date": "2026-01-20",
            "pub_date_card": {
                "ru": "20 января",
                "en": "January 20",
                "zh": "1月20日"
            },
            "hash": "a98c299a62724db7",
            "github_url": "https://github.com/yxf203/Awesome-Efficient-Agents",
            "github_stars": 8,
            "pdf_title_img": "assets/pdf/title_img/2601.14192.jpg",
            "data": {
                "error": "401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth"
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.14046",
            "title": "PRiSM: Benchmarking Phone Realization in Speech Models",
            "url": "https://huggingface.co/papers/2601.14046",
            "abstract": "PRiSM benchmark evaluates phonetic perception in speech models through standardized transcription-based metrics and downstream applications across clinical, educational, and multilingual domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Phone recognition (PR) serves as the atomic interface for language-agnostic modeling for cross-lingual speech processing and phonetic analysis. Despite prolonged efforts in developing PR systems, current evaluations only measure surface-level transcription accuracy. We introduce PRiSM, the first open-source benchmark designed to expose blind spots in phonetic perception through intrinsic and extrinsic evaluation of PR systems. PRiSM standardizes transcription-based evaluation and assesses downstream utility in clinical, educational, and multilingual settings with transcription and representation probes. We find that diverse language exposure during training is key to PR performance, encoder-CTC models are the most stable, and specialized PR models still outperform Large Audio Language Models. PRiSM releases code, recipes, and datasets to move the field toward multilingual speech models with robust phonetic ability: https://github.com/changelinglab/prism.",
            "score": 3,
            "issue_id": 603,
            "pub_date": "2026-01-20",
            "pub_date_card": {
                "ru": "20 января",
                "en": "January 20",
                "zh": "1月20日"
            },
            "hash": "ee74254991809e26",
            "github_url": "https://github.com/changelinglab/prism",
            "github_stars": 1,
            "pdf_title_img": "assets/pdf/title_img/2601.14046.jpg",
            "data": {
                "error": "401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth"
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.13288",
            "title": "A BERTology View of LLM Orchestrations: Token- and Layer-Selective Probes for Efficient Single-Pass Classification",
            "url": "https://huggingface.co/papers/2601.13288",
            "abstract": "Lightweight probes trained on hidden states of LLMs enable efficient classification tasks without additional computational overhead, improving safety and sentiment analysis performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Production LLM systems often rely on separate models for safety and other classification-heavy steps, increasing latency, VRAM footprint, and operational complexity. We instead reuse computation already paid for by the serving LLM: we train lightweight probes on its hidden states and predict labels in the same forward pass used for generation. We frame classification as representation selection over the full token-layer hidden-state tensor, rather than committing to a fixed token or fixed layer (e.g., first-token logits or final-layer pooling). To implement this, we introduce a two-stage aggregator that (i) summarizes tokens within each layer and (ii) aggregates across layer summaries to form a single representation for classification. We instantiate this template with direct pooling, a 100K-parameter scoring-attention gate, and a downcast multi-head self-attention (MHA) probe with up to 35M trainable parameters. Across safety and sentiment benchmarks our probes improve over logit-only reuse (e.g., MULI) and are competitive with substantially larger task-specific baselines, while preserving near-serving latency and avoiding the VRAM and latency costs of a separate guard-model pipeline.",
            "score": 1,
            "issue_id": 603,
            "pub_date": "2026-01-19",
            "pub_date_card": {
                "ru": "19 января",
                "en": "January 19",
                "zh": "1月19日"
            },
            "hash": "16b301dd1efbd006",
            "github_url": "",
            "github_stars": 0,
            "pdf_title_img": "assets/pdf/title_img/2601.13288.jpg",
            "data": {
                "error": "401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth"
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.10700",
            "title": "LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals",
            "url": "https://huggingface.co/papers/2601.10700",
            "abstract": "A framework for generating structured counterfactual pairs using LLMs and SCMs enables improved evaluation and analysis of concept-based explanations in high-stakes domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influence model behavior, which is crucial for decision-makers in high-stakes domains. Recent work evaluates the faithfulness of such explanations by comparing them to reference causal effects estimated from counterfactuals. In practice, existing benchmarks rely on costly human-written counterfactuals that serve as an imperfect proxy. To address this, we introduce a framework for constructing datasets containing structural counterfactual pairs: LIBERTy (LLM-based Interventional Benchmark for Explainability with Reference Targets). LIBERTy is grounded in explicitly defined Structured Causal Models (SCMs) of the text generation, interventions on a concept propagate through the SCM until an LLM generates the counterfactual. We introduce three datasets (disease detection, CV screening, and workplace violence prediction) together with a new evaluation metric, order-faithfulness. Using them, we evaluate a wide range of methods across five models and identify substantial headroom for improving concept-based explanations. LIBERTy also enables systematic analysis of model sensitivity to interventions: we find that proprietary LLMs show markedly reduced sensitivity to demographic concepts, likely due to post-training mitigation. Overall, LIBERTy provides a much-needed benchmark for developing faithful explainability methods.",
            "score": 1,
            "issue_id": 603,
            "pub_date": "2026-01-15",
            "pub_date_card": {
                "ru": "15 января",
                "en": "January 15",
                "zh": "1月15日"
            },
            "hash": "68fa6c7fe60c9fca",
            "github_url": "https://github.com/GilatToker/Liberty-benchmark",
            "github_stars": 1,
            "pdf_title_img": "assets/pdf/title_img/2601.10700.jpg",
            "data": {
                "error": "401 Client Error: Unauthorized for url: https://ngw.devices.sberbank.ru:9443/api/v2/oauth"
            }
        }
    ],
    "link_prev": "2026-01-20.html",
    "link_next": "2026-01-22.html",
    "link_month": "2026-01.html",
    "short_date_prev": {
        "ru": "20.01",
        "en": "01/20",
        "zh": "1月20日"
    },
    "short_date_next": {
        "ru": "22.01",
        "en": "01/22",
        "zh": "1月22日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0,
        "#memory": 0,
        "#retrieval": 0,
        "#prompting": 0,
        "#context_window": 0,
        "#compression": 0,
        "#memorization": 0,
        "#continual_learning": 0,
        "#knowledge_graphs": 0,
        "#retrieval_eval": 0,
        "#prompt_caching": 0,
        "#session_memory": 0,
        "#tool_use": 0,
        "#agent_memory": 0
    }
}